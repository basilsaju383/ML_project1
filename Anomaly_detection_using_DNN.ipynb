{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMB0YgJ/w1cPqDc6zHmCfpr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/basilsaju383/ML_project1/blob/main/Anomaly_detection_using_DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htd7eFumK8if"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('/content/sample_data/mc4 GSM 3 month.xlsx')\n",
        "df = df[['Date','RSSI']]\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df = df.dropna()\n",
        "df.head()"
      ],
      "metadata": {
        "id": "IdhRA8m-K92s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mvrhW1zK94g",
        "outputId": "e877d77b-f67e-4597-e8af-edf8990636b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 4654 entries, 1918 to 132349\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype         \n",
            "---  ------  --------------  -----         \n",
            " 0   Date    4654 non-null   datetime64[ns]\n",
            " 1   RSSI    4654 non-null   float64       \n",
            "dtypes: datetime64[ns](1), float64(1)\n",
            "memory usage: 109.1 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "VKv8Zm8BK966"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "scaler = scaler.fit(np.array(train['RSSI']).reshape(-1,1))\n",
        "\n",
        "train['RSSI'] = scaler.transform(np.array(train['RSSI']).reshape(-1,1))\n",
        "test['RSSI'] = scaler.transform(np.array(test['RSSI']).reshape(-1,1))"
      ],
      "metadata": {
        "id": "sZ_yx08OK9_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TIME_STEPS=50\n",
        "\n",
        "def create_sequences(X, y, time_steps=TIME_STEPS):\n",
        "    X_out, y_out = [], []\n",
        "    for i in range(len(X)-time_steps):\n",
        "        X_out.append(X.iloc[i:(i+time_steps)].values)\n",
        "        y_out.append(y.iloc[i+time_steps])\n",
        "\n",
        "    return np.array(X_out), np.array(y_out)\n",
        "\n",
        "X_train, y_train = create_sequences(train[['RSSI']], train['RSSI'])\n",
        "X_test, y_test = create_sequences(test[['RSSI']], test['RSSI'])\n",
        "print(\"Training input shape: \", X_train.shape)\n",
        "print(\"Testing input shape: \", X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALFx1P6gK-Sr",
        "outputId": "984552c7-42b2-4fe7-893b-54be650a1505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training input shape:  (3673, 50, 1)\n",
            "Testing input shape:  (881, 50, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(21)\n",
        "tf.random.set_seed(21)"
      ],
      "metadata": {
        "id": "xHwPyeteK-U5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    min_delta=0.001, # minimium amount of change to count as an improvement\n",
        "    patience=150, # how many epochs to wait before stopping\n",
        "    restore_best_weights=True,\n",
        ")\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, activation = 'relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model.add(Dropout(rate=0.2))\n",
        "model.add(RepeatVector(X_train.shape[1]))\n",
        "model.add(LSTM(128, activation = 'relu', return_sequences=True))\n",
        "model.add(Dropout(rate=0.2))\n",
        "model.add(LSTM(128, activation = 'relu', return_sequences=True))\n",
        "model.add(Dropout(rate=0.2))\n",
        "model.add(TimeDistributed(Dense(X_train.shape[2])))\n"
      ],
      "metadata": {
        "id": "_tKuFfx5N_Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "def lr_schedule(epoch, lr):\n",
        "    if epoch % 10 == 0 and epoch > 0:\n",
        "        lr = lr * 0.9\n",
        "    return lr\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='mse')\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aI7zAcX19xI",
        "outputId": "8d621268-6623-4353-d70d-c6fa53a11342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_15 (LSTM)              (None, 128)               66560     \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " repeat_vector_2 (RepeatVec  (None, 50, 128)           0         \n",
            " tor)                                                            \n",
            "                                                                 \n",
            " lstm_16 (LSTM)              (None, 50, 128)           131584    \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 50, 128)           0         \n",
            "                                                                 \n",
            " lstm_17 (LSTM)              (None, 50, 128)           131584    \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 50, 128)           0         \n",
            "                                                                 \n",
            " time_distributed_2 (TimeDi  (None, 50, 1)             129       \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 329857 (1.26 MB)\n",
            "Trainable params: 329857 (1.26 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train,\n",
        "                    y_train,\n",
        "                    epochs=500,\n",
        "                    batch_size=64,\n",
        "                    validation_split=0.1,\n",
        "                    callbacks=[early_stopping],\n",
        "                    shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtCZIAGDN_dt",
        "outputId": "47eb608a-877e-4227-90bd-4b6bbf1d5e77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "52/52 [==============================] - 22s 342ms/step - loss: 1.0098 - val_loss: 0.9294\n",
            "Epoch 2/500\n",
            "52/52 [==============================] - 20s 384ms/step - loss: 1.0084 - val_loss: 0.9289\n",
            "Epoch 3/500\n",
            "52/52 [==============================] - 22s 422ms/step - loss: 1.0080 - val_loss: 0.9279\n",
            "Epoch 4/500\n",
            "52/52 [==============================] - 18s 341ms/step - loss: 1.0074 - val_loss: 23949.0820\n",
            "Epoch 5/500\n",
            "52/52 [==============================] - 16s 316ms/step - loss: 1.0092 - val_loss: 0.9295\n",
            "Epoch 6/500\n",
            "52/52 [==============================] - 17s 331ms/step - loss: 1.0077 - val_loss: 0.9292\n",
            "Epoch 7/500\n",
            "52/52 [==============================] - 18s 346ms/step - loss: 1.0072 - val_loss: 0.9280\n",
            "Epoch 8/500\n",
            "52/52 [==============================] - 17s 323ms/step - loss: 1.0083 - val_loss: 0.9288\n",
            "Epoch 9/500\n",
            "52/52 [==============================] - 17s 320ms/step - loss: 1.0072 - val_loss: 0.9260\n",
            "Epoch 10/500\n",
            "52/52 [==============================] - 18s 342ms/step - loss: 1.0070 - val_loss: 0.9278\n",
            "Epoch 11/500\n",
            "52/52 [==============================] - 17s 323ms/step - loss: 1.0070 - val_loss: 0.9274\n",
            "Epoch 12/500\n",
            "52/52 [==============================] - 17s 324ms/step - loss: 1.0066 - val_loss: 0.9259\n",
            "Epoch 13/500\n",
            "52/52 [==============================] - 18s 348ms/step - loss: 1.0065 - val_loss: 0.9244\n",
            "Epoch 14/500\n",
            "52/52 [==============================] - 17s 325ms/step - loss: 1.0063 - val_loss: 0.9246\n",
            "Epoch 15/500\n",
            "52/52 [==============================] - 18s 341ms/step - loss: 1.0062 - val_loss: 0.9245\n",
            "Epoch 16/500\n",
            "52/52 [==============================] - 18s 335ms/step - loss: 1.0062 - val_loss: 0.9244\n",
            "Epoch 17/500\n",
            "52/52 [==============================] - 17s 324ms/step - loss: 1.0061 - val_loss: 0.9244\n",
            "Epoch 18/500\n",
            "52/52 [==============================] - 18s 351ms/step - loss: 1.0061 - val_loss: 0.9246\n",
            "Epoch 19/500\n",
            "52/52 [==============================] - 17s 325ms/step - loss: 1.0060 - val_loss: 0.9243\n",
            "Epoch 20/500\n",
            "52/52 [==============================] - 17s 321ms/step - loss: 1.0059 - val_loss: 0.9242\n",
            "Epoch 21/500\n",
            "52/52 [==============================] - 18s 343ms/step - loss: 1.0059 - val_loss: 0.9245\n",
            "Epoch 22/500\n",
            "52/52 [==============================] - 17s 321ms/step - loss: 1.0058 - val_loss: 0.9243\n",
            "Epoch 23/500\n",
            "52/52 [==============================] - 17s 324ms/step - loss: 1.0058 - val_loss: 0.9242\n",
            "Epoch 24/500\n",
            "52/52 [==============================] - 18s 346ms/step - loss: 1.0058 - val_loss: 0.9245\n",
            "Epoch 25/500\n",
            "52/52 [==============================] - 16s 318ms/step - loss: 1.0057 - val_loss: 0.9243\n",
            "Epoch 26/500\n",
            "52/52 [==============================] - 17s 322ms/step - loss: 1.0056 - val_loss: 0.9243\n",
            "Epoch 27/500\n",
            "52/52 [==============================] - 18s 343ms/step - loss: 1.0056 - val_loss: 0.9243\n",
            "Epoch 28/500\n",
            "52/52 [==============================] - 17s 323ms/step - loss: 1.0056 - val_loss: 0.9243\n",
            "Epoch 29/500\n",
            "52/52 [==============================] - 20s 392ms/step - loss: 1.0056 - val_loss: 0.9244\n",
            "Epoch 30/500\n",
            "52/52 [==============================] - 22s 419ms/step - loss: 1.0055 - val_loss: 0.9243\n",
            "Epoch 31/500\n",
            "52/52 [==============================] - 18s 342ms/step - loss: 1.0055 - val_loss: 0.9243\n",
            "Epoch 32/500\n",
            "52/52 [==============================] - 16s 316ms/step - loss: 1.0053 - val_loss: 0.9242\n",
            "Epoch 33/500\n",
            "52/52 [==============================] - 17s 319ms/step - loss: 1.0054 - val_loss: 0.9243\n",
            "Epoch 34/500\n",
            "52/52 [==============================] - 17s 336ms/step - loss: 1.0053 - val_loss: 0.9241\n",
            "Epoch 35/500\n",
            "52/52 [==============================] - 16s 317ms/step - loss: 1.0052 - val_loss: 0.9241\n",
            "Epoch 36/500\n",
            "52/52 [==============================] - 17s 324ms/step - loss: 1.0052 - val_loss: 0.9242\n",
            "Epoch 37/500\n",
            "52/52 [==============================] - 18s 352ms/step - loss: 1.0052 - val_loss: 0.9240\n",
            "Epoch 38/500\n",
            "52/52 [==============================] - 17s 332ms/step - loss: 1.0051 - val_loss: 0.9241\n",
            "Epoch 39/500\n",
            "52/52 [==============================] - 16s 315ms/step - loss: 1.0050 - val_loss: 0.9242\n",
            "Epoch 40/500\n",
            "52/52 [==============================] - 17s 335ms/step - loss: 1.0049 - val_loss: 0.9241\n",
            "Epoch 41/500\n",
            "52/52 [==============================] - 16s 315ms/step - loss: 1.0050 - val_loss: 0.9242\n",
            "Epoch 42/500\n",
            "52/52 [==============================] - 16s 316ms/step - loss: 1.0048 - val_loss: 0.9243\n",
            "Epoch 43/500\n",
            "52/52 [==============================] - 18s 341ms/step - loss: 1.0048 - val_loss: 0.9243\n",
            "Epoch 44/500\n",
            "52/52 [==============================] - 22s 417ms/step - loss: 1.0047 - val_loss: 0.9244\n",
            "Epoch 45/500\n",
            "52/52 [==============================] - 21s 410ms/step - loss: 1.0046 - val_loss: 0.9245\n",
            "Epoch 46/500\n",
            "52/52 [==============================] - 18s 344ms/step - loss: 1.0045 - val_loss: 0.9245\n",
            "Epoch 47/500\n",
            "52/52 [==============================] - 21s 401ms/step - loss: 1.0045 - val_loss: 0.9246\n",
            "Epoch 48/500\n",
            "52/52 [==============================] - 17s 322ms/step - loss: 1.0045 - val_loss: 0.9247\n",
            "Epoch 49/500\n",
            "52/52 [==============================] - 19s 362ms/step - loss: 1.0043 - val_loss: 0.9248\n",
            "Epoch 50/500\n",
            "52/52 [==============================] - 17s 322ms/step - loss: 1.0042 - val_loss: 0.9249\n",
            "Epoch 51/500\n",
            "52/52 [==============================] - 17s 318ms/step - loss: 1.0040 - val_loss: 0.9249\n",
            "Epoch 52/500\n",
            "52/52 [==============================] - 18s 339ms/step - loss: 1.0039 - val_loss: 0.9251\n",
            "Epoch 53/500\n",
            "52/52 [==============================] - 17s 321ms/step - loss: 1.0039 - val_loss: 0.9250\n",
            "Epoch 54/500\n",
            "52/52 [==============================] - 18s 340ms/step - loss: 1.0038 - val_loss: 0.9251\n",
            "Epoch 55/500\n",
            "52/52 [==============================] - 17s 328ms/step - loss: 1.0038 - val_loss: 0.9250\n",
            "Epoch 56/500\n",
            "52/52 [==============================] - 17s 325ms/step - loss: 1.0037 - val_loss: 0.9252\n",
            "Epoch 57/500\n",
            "52/52 [==============================] - 18s 349ms/step - loss: 1.0034 - val_loss: 0.9253\n",
            "Epoch 58/500\n",
            "52/52 [==============================] - 17s 323ms/step - loss: 1.0034 - val_loss: 0.9253\n",
            "Epoch 59/500\n",
            "52/52 [==============================] - 17s 318ms/step - loss: 1.0033 - val_loss: 0.9254\n",
            "Epoch 60/500\n",
            "52/52 [==============================] - 18s 342ms/step - loss: 1.0032 - val_loss: 0.9257\n",
            "Epoch 61/500\n",
            "52/52 [==============================] - 17s 318ms/step - loss: 1.0030 - val_loss: 0.9257\n",
            "Epoch 62/500\n",
            "52/52 [==============================] - 17s 319ms/step - loss: 1.0030 - val_loss: 0.9262\n",
            "Epoch 63/500\n",
            "52/52 [==============================] - 18s 343ms/step - loss: 1.0030 - val_loss: 0.9264\n",
            "Epoch 64/500\n",
            "52/52 [==============================] - 16s 317ms/step - loss: 1.0028 - val_loss: 0.9264\n",
            "Epoch 65/500\n",
            "52/52 [==============================] - 17s 324ms/step - loss: 1.0030 - val_loss: 0.9260\n",
            "Epoch 66/500\n",
            "52/52 [==============================] - 18s 357ms/step - loss: 1.0027 - val_loss: 0.9262\n",
            "Epoch 67/500\n",
            "52/52 [==============================] - 18s 341ms/step - loss: 1.0025 - val_loss: 0.9265\n",
            "Epoch 68/500\n",
            "52/52 [==============================] - 17s 326ms/step - loss: 683849.2500 - val_loss: 0.9339\n",
            "Epoch 69/500\n",
            "52/52 [==============================] - 18s 334ms/step - loss: 1.0094 - val_loss: 0.9301\n",
            "Epoch 70/500\n",
            "52/52 [==============================] - 16s 312ms/step - loss: 1.0089 - val_loss: 0.9295\n",
            "Epoch 71/500\n",
            "52/52 [==============================] - 17s 323ms/step - loss: 1.0085 - val_loss: 0.9293\n",
            "Epoch 72/500\n",
            "52/52 [==============================] - 18s 338ms/step - loss: 1.0083 - val_loss: 0.9292\n",
            "Epoch 73/500\n",
            "52/52 [==============================] - 16s 313ms/step - loss: 1.0082 - val_loss: 0.9291\n",
            "Epoch 74/500\n",
            "52/52 [==============================] - 16s 312ms/step - loss: 1.0082 - val_loss: 0.9291\n",
            "Epoch 75/500\n",
            "52/52 [==============================] - 18s 351ms/step - loss: 1.0081 - val_loss: 0.9287\n",
            "Epoch 76/500\n",
            "52/52 [==============================] - 17s 337ms/step - loss: 1.0080 - val_loss: 0.9286\n",
            "Epoch 77/500\n",
            "52/52 [==============================] - 18s 346ms/step - loss: 1.0078 - val_loss: 0.9283\n",
            "Epoch 78/500\n",
            "52/52 [==============================] - 18s 348ms/step - loss: 1.0076 - val_loss: 0.9279\n",
            "Epoch 79/500\n",
            "52/52 [==============================] - 16s 318ms/step - loss: 1.0075 - val_loss: 0.9271\n",
            "Epoch 80/500\n",
            "52/52 [==============================] - 18s 344ms/step - loss: 1.0071 - val_loss: 5.0939\n",
            "Epoch 81/500\n",
            "52/52 [==============================] - 16s 315ms/step - loss: 1.0084 - val_loss: 0.9287\n",
            "Epoch 82/500\n",
            "52/52 [==============================] - 16s 313ms/step - loss: 1.0075 - val_loss: 0.9284\n",
            "Epoch 83/500\n",
            "52/52 [==============================] - 17s 327ms/step - loss: 1.0075 - val_loss: 0.9283\n",
            "Epoch 84/500\n",
            "52/52 [==============================] - 17s 318ms/step - loss: 1.0075 - val_loss: 0.9282\n",
            "Epoch 85/500\n",
            "52/52 [==============================] - 16s 313ms/step - loss: 1.0074 - val_loss: 0.9282\n",
            "Epoch 86/500\n",
            "52/52 [==============================] - 17s 329ms/step - loss: 1.0075 - val_loss: 0.9281\n",
            "Epoch 87/500\n",
            "52/52 [==============================] - 17s 330ms/step - loss: 1.0074 - val_loss: 0.9281\n",
            "Epoch 88/500\n",
            "52/52 [==============================] - 16s 314ms/step - loss: 1.0073 - val_loss: 0.9279\n",
            "Epoch 89/500\n",
            "52/52 [==============================] - 17s 326ms/step - loss: 1.0074 - val_loss: 0.9280\n",
            "Epoch 90/500\n",
            "52/52 [==============================] - 17s 319ms/step - loss: 1.0072 - val_loss: 0.9279\n",
            "Epoch 91/500\n",
            "52/52 [==============================] - 16s 313ms/step - loss: 1.0073 - val_loss: 0.9279\n",
            "Epoch 92/500\n",
            "52/52 [==============================] - 16s 313ms/step - loss: 1.0071 - val_loss: 0.9278\n",
            "Epoch 93/500\n",
            "52/52 [==============================] - 17s 330ms/step - loss: 1.0073 - val_loss: 0.9277\n",
            "Epoch 94/500\n",
            "52/52 [==============================] - 16s 311ms/step - loss: 1.0072 - val_loss: 0.9274\n",
            "Epoch 95/500\n",
            "52/52 [==============================] - 16s 310ms/step - loss: 1.0068 - val_loss: 0.9263\n",
            "Epoch 96/500\n",
            "52/52 [==============================] - 17s 336ms/step - loss: 68388.2188 - val_loss: 0.9315\n",
            "Epoch 97/500\n",
            "52/52 [==============================] - 16s 314ms/step - loss: 1.0079 - val_loss: 0.9290\n",
            "Epoch 98/500\n",
            "52/52 [==============================] - 16s 313ms/step - loss: 1.0079 - val_loss: 0.9291\n",
            "Epoch 99/500\n",
            "52/52 [==============================] - 17s 337ms/step - loss: 1.0077 - val_loss: 0.9289\n",
            "Epoch 100/500\n",
            "52/52 [==============================] - 16s 313ms/step - loss: 1.0076 - val_loss: 0.9289\n",
            "Epoch 101/500\n",
            "52/52 [==============================] - 16s 313ms/step - loss: 1.0076 - val_loss: 0.9288\n",
            "Epoch 102/500\n",
            "52/52 [==============================] - 17s 331ms/step - loss: 1.0075 - val_loss: 0.9288\n",
            "Epoch 103/500\n",
            "52/52 [==============================] - 17s 318ms/step - loss: 1.0075 - val_loss: 0.9288\n",
            "Epoch 104/500\n",
            "52/52 [==============================] - 16s 310ms/step - loss: 1.0075 - val_loss: 0.9287\n",
            "Epoch 105/500\n",
            "52/52 [==============================] - 16s 311ms/step - loss: 1.0074 - val_loss: 0.9287\n",
            "Epoch 106/500\n",
            "52/52 [==============================] - 17s 330ms/step - loss: 1.0075 - val_loss: 0.9287\n",
            "Epoch 107/500\n",
            "52/52 [==============================] - 16s 314ms/step - loss: 1.0076 - val_loss: 0.9287\n",
            "Epoch 108/500\n",
            "52/52 [==============================] - 16s 313ms/step - loss: 1.0075 - val_loss: 0.9287\n",
            "Epoch 109/500\n",
            "52/52 [==============================] - 17s 333ms/step - loss: 1.0075 - val_loss: 0.9286\n",
            "Epoch 110/500\n",
            "52/52 [==============================] - 16s 312ms/step - loss: 1.0074 - val_loss: 0.9286\n",
            "Epoch 111/500\n",
            "52/52 [==============================] - 16s 311ms/step - loss: 1.0074 - val_loss: 0.9286\n",
            "Epoch 112/500\n",
            "52/52 [==============================] - 17s 336ms/step - loss: 1.0073 - val_loss: 0.9285\n",
            "Epoch 113/500\n",
            "52/52 [==============================] - 16s 313ms/step - loss: 1.0074 - val_loss: 0.9285\n",
            "Epoch 114/500\n",
            "52/52 [==============================] - 16s 313ms/step - loss: 1.0074 - val_loss: 0.9285\n",
            "Epoch 115/500\n",
            "52/52 [==============================] - 17s 329ms/step - loss: 1.0074 - val_loss: 0.9285\n",
            "Epoch 116/500\n",
            "52/52 [==============================] - 16s 314ms/step - loss: 1.0073 - val_loss: 0.9285\n",
            "Epoch 117/500\n",
            "52/52 [==============================] - 16s 312ms/step - loss: 1.0073 - val_loss: 0.9285\n",
            "Epoch 118/500\n",
            "52/52 [==============================] - 16s 313ms/step - loss: 1.0073 - val_loss: 0.9285\n",
            "Epoch 119/500\n",
            "52/52 [==============================] - 17s 328ms/step - loss: 1.0073 - val_loss: 0.9285\n",
            "Epoch 120/500\n",
            "52/52 [==============================] - 16s 311ms/step - loss: 1.0074 - val_loss: 0.9284\n",
            "Epoch 121/500\n",
            "52/52 [==============================] - 16s 312ms/step - loss: 1.0073 - val_loss: 0.9284\n",
            "Epoch 122/500\n",
            "52/52 [==============================] - 17s 336ms/step - loss: 1.0072 - val_loss: 0.9284\n",
            "Epoch 123/500\n",
            "52/52 [==============================] - 16s 314ms/step - loss: 1.0072 - val_loss: 0.9284\n",
            "Epoch 124/500\n",
            "52/52 [==============================] - 17s 321ms/step - loss: 1.0072 - val_loss: 0.9282\n",
            "Epoch 125/500\n",
            "52/52 [==============================] - 18s 350ms/step - loss: 1.0072 - val_loss: 0.9283\n",
            "Epoch 126/500\n",
            "52/52 [==============================] - 17s 331ms/step - loss: 1.0073 - val_loss: 0.9283\n",
            "Epoch 127/500\n",
            "52/52 [==============================] - 17s 323ms/step - loss: 1.0072 - val_loss: 0.9283\n",
            "Epoch 128/500\n",
            "52/52 [==============================] - 18s 350ms/step - loss: 1.0071 - val_loss: 0.9282\n",
            "Epoch 129/500\n",
            "52/52 [==============================] - 17s 325ms/step - loss: 1.0072 - val_loss: 0.9282\n",
            "Epoch 130/500\n",
            "52/52 [==============================] - 16s 314ms/step - loss: 1.0072 - val_loss: 0.9282\n",
            "Epoch 131/500\n",
            "52/52 [==============================] - 17s 335ms/step - loss: 1.0070 - val_loss: 0.9282\n",
            "Epoch 132/500\n",
            "52/52 [==============================] - 16s 312ms/step - loss: 1.0070 - val_loss: 0.9282\n",
            "Epoch 133/500\n",
            "52/52 [==============================] - 16s 312ms/step - loss: 1.0070 - val_loss: 0.9281\n",
            "Epoch 134/500\n",
            "52/52 [==============================] - 18s 340ms/step - loss: 1.0071 - val_loss: 0.9281\n",
            "Epoch 135/500\n",
            "52/52 [==============================] - 19s 360ms/step - loss: 1.0071 - val_loss: 0.9281\n",
            "Epoch 136/500\n",
            "52/52 [==============================] - 22s 427ms/step - loss: 1.0068 - val_loss: 0.9280\n",
            "Epoch 137/500\n",
            "52/52 [==============================] - 20s 383ms/step - loss: 1.0071 - val_loss: 0.9280\n",
            "Epoch 138/500\n",
            "52/52 [==============================] - 21s 400ms/step - loss: 1.0070 - val_loss: 0.9280\n",
            "Epoch 139/500\n",
            "52/52 [==============================] - 20s 385ms/step - loss: 1.0069 - val_loss: 0.9280\n",
            "Epoch 140/500\n",
            "52/52 [==============================] - 21s 404ms/step - loss: 1.0068 - val_loss: 0.9279\n",
            "Epoch 141/500\n",
            "52/52 [==============================] - 21s 413ms/step - loss: 1.0068 - val_loss: 0.9278\n",
            "Epoch 142/500\n",
            "52/52 [==============================] - 22s 420ms/step - loss: 1.0069 - val_loss: 0.9277\n",
            "Epoch 143/500\n",
            "52/52 [==============================] - 25s 485ms/step - loss: 1.0066 - val_loss: 0.9276\n",
            "Epoch 144/500\n",
            "52/52 [==============================] - 24s 469ms/step - loss: 1.0074 - val_loss: 0.9271\n",
            "Epoch 145/500\n",
            "52/52 [==============================] - 26s 510ms/step - loss: 1.0070 - val_loss: 0.9278\n",
            "Epoch 146/500\n",
            "52/52 [==============================] - 25s 484ms/step - loss: 1.0069 - val_loss: 0.9279\n",
            "Epoch 147/500\n",
            "52/52 [==============================] - 23s 446ms/step - loss: 1.0070 - val_loss: 0.9278\n",
            "Epoch 148/500\n",
            "52/52 [==============================] - 22s 426ms/step - loss: 1.0067 - val_loss: 0.9279\n",
            "Epoch 149/500\n",
            "52/52 [==============================] - 24s 454ms/step - loss: 1.0067 - val_loss: 0.9278\n",
            "Epoch 150/500\n",
            "52/52 [==============================] - 26s 494ms/step - loss: 1.0068 - val_loss: 0.9278\n",
            "Epoch 151/500\n",
            "52/52 [==============================] - 24s 471ms/step - loss: 1.0068 - val_loss: 0.9278\n",
            "Epoch 152/500\n",
            "52/52 [==============================] - 21s 409ms/step - loss: 1.0068 - val_loss: 0.9278\n",
            "Epoch 153/500\n",
            "52/52 [==============================] - 23s 449ms/step - loss: 1.0068 - val_loss: 0.9278\n",
            "Epoch 154/500\n",
            "52/52 [==============================] - 24s 457ms/step - loss: 1.0068 - val_loss: 0.9277\n",
            "Epoch 155/500\n",
            "52/52 [==============================] - 25s 479ms/step - loss: 1.0068 - val_loss: 0.9277\n",
            "Epoch 156/500\n",
            "52/52 [==============================] - 25s 475ms/step - loss: 1.0067 - val_loss: 0.9277\n",
            "Epoch 157/500\n",
            "52/52 [==============================] - 25s 483ms/step - loss: 1.0067 - val_loss: 0.9277\n",
            "Epoch 158/500\n",
            "52/52 [==============================] - 24s 462ms/step - loss: 1.0067 - val_loss: 0.9276\n",
            "Epoch 159/500\n",
            "52/52 [==============================] - 23s 430ms/step - loss: 1.0068 - val_loss: 0.9276\n",
            "Epoch 160/500\n",
            "52/52 [==============================] - 24s 455ms/step - loss: 1.0067 - val_loss: 0.9276\n",
            "Epoch 161/500\n",
            "52/52 [==============================] - 23s 445ms/step - loss: 1.0066 - val_loss: 0.9276\n",
            "Epoch 162/500\n",
            "52/52 [==============================] - 22s 428ms/step - loss: 1.0067 - val_loss: 0.9276\n",
            "Epoch 163/500\n",
            "52/52 [==============================] - 23s 432ms/step - loss: 1.0067 - val_loss: 0.9275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='Training loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "ZM-6DJxRN_go",
        "outputId": "b8e5d2a2-e6dd-48b7-a575-c70fb838c45c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "np.random.seed(21)\n",
        "tf.random.set_seed(21)\n",
        "\n",
        "def create_sequences(X, y, time_steps=100):\n",
        "    X_out, y_out = [], []\n",
        "    for i in range(len(X)-time_steps):\n",
        "        X_out.append(X.iloc[i:(i+time_steps)].values)\n",
        "        y_out.append(y.iloc[i+time_steps])\n",
        "\n",
        "    return np.array(X_out), np.array(y_out)\n",
        "\n",
        "df = pd.read_excel('/content/sample_data/mc4 GSM 3 month.xlsx')\n",
        "df = df[['Date','RSSI']]\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df = df.dropna()\n",
        "\n",
        "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "scaler = scaler.fit(np.array(train['RSSI']).reshape(-1,1))\n",
        "\n",
        "train['RSSI'] = scaler.transform(np.array(train['RSSI']).reshape(-1,1))\n",
        "test['RSSI'] = scaler.transform(np.array(test['RSSI']).reshape(-1,1))\n",
        "\n",
        "X_train, y_train = create_sequences(train[['RSSI']], train['RSSI'])\n",
        "X_test, y_test = create_sequences(test[['RSSI']], test['RSSI'])\n",
        "\n",
        "print(\"Training input shape: \", X_train.shape)\n",
        "print(\"Testing input shape: \", X_test.shape)\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    min_delta=0.001,\n",
        "    patience=50,\n",
        "    restore_best_weights=True,\n",
        ")\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256, activation='relu', return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss='mse')\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=200,\n",
        "                    batch_size=64,\n",
        "                    validation_split=0.1,\n",
        "                    callbacks=[early_stopping],\n",
        "                    verbose=1,\n",
        "                    shuffle=False)\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b9q8Pzpt4dF8",
        "outputId": "91a26176-30d7-48a4-892f-7b2937990c14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training input shape:  (3623, 100, 1)\n",
            "Testing input shape:  (831, 100, 1)\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_18 (LSTM)              (None, 100, 256)          264192    \n",
            "                                                                 \n",
            " dropout_18 (Dropout)        (None, 100, 256)          0         \n",
            "                                                                 \n",
            " lstm_19 (LSTM)              (None, 100, 256)          525312    \n",
            "                                                                 \n",
            " dropout_19 (Dropout)        (None, 100, 256)          0         \n",
            "                                                                 \n",
            " lstm_20 (LSTM)              (None, 256)               525312    \n",
            "                                                                 \n",
            " dropout_20 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1315073 (5.02 MB)\n",
            "Trainable params: 1315073 (5.02 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "51/51 [==============================] - 130s 2s/step - loss: 1.0100 - val_loss: 0.9310\n",
            "Epoch 2/200\n",
            "51/51 [==============================] - 117s 2s/step - loss: 1.0096 - val_loss: 0.9311\n",
            "Epoch 3/200\n",
            "51/51 [==============================] - 117s 2s/step - loss: 1.0093 - val_loss: 0.9309\n",
            "Epoch 4/200\n",
            "51/51 [==============================] - 114s 2s/step - loss: 1.0091 - val_loss: 0.9310\n",
            "Epoch 5/200\n",
            "51/51 [==============================] - 109s 2s/step - loss: 1.0090 - val_loss: 0.9310\n",
            "Epoch 6/200\n",
            "51/51 [==============================] - 110s 2s/step - loss: 1.0089 - val_loss: 0.9310\n",
            "Epoch 7/200\n",
            "51/51 [==============================] - 102s 2s/step - loss: 1.0084 - val_loss: 0.9309\n",
            "Epoch 8/200\n",
            "51/51 [==============================] - 102s 2s/step - loss: 1.0081 - val_loss: 0.9315\n",
            "Epoch 9/200\n",
            "51/51 [==============================] - 103s 2s/step - loss: 1.0080 - val_loss: 0.9321\n",
            "Epoch 10/200\n",
            "51/51 [==============================] - 101s 2s/step - loss: 1.0077 - val_loss: 0.9323\n",
            "Epoch 11/200\n",
            "51/51 [==============================] - 99s 2s/step - loss: 1.0073 - val_loss: 0.9324\n",
            "Epoch 12/200\n",
            "51/51 [==============================] - 99s 2s/step - loss: 1.0060 - val_loss: 0.9320\n",
            "Epoch 13/200\n",
            "51/51 [==============================] - 99s 2s/step - loss: 1.0069 - val_loss: 0.9335\n",
            "Epoch 14/200\n",
            "51/51 [==============================] - 91s 2s/step - loss: 1.0066 - val_loss: 0.9326\n",
            "Epoch 15/200\n",
            "51/51 [==============================] - 89s 2s/step - loss: 1.0064 - val_loss: 0.9294\n",
            "Epoch 16/200\n",
            "51/51 [==============================] - 89s 2s/step - loss: 1.0059 - val_loss: 0.9327\n",
            "Epoch 17/200\n",
            "51/51 [==============================] - 90s 2s/step - loss: 1.0050 - val_loss: 0.9321\n",
            "Epoch 18/200\n",
            "51/51 [==============================] - 89s 2s/step - loss: 1.0048 - val_loss: 0.9336\n",
            "Epoch 19/200\n",
            "51/51 [==============================] - 90s 2s/step - loss: 1.0048 - val_loss: 0.9334\n",
            "Epoch 20/200\n",
            "51/51 [==============================] - 89s 2s/step - loss: 1.0045 - val_loss: 0.9306\n",
            "Epoch 21/200\n",
            "51/51 [==============================] - 89s 2s/step - loss: 1.0042 - val_loss: 0.9309\n",
            "Epoch 22/200\n",
            "51/51 [==============================] - 91s 2s/step - loss: 1.0044 - val_loss: 0.9309\n",
            "Epoch 23/200\n",
            "51/51 [==============================] - 91s 2s/step - loss: 1.0034 - val_loss: 0.9323\n",
            "Epoch 24/200\n",
            "51/51 [==============================] - 91s 2s/step - loss: 1.0030 - val_loss: 0.9310\n",
            "Epoch 25/200\n",
            "51/51 [==============================] - 90s 2s/step - loss: 1.0022 - val_loss: 0.9326\n",
            "Epoch 26/200\n",
            "51/51 [==============================] - 92s 2s/step - loss: 1.0018 - val_loss: 0.9323\n",
            "Epoch 27/200\n",
            "51/51 [==============================] - 92s 2s/step - loss: 1.0006 - val_loss: 0.9307\n",
            "Epoch 28/200\n",
            "51/51 [==============================] - 96s 2s/step - loss: 1.0011 - val_loss: 0.9357\n",
            "Epoch 29/200\n",
            "51/51 [==============================] - 106s 2s/step - loss: 1.0009 - val_loss: 0.9338\n",
            "Epoch 30/200\n",
            "51/51 [==============================] - 98s 2s/step - loss: 1.0009 - val_loss: 0.9332\n",
            "Epoch 31/200\n",
            "51/51 [==============================] - 94s 2s/step - loss: 0.9997 - val_loss: 0.9351\n",
            "Epoch 32/200\n",
            "51/51 [==============================] - 97s 2s/step - loss: 0.9972 - val_loss: 0.9321\n",
            "Epoch 33/200\n",
            "51/51 [==============================] - 97s 2s/step - loss: nan - val_loss: nan\n",
            "Epoch 34/200\n",
            "51/51 [==============================] - 96s 2s/step - loss: nan - val_loss: nan\n",
            "Epoch 35/200\n",
            "51/51 [==============================] - 92s 2s/step - loss: nan - val_loss: nan\n",
            "Epoch 36/200\n",
            "51/51 [==============================] - 91s 2s/step - loss: nan - val_loss: nan\n",
            "Epoch 37/200\n",
            "51/51 [==============================] - 91s 2s/step - loss: nan - val_loss: nan\n",
            "Epoch 38/200\n",
            "51/51 [==============================] - 91s 2s/step - loss: nan - val_loss: nan\n",
            "Epoch 39/200\n",
            "51/51 [==============================] - 93s 2s/step - loss: nan - val_loss: nan\n",
            "Epoch 40/200\n",
            " 9/51 [====>.........................] - ETA: 1:11 - loss: nan"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-7746ff592a5d>\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m history = model.fit(X_train, y_train,\n\u001b[0m\u001b[1;32m     58\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CR8TQZ744ZC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AROwChx8N_ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "imvuyt1jK-Zf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}