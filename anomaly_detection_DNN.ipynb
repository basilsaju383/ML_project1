{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0qqMQxL7fgL/NAKfntZUd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/basilsaju383/ML_project1/blob/main/anomaly_detection_DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GErYZlNKWSh8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data from Excel\n",
        "data = pd.read_excel('/content/sample_data/mc4 GSM 3 month.xlsx')"
      ],
      "metadata": {
        "id": "I7xMYqdNWclE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pE-beo8Wcne",
        "outputId": "6a529ff3-1b60-4e72-afca-40531f7fcbdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Day', 'Date', 'Time', 'Device Name', 'Location', 'GPS Location',\n",
              "       'Aggregation', 'RSRP', 'RSRQ', 'RSSI', 'SNR'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = data['RSSI']\n",
        "df = df.dropna()\n",
        "df"
      ],
      "metadata": {
        "id": "N0GaZQmYWcp4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20b0b62f-1dc9-4b50-e463-ed1c9a13230c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1918     -61.000000\n",
              "1925     -62.000000\n",
              "1941     -63.000000\n",
              "1956     -66.000000\n",
              "1971     -62.000000\n",
              "            ...    \n",
              "132303   -63.000000\n",
              "132318   -65.000000\n",
              "132333   -63.000000\n",
              "132349   -66.000000\n",
              "132660   -64.349377\n",
              "Name: RSSI, Length: 4655, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test = train_test_split(df, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "884UBOOlWcsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.values.reshape((-1, 1))\n",
        "X_test = X_test.values.reshape((-1, 1))\n"
      ],
      "metadata": {
        "id": "ClHToK7Hc5OZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = keras.Sequential([\n",
        "    layers.BatchNormalization(input_shape=(X_train.shape[1],)),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(1),\n",
        "])"
      ],
      "metadata": {
        "id": "U6-ASpfiWcvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.compile(\n",
        "    optimizer='sgd',\n",
        "    loss='mae',\n",
        "    metrics=['mae'],\n",
        ")\n",
        "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
        "EPOCHS = 500\n",
        "history = model.fit(\n",
        "    X_train, X_train,\n",
        "    validation_data=(X_test, X_test),\n",
        "    batch_size=64,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[checkpoint],\n",
        "    verbose=1,\n",
        ")"
      ],
      "metadata": {
        "id": "0jLx1cVgWcxk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfb8e78a-7a85-42ae-c072-7a79d4721ee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 63.4769 - mae: 63.4769\n",
            "Epoch 1: val_loss improved from inf to 62.46822, saving model to best_model.h5\n",
            "59/59 [==============================] - 2s 14ms/step - loss: 63.4734 - mae: 63.4734 - val_loss: 62.4682 - val_mae: 62.4682\n",
            "Epoch 2/500\n",
            "13/59 [=====>........................] - ETA: 0s - loss: 62.0876 - mae: 62.0876"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55/59 [==========================>...] - ETA: 0s - loss: 60.7233 - mae: 60.7233\n",
            "Epoch 2: val_loss improved from 62.46822 to 58.75595, saving model to best_model.h5\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 60.5924 - mae: 60.5924 - val_loss: 58.7559 - val_mae: 58.7559\n",
            "Epoch 3/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 53.6903 - mae: 53.6903\n",
            "Epoch 3: val_loss improved from 58.75595 to 47.56165, saving model to best_model.h5\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 53.1954 - mae: 53.1954 - val_loss: 47.5617 - val_mae: 47.5617\n",
            "Epoch 4/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 31.5063 - mae: 31.5063\n",
            "Epoch 4: val_loss improved from 47.56165 to 31.62151, saving model to best_model.h5\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 31.5063 - mae: 31.5063 - val_loss: 31.6215 - val_mae: 31.6215\n",
            "Epoch 5/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 4.6192 - mae: 4.6192\n",
            "Epoch 5: val_loss improved from 31.62151 to 1.35431, saving model to best_model.h5\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 4.6127 - mae: 4.6127 - val_loss: 1.3543 - val_mae: 1.3543\n",
            "Epoch 6/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 2.1068 - mae: 2.1068\n",
            "Epoch 6: val_loss did not improve from 1.35431\n",
            "59/59 [==============================] - 1s 9ms/step - loss: 2.0896 - mae: 2.0896 - val_loss: 1.9204 - val_mae: 1.9204\n",
            "Epoch 7/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 1.6717 - mae: 1.6717\n",
            "Epoch 7: val_loss improved from 1.35431 to 1.23344, saving model to best_model.h5\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 1.6689 - mae: 1.6689 - val_loss: 1.2334 - val_mae: 1.2334\n",
            "Epoch 8/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 1.2825 - mae: 1.2825\n",
            "Epoch 8: val_loss improved from 1.23344 to 1.13282, saving model to best_model.h5\n",
            "59/59 [==============================] - 1s 18ms/step - loss: 1.2904 - mae: 1.2904 - val_loss: 1.1328 - val_mae: 1.1328\n",
            "Epoch 9/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 1.2963 - mae: 1.2963\n",
            "Epoch 9: val_loss did not improve from 1.13282\n",
            "59/59 [==============================] - 1s 16ms/step - loss: 1.3074 - mae: 1.3074 - val_loss: 1.2647 - val_mae: 1.2647\n",
            "Epoch 10/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 1.2143 - mae: 1.2143\n",
            "Epoch 10: val_loss did not improve from 1.13282\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 1.2181 - mae: 1.2181 - val_loss: 1.2083 - val_mae: 1.2083\n",
            "Epoch 11/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 1.1443 - mae: 1.1443\n",
            "Epoch 11: val_loss did not improve from 1.13282\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 1.1443 - mae: 1.1443 - val_loss: 1.8977 - val_mae: 1.8977\n",
            "Epoch 12/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 1.0575 - mae: 1.0575\n",
            "Epoch 12: val_loss improved from 1.13282 to 0.92798, saving model to best_model.h5\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 1.0604 - mae: 1.0604 - val_loss: 0.9280 - val_mae: 0.9280\n",
            "Epoch 13/500\n",
            "53/59 [=========================>....] - ETA: 0s - loss: 0.9743 - mae: 0.9743\n",
            "Epoch 13: val_loss did not improve from 0.92798\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.9895 - mae: 0.9895 - val_loss: 1.0657 - val_mae: 1.0657\n",
            "Epoch 14/500\n",
            "53/59 [=========================>....] - ETA: 0s - loss: 1.0405 - mae: 1.0405\n",
            "Epoch 14: val_loss did not improve from 0.92798\n",
            "59/59 [==============================] - 1s 9ms/step - loss: 1.0484 - mae: 1.0484 - val_loss: 1.3358 - val_mae: 1.3358\n",
            "Epoch 15/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 1.0819 - mae: 1.0819\n",
            "Epoch 15: val_loss improved from 0.92798 to 0.69472, saving model to best_model.h5\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 1.0598 - mae: 1.0598 - val_loss: 0.6947 - val_mae: 0.6947\n",
            "Epoch 16/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.9137 - mae: 0.9137\n",
            "Epoch 16: val_loss did not improve from 0.69472\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.9158 - mae: 0.9158 - val_loss: 1.2643 - val_mae: 1.2643\n",
            "Epoch 17/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.9006 - mae: 0.9006\n",
            "Epoch 17: val_loss did not improve from 0.69472\n",
            "59/59 [==============================] - 1s 9ms/step - loss: 0.9101 - mae: 0.9101 - val_loss: 2.1737 - val_mae: 2.1737\n",
            "Epoch 18/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.9371 - mae: 0.9371\n",
            "Epoch 18: val_loss did not improve from 0.69472\n",
            "59/59 [==============================] - 1s 9ms/step - loss: 0.9260 - mae: 0.9260 - val_loss: 1.1922 - val_mae: 1.1922\n",
            "Epoch 19/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.9644 - mae: 0.9644\n",
            "Epoch 19: val_loss did not improve from 0.69472\n",
            "59/59 [==============================] - 1s 9ms/step - loss: 0.9591 - mae: 0.9591 - val_loss: 1.1546 - val_mae: 1.1546\n",
            "Epoch 20/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8943 - mae: 0.8943\n",
            "Epoch 20: val_loss did not improve from 0.69472\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.8943 - mae: 0.8943 - val_loss: 1.2828 - val_mae: 1.2828\n",
            "Epoch 21/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.9399 - mae: 0.9399\n",
            "Epoch 21: val_loss did not improve from 0.69472\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.9364 - mae: 0.9364 - val_loss: 1.7137 - val_mae: 1.7137\n",
            "Epoch 22/500\n",
            "53/59 [=========================>....] - ETA: 0s - loss: 0.8845 - mae: 0.8845\n",
            "Epoch 22: val_loss did not improve from 0.69472\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.8910 - mae: 0.8910 - val_loss: 1.1202 - val_mae: 1.1202\n",
            "Epoch 23/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8757 - mae: 0.8757\n",
            "Epoch 23: val_loss did not improve from 0.69472\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.8757 - mae: 0.8757 - val_loss: 1.4118 - val_mae: 1.4118\n",
            "Epoch 24/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.8924 - mae: 0.8924\n",
            "Epoch 24: val_loss did not improve from 0.69472\n",
            "59/59 [==============================] - 1s 9ms/step - loss: 0.8812 - mae: 0.8812 - val_loss: 1.5148 - val_mae: 1.5148\n",
            "Epoch 25/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.9183 - mae: 0.9183\n",
            "Epoch 25: val_loss did not improve from 0.69472\n",
            "59/59 [==============================] - 1s 9ms/step - loss: 0.9100 - mae: 0.9100 - val_loss: 1.1203 - val_mae: 1.1203\n",
            "Epoch 26/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.8524 - mae: 0.8524\n",
            "Epoch 26: val_loss improved from 0.69472 to 0.67414, saving model to best_model.h5\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.8550 - mae: 0.8550 - val_loss: 0.6741 - val_mae: 0.6741\n",
            "Epoch 27/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.8568 - mae: 0.8568\n",
            "Epoch 27: val_loss did not improve from 0.67414\n",
            "59/59 [==============================] - 1s 13ms/step - loss: 0.8486 - mae: 0.8486 - val_loss: 0.9446 - val_mae: 0.9446\n",
            "Epoch 28/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.8722 - mae: 0.8722\n",
            "Epoch 28: val_loss did not improve from 0.67414\n",
            "59/59 [==============================] - 1s 16ms/step - loss: 0.8677 - mae: 0.8677 - val_loss: 1.8732 - val_mae: 1.8732\n",
            "Epoch 29/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.8695 - mae: 0.8695\n",
            "Epoch 29: val_loss did not improve from 0.67414\n",
            "59/59 [==============================] - 1s 17ms/step - loss: 0.8689 - mae: 0.8689 - val_loss: 1.3076 - val_mae: 1.3076\n",
            "Epoch 30/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.8107 - mae: 0.8107\n",
            "Epoch 30: val_loss did not improve from 0.67414\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.8041 - mae: 0.8041 - val_loss: 1.1890 - val_mae: 1.1890\n",
            "Epoch 31/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.8510 - mae: 0.8510\n",
            "Epoch 31: val_loss did not improve from 0.67414\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.8543 - mae: 0.8543 - val_loss: 1.4959 - val_mae: 1.4959\n",
            "Epoch 32/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.8780 - mae: 0.8780\n",
            "Epoch 32: val_loss did not improve from 0.67414\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.8674 - mae: 0.8674 - val_loss: 1.7733 - val_mae: 1.7733\n",
            "Epoch 33/500\n",
            "53/59 [=========================>....] - ETA: 0s - loss: 0.8263 - mae: 0.8263\n",
            "Epoch 33: val_loss did not improve from 0.67414\n",
            "59/59 [==============================] - 1s 9ms/step - loss: 0.8389 - mae: 0.8389 - val_loss: 1.5632 - val_mae: 1.5632\n",
            "Epoch 34/500\n",
            "53/59 [=========================>....] - ETA: 0s - loss: 0.8461 - mae: 0.8461\n",
            "Epoch 34: val_loss improved from 0.67414 to 0.58244, saving model to best_model.h5\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.8448 - mae: 0.8448 - val_loss: 0.5824 - val_mae: 0.5824\n",
            "Epoch 35/500\n",
            "53/59 [=========================>....] - ETA: 0s - loss: 0.8342 - mae: 0.8342\n",
            "Epoch 35: val_loss improved from 0.58244 to 0.37681, saving model to best_model.h5\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.8379 - mae: 0.8379 - val_loss: 0.3768 - val_mae: 0.3768\n",
            "Epoch 36/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.7817 - mae: 0.7817\n",
            "Epoch 36: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7832 - mae: 0.7832 - val_loss: 1.3290 - val_mae: 1.3290\n",
            "Epoch 37/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.8188 - mae: 0.8188\n",
            "Epoch 37: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 9ms/step - loss: 0.8247 - mae: 0.8247 - val_loss: 1.2277 - val_mae: 1.2277\n",
            "Epoch 38/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.7681 - mae: 0.7681\n",
            "Epoch 38: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 9ms/step - loss: 0.7648 - mae: 0.7648 - val_loss: 0.9087 - val_mae: 0.9087\n",
            "Epoch 39/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.8738 - mae: 0.8738\n",
            "Epoch 39: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 9ms/step - loss: 0.8621 - mae: 0.8621 - val_loss: 1.2724 - val_mae: 1.2724\n",
            "Epoch 40/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.8086 - mae: 0.8086\n",
            "Epoch 40: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.8096 - mae: 0.8096 - val_loss: 1.8215 - val_mae: 1.8215\n",
            "Epoch 41/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.8172 - mae: 0.8172\n",
            "Epoch 41: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.8273 - mae: 0.8273 - val_loss: 1.1462 - val_mae: 1.1462\n",
            "Epoch 42/500\n",
            "53/59 [=========================>....] - ETA: 0s - loss: 0.8306 - mae: 0.8306\n",
            "Epoch 42: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.8172 - mae: 0.8172 - val_loss: 1.8199 - val_mae: 1.8199\n",
            "Epoch 43/500\n",
            "53/59 [=========================>....] - ETA: 0s - loss: 0.7751 - mae: 0.7751\n",
            "Epoch 43: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 9ms/step - loss: 0.7690 - mae: 0.7690 - val_loss: 0.9169 - val_mae: 0.9169\n",
            "Epoch 44/500\n",
            "53/59 [=========================>....] - ETA: 0s - loss: 0.7683 - mae: 0.7683\n",
            "Epoch 44: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7616 - mae: 0.7616 - val_loss: 0.9542 - val_mae: 0.9542\n",
            "Epoch 45/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.7917 - mae: 0.7917\n",
            "Epoch 45: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7934 - mae: 0.7934 - val_loss: 1.5439 - val_mae: 1.5439\n",
            "Epoch 46/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.7916 - mae: 0.7916\n",
            "Epoch 46: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 9ms/step - loss: 0.7960 - mae: 0.7960 - val_loss: 0.6282 - val_mae: 0.6282\n",
            "Epoch 47/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.7554 - mae: 0.7554\n",
            "Epoch 47: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 15ms/step - loss: 0.7527 - mae: 0.7527 - val_loss: 1.3513 - val_mae: 1.3513\n",
            "Epoch 48/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.8052 - mae: 0.8052\n",
            "Epoch 48: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 16ms/step - loss: 0.8018 - mae: 0.8018 - val_loss: 1.0425 - val_mae: 1.0425\n",
            "Epoch 49/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.7538 - mae: 0.7538\n",
            "Epoch 49: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 14ms/step - loss: 0.7520 - mae: 0.7520 - val_loss: 0.9633 - val_mae: 0.9633\n",
            "Epoch 50/500\n",
            "53/59 [=========================>....] - ETA: 0s - loss: 0.7422 - mae: 0.7422\n",
            "Epoch 50: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 9ms/step - loss: 0.7296 - mae: 0.7296 - val_loss: 1.6354 - val_mae: 1.6354\n",
            "Epoch 51/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.7824 - mae: 0.7824\n",
            "Epoch 51: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7817 - mae: 0.7817 - val_loss: 0.9623 - val_mae: 0.9623\n",
            "Epoch 52/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.7222 - mae: 0.7222\n",
            "Epoch 52: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7253 - mae: 0.7253 - val_loss: 0.4358 - val_mae: 0.4358\n",
            "Epoch 53/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.7868 - mae: 0.7868\n",
            "Epoch 53: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7820 - mae: 0.7820 - val_loss: 0.8711 - val_mae: 0.8711\n",
            "Epoch 54/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.7441 - mae: 0.7441\n",
            "Epoch 54: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7434 - mae: 0.7434 - val_loss: 1.3875 - val_mae: 1.3875\n",
            "Epoch 55/500\n",
            "53/59 [=========================>....] - ETA: 0s - loss: 0.7548 - mae: 0.7548\n",
            "Epoch 55: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 9ms/step - loss: 0.7498 - mae: 0.7498 - val_loss: 1.2219 - val_mae: 1.2219\n",
            "Epoch 56/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.7530 - mae: 0.7530\n",
            "Epoch 56: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7503 - mae: 0.7503 - val_loss: 0.5931 - val_mae: 0.5931\n",
            "Epoch 57/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.7353 - mae: 0.7353\n",
            "Epoch 57: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 9ms/step - loss: 0.7429 - mae: 0.7429 - val_loss: 1.2485 - val_mae: 1.2485\n",
            "Epoch 58/500\n",
            "53/59 [=========================>....] - ETA: 0s - loss: 0.7233 - mae: 0.7233\n",
            "Epoch 58: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 9ms/step - loss: 0.7182 - mae: 0.7182 - val_loss: 0.8123 - val_mae: 0.8123\n",
            "Epoch 59/500\n",
            "53/59 [=========================>....] - ETA: 0s - loss: 0.7175 - mae: 0.7175\n",
            "Epoch 59: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 9ms/step - loss: 0.7084 - mae: 0.7084 - val_loss: 0.9783 - val_mae: 0.9783\n",
            "Epoch 60/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.7269 - mae: 0.7269\n",
            "Epoch 60: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 9ms/step - loss: 0.7349 - mae: 0.7349 - val_loss: 1.3252 - val_mae: 1.3252\n",
            "Epoch 61/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.7622 - mae: 0.7622\n",
            "Epoch 61: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7662 - mae: 0.7662 - val_loss: 0.8465 - val_mae: 0.8465\n",
            "Epoch 62/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.7745 - mae: 0.7745\n",
            "Epoch 62: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7736 - mae: 0.7736 - val_loss: 1.3863 - val_mae: 1.3863\n",
            "Epoch 63/500\n",
            "53/59 [=========================>....] - ETA: 0s - loss: 0.7250 - mae: 0.7250\n",
            "Epoch 63: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7302 - mae: 0.7302 - val_loss: 0.7010 - val_mae: 0.7010\n",
            "Epoch 64/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7290 - mae: 0.7290\n",
            "Epoch 64: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7290 - mae: 0.7290 - val_loss: 1.6134 - val_mae: 1.6134\n",
            "Epoch 65/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.8003 - mae: 0.8003\n",
            "Epoch 65: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 9ms/step - loss: 0.7992 - mae: 0.7992 - val_loss: 1.1339 - val_mae: 1.1339\n",
            "Epoch 66/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.7581 - mae: 0.7581\n",
            "Epoch 66: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7564 - mae: 0.7564 - val_loss: 0.7827 - val_mae: 0.7827\n",
            "Epoch 67/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6879 - mae: 0.6879\n",
            "Epoch 67: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 16ms/step - loss: 0.6824 - mae: 0.6824 - val_loss: 0.4008 - val_mae: 0.4008\n",
            "Epoch 68/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.7461 - mae: 0.7461\n",
            "Epoch 68: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 15ms/step - loss: 0.7476 - mae: 0.7476 - val_loss: 0.6200 - val_mae: 0.6200\n",
            "Epoch 69/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.7572 - mae: 0.7572\n",
            "Epoch 69: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 14ms/step - loss: 0.7576 - mae: 0.7576 - val_loss: 0.7987 - val_mae: 0.7987\n",
            "Epoch 70/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7536 - mae: 0.7536\n",
            "Epoch 70: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7536 - mae: 0.7536 - val_loss: 0.7683 - val_mae: 0.7683\n",
            "Epoch 71/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7473 - mae: 0.7473\n",
            "Epoch 71: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7473 - mae: 0.7473 - val_loss: 1.1278 - val_mae: 1.1278\n",
            "Epoch 72/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.7453 - mae: 0.7453\n",
            "Epoch 72: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7428 - mae: 0.7428 - val_loss: 0.8239 - val_mae: 0.8239\n",
            "Epoch 73/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6602 - mae: 0.6602\n",
            "Epoch 73: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6753 - mae: 0.6753 - val_loss: 1.0079 - val_mae: 1.0079\n",
            "Epoch 74/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6827 - mae: 0.6827\n",
            "Epoch 74: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 13ms/step - loss: 0.6768 - mae: 0.6768 - val_loss: 1.2218 - val_mae: 1.2218\n",
            "Epoch 75/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.7076 - mae: 0.7076\n",
            "Epoch 75: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7226 - mae: 0.7226 - val_loss: 2.9257 - val_mae: 2.9257\n",
            "Epoch 76/500\n",
            "53/59 [=========================>....] - ETA: 0s - loss: 0.7629 - mae: 0.7629\n",
            "Epoch 76: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 9ms/step - loss: 0.7651 - mae: 0.7651 - val_loss: 0.7347 - val_mae: 0.7347\n",
            "Epoch 77/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.7408 - mae: 0.7408\n",
            "Epoch 77: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 9ms/step - loss: 0.7360 - mae: 0.7360 - val_loss: 0.8336 - val_mae: 0.8336\n",
            "Epoch 78/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.7302 - mae: 0.7302\n",
            "Epoch 78: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 12ms/step - loss: 0.7413 - mae: 0.7413 - val_loss: 0.5254 - val_mae: 0.5254\n",
            "Epoch 79/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6929 - mae: 0.6929\n",
            "Epoch 79: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6939 - mae: 0.6939 - val_loss: 1.0242 - val_mae: 1.0242\n",
            "Epoch 80/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6962 - mae: 0.6962\n",
            "Epoch 80: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7027 - mae: 0.7027 - val_loss: 0.9047 - val_mae: 0.9047\n",
            "Epoch 81/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.7420 - mae: 0.7420\n",
            "Epoch 81: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7439 - mae: 0.7439 - val_loss: 0.8767 - val_mae: 0.8767\n",
            "Epoch 82/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.7018 - mae: 0.7018\n",
            "Epoch 82: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7051 - mae: 0.7051 - val_loss: 0.7427 - val_mae: 0.7427\n",
            "Epoch 83/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.7442 - mae: 0.7442\n",
            "Epoch 83: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7468 - mae: 0.7468 - val_loss: 0.4282 - val_mae: 0.4282\n",
            "Epoch 84/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6731 - mae: 0.6731\n",
            "Epoch 84: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6748 - mae: 0.6748 - val_loss: 0.7892 - val_mae: 0.7892\n",
            "Epoch 85/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6883 - mae: 0.6883\n",
            "Epoch 85: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6957 - mae: 0.6957 - val_loss: 0.6281 - val_mae: 0.6281\n",
            "Epoch 86/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.7272 - mae: 0.7272\n",
            "Epoch 86: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 14ms/step - loss: 0.7290 - mae: 0.7290 - val_loss: 1.3953 - val_mae: 1.3953\n",
            "Epoch 87/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.7297 - mae: 0.7297\n",
            "Epoch 87: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 16ms/step - loss: 0.7242 - mae: 0.7242 - val_loss: 1.2033 - val_mae: 1.2033\n",
            "Epoch 88/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6932 - mae: 0.6932\n",
            "Epoch 88: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 16ms/step - loss: 0.6837 - mae: 0.6837 - val_loss: 1.0131 - val_mae: 1.0131\n",
            "Epoch 89/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.7085 - mae: 0.7085\n",
            "Epoch 89: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7108 - mae: 0.7108 - val_loss: 1.4031 - val_mae: 1.4031\n",
            "Epoch 90/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.7330 - mae: 0.7330\n",
            "Epoch 90: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7365 - mae: 0.7365 - val_loss: 0.4944 - val_mae: 0.4944\n",
            "Epoch 91/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6790 - mae: 0.6790\n",
            "Epoch 91: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6773 - mae: 0.6773 - val_loss: 0.4319 - val_mae: 0.4319\n",
            "Epoch 92/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6961 - mae: 0.6961\n",
            "Epoch 92: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6954 - mae: 0.6954 - val_loss: 1.0935 - val_mae: 1.0935\n",
            "Epoch 93/500\n",
            "53/59 [=========================>....] - ETA: 0s - loss: 0.7168 - mae: 0.7168\n",
            "Epoch 93: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7220 - mae: 0.7220 - val_loss: 0.7909 - val_mae: 0.7909\n",
            "Epoch 94/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6674 - mae: 0.6674\n",
            "Epoch 94: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6765 - mae: 0.6765 - val_loss: 1.2261 - val_mae: 1.2261\n",
            "Epoch 95/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.7300 - mae: 0.7300\n",
            "Epoch 95: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7296 - mae: 0.7296 - val_loss: 1.5242 - val_mae: 1.5242\n",
            "Epoch 96/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.7030 - mae: 0.7030\n",
            "Epoch 96: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7074 - mae: 0.7074 - val_loss: 0.6917 - val_mae: 0.6917\n",
            "Epoch 97/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6699 - mae: 0.6699\n",
            "Epoch 97: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6865 - mae: 0.6865 - val_loss: 1.6355 - val_mae: 1.6355\n",
            "Epoch 98/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.7199 - mae: 0.7199\n",
            "Epoch 98: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7249 - mae: 0.7249 - val_loss: 0.4786 - val_mae: 0.4786\n",
            "Epoch 99/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.7080 - mae: 0.7080\n",
            "Epoch 99: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7101 - mae: 0.7101 - val_loss: 1.0363 - val_mae: 1.0363\n",
            "Epoch 100/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.7026 - mae: 0.7026\n",
            "Epoch 100: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7027 - mae: 0.7027 - val_loss: 1.5950 - val_mae: 1.5950\n",
            "Epoch 101/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.7037 - mae: 0.7037\n",
            "Epoch 101: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7070 - mae: 0.7070 - val_loss: 0.5214 - val_mae: 0.5214\n",
            "Epoch 102/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6961 - mae: 0.6961\n",
            "Epoch 102: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6960 - mae: 0.6960 - val_loss: 1.0629 - val_mae: 1.0629\n",
            "Epoch 103/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6860 - mae: 0.6860\n",
            "Epoch 103: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6879 - mae: 0.6879 - val_loss: 1.1823 - val_mae: 1.1823\n",
            "Epoch 104/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6873 - mae: 0.6873\n",
            "Epoch 104: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6898 - mae: 0.6898 - val_loss: 0.6174 - val_mae: 0.6174\n",
            "Epoch 105/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.7210 - mae: 0.7210\n",
            "Epoch 105: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 12ms/step - loss: 0.7197 - mae: 0.7197 - val_loss: 0.4889 - val_mae: 0.4889\n",
            "Epoch 106/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6930 - mae: 0.6930\n",
            "Epoch 106: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 17ms/step - loss: 0.6885 - mae: 0.6885 - val_loss: 1.5512 - val_mae: 1.5512\n",
            "Epoch 107/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.7155 - mae: 0.7155\n",
            "Epoch 107: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 16ms/step - loss: 0.7155 - mae: 0.7155 - val_loss: 1.5118 - val_mae: 1.5118\n",
            "Epoch 108/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.7235 - mae: 0.7235\n",
            "Epoch 108: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 12ms/step - loss: 0.7235 - mae: 0.7235 - val_loss: 0.8589 - val_mae: 0.8589\n",
            "Epoch 109/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6726 - mae: 0.6726\n",
            "Epoch 109: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6726 - mae: 0.6726 - val_loss: 0.6500 - val_mae: 0.6500\n",
            "Epoch 110/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.7004 - mae: 0.7004\n",
            "Epoch 110: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6988 - mae: 0.6988 - val_loss: 0.7761 - val_mae: 0.7761\n",
            "Epoch 111/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6960 - mae: 0.6960\n",
            "Epoch 111: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7004 - mae: 0.7004 - val_loss: 1.6321 - val_mae: 1.6321\n",
            "Epoch 112/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.7227 - mae: 0.7227\n",
            "Epoch 112: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7227 - mae: 0.7227 - val_loss: 0.8483 - val_mae: 0.8483\n",
            "Epoch 113/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.7108 - mae: 0.7108\n",
            "Epoch 113: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7094 - mae: 0.7094 - val_loss: 1.0951 - val_mae: 1.0951\n",
            "Epoch 114/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6682 - mae: 0.6682\n",
            "Epoch 114: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6788 - mae: 0.6788 - val_loss: 2.0849 - val_mae: 2.0849\n",
            "Epoch 115/500\n",
            "53/59 [=========================>....] - ETA: 0s - loss: 0.7728 - mae: 0.7728\n",
            "Epoch 115: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.7529 - mae: 0.7529 - val_loss: 1.4155 - val_mae: 1.4155\n",
            "Epoch 116/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.7525 - mae: 0.7525\n",
            "Epoch 116: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7513 - mae: 0.7513 - val_loss: 0.7500 - val_mae: 0.7500\n",
            "Epoch 117/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6738 - mae: 0.6738\n",
            "Epoch 117: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6794 - mae: 0.6794 - val_loss: 1.8171 - val_mae: 1.8171\n",
            "Epoch 118/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.7169 - mae: 0.7169\n",
            "Epoch 118: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7178 - mae: 0.7178 - val_loss: 1.3726 - val_mae: 1.3726\n",
            "Epoch 119/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.7068 - mae: 0.7068\n",
            "Epoch 119: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7005 - mae: 0.7005 - val_loss: 0.6181 - val_mae: 0.6181\n",
            "Epoch 120/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6926 - mae: 0.6926\n",
            "Epoch 120: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6913 - mae: 0.6913 - val_loss: 0.8763 - val_mae: 0.8763\n",
            "Epoch 121/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.7094 - mae: 0.7094\n",
            "Epoch 121: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7115 - mae: 0.7115 - val_loss: 1.4064 - val_mae: 1.4064\n",
            "Epoch 122/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6463 - mae: 0.6463\n",
            "Epoch 122: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6449 - mae: 0.6449 - val_loss: 0.8130 - val_mae: 0.8130\n",
            "Epoch 123/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.7532 - mae: 0.7532\n",
            "Epoch 123: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7536 - mae: 0.7536 - val_loss: 1.1040 - val_mae: 1.1040\n",
            "Epoch 124/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6783 - mae: 0.6783\n",
            "Epoch 124: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 12ms/step - loss: 0.6735 - mae: 0.6735 - val_loss: 0.5408 - val_mae: 0.5408\n",
            "Epoch 125/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6915 - mae: 0.6915\n",
            "Epoch 125: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 16ms/step - loss: 0.6914 - mae: 0.6914 - val_loss: 1.0828 - val_mae: 1.0828\n",
            "Epoch 126/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6656 - mae: 0.6656\n",
            "Epoch 126: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 15ms/step - loss: 0.6757 - mae: 0.6757 - val_loss: 0.9886 - val_mae: 0.9886\n",
            "Epoch 127/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.5993 - mae: 0.5993\n",
            "Epoch 127: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 13ms/step - loss: 0.6070 - mae: 0.6070 - val_loss: 1.0541 - val_mae: 1.0541\n",
            "Epoch 128/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6703 - mae: 0.6703\n",
            "Epoch 128: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6695 - mae: 0.6695 - val_loss: 1.1024 - val_mae: 1.1024\n",
            "Epoch 129/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6673 - mae: 0.6673\n",
            "Epoch 129: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6557 - mae: 0.6557 - val_loss: 1.2227 - val_mae: 1.2227\n",
            "Epoch 130/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6890 - mae: 0.6890\n",
            "Epoch 130: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6879 - mae: 0.6879 - val_loss: 1.1935 - val_mae: 1.1935\n",
            "Epoch 131/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.7203 - mae: 0.7203\n",
            "Epoch 131: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7182 - mae: 0.7182 - val_loss: 0.8818 - val_mae: 0.8818\n",
            "Epoch 132/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.7019 - mae: 0.7019\n",
            "Epoch 132: val_loss did not improve from 0.37681\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7015 - mae: 0.7015 - val_loss: 0.8822 - val_mae: 0.8822\n",
            "Epoch 133/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6844 - mae: 0.6844\n",
            "Epoch 133: val_loss improved from 0.37681 to 0.37624, saving model to best_model.h5\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6690 - mae: 0.6690 - val_loss: 0.3762 - val_mae: 0.3762\n",
            "Epoch 134/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6569 - mae: 0.6569\n",
            "Epoch 134: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6609 - mae: 0.6609 - val_loss: 0.4859 - val_mae: 0.4859\n",
            "Epoch 135/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6678 - mae: 0.6678\n",
            "Epoch 135: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6701 - mae: 0.6701 - val_loss: 0.7113 - val_mae: 0.7113\n",
            "Epoch 136/500\n",
            "53/59 [=========================>....] - ETA: 0s - loss: 0.6955 - mae: 0.6955\n",
            "Epoch 136: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7090 - mae: 0.7090 - val_loss: 0.6029 - val_mae: 0.6029\n",
            "Epoch 137/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6332 - mae: 0.6332\n",
            "Epoch 137: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6291 - mae: 0.6291 - val_loss: 1.3283 - val_mae: 1.3283\n",
            "Epoch 138/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6680 - mae: 0.6680\n",
            "Epoch 138: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6754 - mae: 0.6754 - val_loss: 0.7002 - val_mae: 0.7002\n",
            "Epoch 139/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6636 - mae: 0.6636\n",
            "Epoch 139: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6610 - mae: 0.6610 - val_loss: 0.5534 - val_mae: 0.5534\n",
            "Epoch 140/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6401 - mae: 0.6401\n",
            "Epoch 140: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6397 - mae: 0.6397 - val_loss: 1.5684 - val_mae: 1.5684\n",
            "Epoch 141/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6829 - mae: 0.6829\n",
            "Epoch 141: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6828 - mae: 0.6828 - val_loss: 1.2651 - val_mae: 1.2651\n",
            "Epoch 142/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6689 - mae: 0.6689\n",
            "Epoch 142: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6689 - mae: 0.6689 - val_loss: 0.8170 - val_mae: 0.8170\n",
            "Epoch 143/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.7114 - mae: 0.7114\n",
            "Epoch 143: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 12ms/step - loss: 0.7119 - mae: 0.7119 - val_loss: 0.9268 - val_mae: 0.9268\n",
            "Epoch 144/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6996 - mae: 0.6996\n",
            "Epoch 144: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 17ms/step - loss: 0.7004 - mae: 0.7004 - val_loss: 1.0733 - val_mae: 1.0733\n",
            "Epoch 145/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7227 - mae: 0.7227\n",
            "Epoch 145: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 16ms/step - loss: 0.7227 - mae: 0.7227 - val_loss: 1.2752 - val_mae: 1.2752\n",
            "Epoch 146/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.7642 - mae: 0.7642\n",
            "Epoch 146: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 13ms/step - loss: 0.7578 - mae: 0.7578 - val_loss: 0.9959 - val_mae: 0.9959\n",
            "Epoch 147/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6459 - mae: 0.6459\n",
            "Epoch 147: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6452 - mae: 0.6452 - val_loss: 1.2931 - val_mae: 1.2931\n",
            "Epoch 148/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6738 - mae: 0.6738\n",
            "Epoch 148: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6738 - mae: 0.6738 - val_loss: 0.8463 - val_mae: 0.8463\n",
            "Epoch 149/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6380 - mae: 0.6380\n",
            "Epoch 149: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6329 - mae: 0.6329 - val_loss: 0.6836 - val_mae: 0.6836\n",
            "Epoch 150/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.7397 - mae: 0.7397\n",
            "Epoch 150: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7405 - mae: 0.7405 - val_loss: 0.6503 - val_mae: 0.6503\n",
            "Epoch 151/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6090 - mae: 0.6090\n",
            "Epoch 151: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6240 - mae: 0.6240 - val_loss: 0.6544 - val_mae: 0.6544\n",
            "Epoch 152/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6884 - mae: 0.6884\n",
            "Epoch 152: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6952 - mae: 0.6952 - val_loss: 0.6621 - val_mae: 0.6621\n",
            "Epoch 153/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6072 - mae: 0.6072\n",
            "Epoch 153: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6062 - mae: 0.6062 - val_loss: 0.5980 - val_mae: 0.5980\n",
            "Epoch 154/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6476 - mae: 0.6476\n",
            "Epoch 154: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6412 - mae: 0.6412 - val_loss: 1.2554 - val_mae: 1.2554\n",
            "Epoch 155/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6129 - mae: 0.6129\n",
            "Epoch 155: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6124 - mae: 0.6124 - val_loss: 2.0450 - val_mae: 2.0450\n",
            "Epoch 156/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6304 - mae: 0.6304\n",
            "Epoch 156: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6401 - mae: 0.6401 - val_loss: 0.8605 - val_mae: 0.8605\n",
            "Epoch 157/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.7395 - mae: 0.7395\n",
            "Epoch 157: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7351 - mae: 0.7351 - val_loss: 1.1474 - val_mae: 1.1474\n",
            "Epoch 158/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6482 - mae: 0.6482\n",
            "Epoch 158: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6551 - mae: 0.6551 - val_loss: 0.7209 - val_mae: 0.7209\n",
            "Epoch 159/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6876 - mae: 0.6876\n",
            "Epoch 159: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6784 - mae: 0.6784 - val_loss: 0.6436 - val_mae: 0.6436\n",
            "Epoch 160/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6746 - mae: 0.6746\n",
            "Epoch 160: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6735 - mae: 0.6735 - val_loss: 0.6626 - val_mae: 0.6626\n",
            "Epoch 161/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6173 - mae: 0.6173\n",
            "Epoch 161: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6181 - mae: 0.6181 - val_loss: 1.1124 - val_mae: 1.1124\n",
            "Epoch 162/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6258 - mae: 0.6258\n",
            "Epoch 162: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 12ms/step - loss: 0.6318 - mae: 0.6318 - val_loss: 1.1192 - val_mae: 1.1192\n",
            "Epoch 163/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6744 - mae: 0.6744\n",
            "Epoch 163: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 17ms/step - loss: 0.6766 - mae: 0.6766 - val_loss: 1.1028 - val_mae: 1.1028\n",
            "Epoch 164/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.7004 - mae: 0.7004\n",
            "Epoch 164: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 18ms/step - loss: 0.7022 - mae: 0.7022 - val_loss: 0.7404 - val_mae: 0.7404\n",
            "Epoch 165/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6556 - mae: 0.6556\n",
            "Epoch 165: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 12ms/step - loss: 0.6467 - mae: 0.6467 - val_loss: 0.9252 - val_mae: 0.9252\n",
            "Epoch 166/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6910 - mae: 0.6910\n",
            "Epoch 166: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6910 - mae: 0.6910 - val_loss: 1.4606 - val_mae: 1.4606\n",
            "Epoch 167/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6715 - mae: 0.6715\n",
            "Epoch 167: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6686 - mae: 0.6686 - val_loss: 0.7388 - val_mae: 0.7388\n",
            "Epoch 168/500\n",
            "53/59 [=========================>....] - ETA: 0s - loss: 0.6606 - mae: 0.6606\n",
            "Epoch 168: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6538 - mae: 0.6538 - val_loss: 1.7729 - val_mae: 1.7729\n",
            "Epoch 169/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.7021 - mae: 0.7021\n",
            "Epoch 169: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6987 - mae: 0.6987 - val_loss: 0.6206 - val_mae: 0.6206\n",
            "Epoch 170/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6498 - mae: 0.6498\n",
            "Epoch 170: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6479 - mae: 0.6479 - val_loss: 0.6511 - val_mae: 0.6511\n",
            "Epoch 171/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6276 - mae: 0.6276\n",
            "Epoch 171: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6310 - mae: 0.6310 - val_loss: 0.3876 - val_mae: 0.3876\n",
            "Epoch 172/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6418 - mae: 0.6418\n",
            "Epoch 172: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6501 - mae: 0.6501 - val_loss: 0.6770 - val_mae: 0.6770\n",
            "Epoch 173/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6608 - mae: 0.6608\n",
            "Epoch 173: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6666 - mae: 0.6666 - val_loss: 1.6880 - val_mae: 1.6880\n",
            "Epoch 174/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.7312 - mae: 0.7312\n",
            "Epoch 174: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7192 - mae: 0.7192 - val_loss: 1.9466 - val_mae: 1.9466\n",
            "Epoch 175/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.7268 - mae: 0.7268\n",
            "Epoch 175: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.7182 - mae: 0.7182 - val_loss: 0.9997 - val_mae: 0.9997\n",
            "Epoch 176/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6616 - mae: 0.6616\n",
            "Epoch 176: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6763 - mae: 0.6763 - val_loss: 0.8699 - val_mae: 0.8699\n",
            "Epoch 177/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6622 - mae: 0.6622\n",
            "Epoch 177: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6622 - mae: 0.6622 - val_loss: 0.4161 - val_mae: 0.4161\n",
            "Epoch 178/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6342 - mae: 0.6342\n",
            "Epoch 178: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6342 - mae: 0.6342 - val_loss: 0.6657 - val_mae: 0.6657\n",
            "Epoch 179/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6893 - mae: 0.6893\n",
            "Epoch 179: val_loss did not improve from 0.37624\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6893 - mae: 0.6893 - val_loss: 0.5841 - val_mae: 0.5841\n",
            "Epoch 180/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6523 - mae: 0.6523\n",
            "Epoch 180: val_loss improved from 0.37624 to 0.35850, saving model to best_model.h5\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6459 - mae: 0.6459 - val_loss: 0.3585 - val_mae: 0.3585\n",
            "Epoch 181/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6470 - mae: 0.6470\n",
            "Epoch 181: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 15ms/step - loss: 0.6470 - mae: 0.6470 - val_loss: 0.7602 - val_mae: 0.7602\n",
            "Epoch 182/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6787 - mae: 0.6787\n",
            "Epoch 182: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 17ms/step - loss: 0.6804 - mae: 0.6804 - val_loss: 0.8704 - val_mae: 0.8704\n",
            "Epoch 183/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.7007 - mae: 0.7007\n",
            "Epoch 183: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 16ms/step - loss: 0.7018 - mae: 0.7018 - val_loss: 0.6257 - val_mae: 0.6257\n",
            "Epoch 184/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6839 - mae: 0.6839\n",
            "Epoch 184: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6786 - mae: 0.6786 - val_loss: 0.7672 - val_mae: 0.7672\n",
            "Epoch 185/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6176 - mae: 0.6176\n",
            "Epoch 185: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6202 - mae: 0.6202 - val_loss: 0.9288 - val_mae: 0.9288\n",
            "Epoch 186/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6538 - mae: 0.6538\n",
            "Epoch 186: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6543 - mae: 0.6543 - val_loss: 1.1645 - val_mae: 1.1645\n",
            "Epoch 187/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6596 - mae: 0.6596\n",
            "Epoch 187: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6556 - mae: 0.6556 - val_loss: 2.1126 - val_mae: 2.1126\n",
            "Epoch 188/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6830 - mae: 0.6830\n",
            "Epoch 188: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6886 - mae: 0.6886 - val_loss: 1.4410 - val_mae: 1.4410\n",
            "Epoch 189/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6551 - mae: 0.6551\n",
            "Epoch 189: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6551 - mae: 0.6551 - val_loss: 0.8008 - val_mae: 0.8008\n",
            "Epoch 190/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6801 - mae: 0.6801\n",
            "Epoch 190: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6746 - mae: 0.6746 - val_loss: 1.0379 - val_mae: 1.0379\n",
            "Epoch 191/500\n",
            "53/59 [=========================>....] - ETA: 0s - loss: 0.6397 - mae: 0.6397\n",
            "Epoch 191: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6354 - mae: 0.6354 - val_loss: 1.1932 - val_mae: 1.1932\n",
            "Epoch 192/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6770 - mae: 0.6770\n",
            "Epoch 192: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6859 - mae: 0.6859 - val_loss: 1.8541 - val_mae: 1.8541\n",
            "Epoch 193/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6985 - mae: 0.6985\n",
            "Epoch 193: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6985 - mae: 0.6985 - val_loss: 2.1357 - val_mae: 2.1357\n",
            "Epoch 194/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6708 - mae: 0.6708\n",
            "Epoch 194: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6719 - mae: 0.6719 - val_loss: 1.1191 - val_mae: 1.1191\n",
            "Epoch 195/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6710 - mae: 0.6710\n",
            "Epoch 195: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6711 - mae: 0.6711 - val_loss: 1.0797 - val_mae: 1.0797\n",
            "Epoch 196/500\n",
            "53/59 [=========================>....] - ETA: 0s - loss: 0.6743 - mae: 0.6743\n",
            "Epoch 196: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6754 - mae: 0.6754 - val_loss: 1.1839 - val_mae: 1.1839\n",
            "Epoch 197/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6432 - mae: 0.6432\n",
            "Epoch 197: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6488 - mae: 0.6488 - val_loss: 1.0211 - val_mae: 1.0211\n",
            "Epoch 198/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6270 - mae: 0.6270\n",
            "Epoch 198: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6270 - mae: 0.6270 - val_loss: 0.8037 - val_mae: 0.8037\n",
            "Epoch 199/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6131 - mae: 0.6131\n",
            "Epoch 199: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6158 - mae: 0.6158 - val_loss: 1.0738 - val_mae: 1.0738\n",
            "Epoch 200/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.7117 - mae: 0.7117\n",
            "Epoch 200: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 15ms/step - loss: 0.7173 - mae: 0.7173 - val_loss: 0.4105 - val_mae: 0.4105\n",
            "Epoch 201/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6567 - mae: 0.6567\n",
            "Epoch 201: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 15ms/step - loss: 0.6579 - mae: 0.6579 - val_loss: 0.5660 - val_mae: 0.5660\n",
            "Epoch 202/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6904 - mae: 0.6904\n",
            "Epoch 202: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 17ms/step - loss: 0.6889 - mae: 0.6889 - val_loss: 1.2428 - val_mae: 1.2428\n",
            "Epoch 203/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.5549 - mae: 0.5549\n",
            "Epoch 203: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 12ms/step - loss: 0.5589 - mae: 0.5589 - val_loss: 1.0408 - val_mae: 1.0408\n",
            "Epoch 204/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6588 - mae: 0.6588\n",
            "Epoch 204: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6701 - mae: 0.6701 - val_loss: 0.5119 - val_mae: 0.5119\n",
            "Epoch 205/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6140 - mae: 0.6140\n",
            "Epoch 205: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6226 - mae: 0.6226 - val_loss: 0.7754 - val_mae: 0.7754\n",
            "Epoch 206/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6495 - mae: 0.6495\n",
            "Epoch 206: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6514 - mae: 0.6514 - val_loss: 0.7665 - val_mae: 0.7665\n",
            "Epoch 207/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6793 - mae: 0.6793\n",
            "Epoch 207: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6889 - mae: 0.6889 - val_loss: 0.5002 - val_mae: 0.5002\n",
            "Epoch 208/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.5842 - mae: 0.5842\n",
            "Epoch 208: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6019 - mae: 0.6019 - val_loss: 0.8060 - val_mae: 0.8060\n",
            "Epoch 209/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6758 - mae: 0.6758\n",
            "Epoch 209: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6769 - mae: 0.6769 - val_loss: 1.8293 - val_mae: 1.8293\n",
            "Epoch 210/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6638 - mae: 0.6638\n",
            "Epoch 210: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6661 - mae: 0.6661 - val_loss: 0.9349 - val_mae: 0.9349\n",
            "Epoch 211/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6195 - mae: 0.6195\n",
            "Epoch 211: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6201 - mae: 0.6201 - val_loss: 0.4052 - val_mae: 0.4052\n",
            "Epoch 212/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6699 - mae: 0.6699\n",
            "Epoch 212: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6656 - mae: 0.6656 - val_loss: 0.8525 - val_mae: 0.8525\n",
            "Epoch 213/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6905 - mae: 0.6905\n",
            "Epoch 213: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6802 - mae: 0.6802 - val_loss: 0.5241 - val_mae: 0.5241\n",
            "Epoch 214/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6873 - mae: 0.6873\n",
            "Epoch 214: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6886 - mae: 0.6886 - val_loss: 1.0647 - val_mae: 1.0647\n",
            "Epoch 215/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6900 - mae: 0.6900\n",
            "Epoch 215: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6950 - mae: 0.6950 - val_loss: 1.3536 - val_mae: 1.3536\n",
            "Epoch 216/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6411 - mae: 0.6411\n",
            "Epoch 216: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6437 - mae: 0.6437 - val_loss: 0.8751 - val_mae: 0.8751\n",
            "Epoch 217/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6281 - mae: 0.6281\n",
            "Epoch 217: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6327 - mae: 0.6327 - val_loss: 0.3907 - val_mae: 0.3907\n",
            "Epoch 218/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6719 - mae: 0.6719\n",
            "Epoch 218: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6656 - mae: 0.6656 - val_loss: 0.9115 - val_mae: 0.9115\n",
            "Epoch 219/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6518 - mae: 0.6518\n",
            "Epoch 219: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 18ms/step - loss: 0.6572 - mae: 0.6572 - val_loss: 0.7729 - val_mae: 0.7729\n",
            "Epoch 220/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6754 - mae: 0.6754\n",
            "Epoch 220: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 15ms/step - loss: 0.6757 - mae: 0.6757 - val_loss: 0.7890 - val_mae: 0.7890\n",
            "Epoch 221/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6896 - mae: 0.6896\n",
            "Epoch 221: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 16ms/step - loss: 0.6892 - mae: 0.6892 - val_loss: 1.1873 - val_mae: 1.1873\n",
            "Epoch 222/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.7553 - mae: 0.7553\n",
            "Epoch 222: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.7554 - mae: 0.7554 - val_loss: 2.1098 - val_mae: 2.1098\n",
            "Epoch 223/500\n",
            "53/59 [=========================>....] - ETA: 0s - loss: 0.6659 - mae: 0.6659\n",
            "Epoch 223: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6616 - mae: 0.6616 - val_loss: 0.3852 - val_mae: 0.3852\n",
            "Epoch 224/500\n",
            "53/59 [=========================>....] - ETA: 0s - loss: 0.6854 - mae: 0.6854\n",
            "Epoch 224: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6756 - mae: 0.6756 - val_loss: 0.5290 - val_mae: 0.5290\n",
            "Epoch 225/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6497 - mae: 0.6497\n",
            "Epoch 225: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6556 - mae: 0.6556 - val_loss: 1.0547 - val_mae: 1.0547\n",
            "Epoch 226/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6410 - mae: 0.6410\n",
            "Epoch 226: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6395 - mae: 0.6395 - val_loss: 0.7681 - val_mae: 0.7681\n",
            "Epoch 227/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6542 - mae: 0.6542\n",
            "Epoch 227: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6567 - mae: 0.6567 - val_loss: 0.4889 - val_mae: 0.4889\n",
            "Epoch 228/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6692 - mae: 0.6692\n",
            "Epoch 228: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6664 - mae: 0.6664 - val_loss: 1.2141 - val_mae: 1.2141\n",
            "Epoch 229/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6635 - mae: 0.6635\n",
            "Epoch 229: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6696 - mae: 0.6696 - val_loss: 0.6199 - val_mae: 0.6199\n",
            "Epoch 230/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6420 - mae: 0.6420\n",
            "Epoch 230: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6387 - mae: 0.6387 - val_loss: 0.8742 - val_mae: 0.8742\n",
            "Epoch 231/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6633 - mae: 0.6633\n",
            "Epoch 231: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6582 - mae: 0.6582 - val_loss: 1.7708 - val_mae: 1.7708\n",
            "Epoch 232/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6904 - mae: 0.6904\n",
            "Epoch 232: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6904 - mae: 0.6904 - val_loss: 1.8842 - val_mae: 1.8842\n",
            "Epoch 233/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6755 - mae: 0.6755\n",
            "Epoch 233: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6710 - mae: 0.6710 - val_loss: 2.9167 - val_mae: 2.9167\n",
            "Epoch 234/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6684 - mae: 0.6684\n",
            "Epoch 234: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6684 - mae: 0.6684 - val_loss: 1.0046 - val_mae: 1.0046\n",
            "Epoch 235/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6639 - mae: 0.6639\n",
            "Epoch 235: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6682 - mae: 0.6682 - val_loss: 1.2527 - val_mae: 1.2527\n",
            "Epoch 236/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6069 - mae: 0.6069\n",
            "Epoch 236: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6053 - mae: 0.6053 - val_loss: 1.3172 - val_mae: 1.3172\n",
            "Epoch 237/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.5968 - mae: 0.5968\n",
            "Epoch 237: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 13ms/step - loss: 0.5972 - mae: 0.5972 - val_loss: 1.2828 - val_mae: 1.2828\n",
            "Epoch 238/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6888 - mae: 0.6888\n",
            "Epoch 238: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 16ms/step - loss: 0.6801 - mae: 0.6801 - val_loss: 0.7373 - val_mae: 0.7373\n",
            "Epoch 239/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6539 - mae: 0.6539\n",
            "Epoch 239: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 17ms/step - loss: 0.6535 - mae: 0.6535 - val_loss: 1.4496 - val_mae: 1.4496\n",
            "Epoch 240/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6849 - mae: 0.6849\n",
            "Epoch 240: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 14ms/step - loss: 0.6870 - mae: 0.6870 - val_loss: 0.8101 - val_mae: 0.8101\n",
            "Epoch 241/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6676 - mae: 0.6676\n",
            "Epoch 241: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6674 - mae: 0.6674 - val_loss: 1.6404 - val_mae: 1.6404\n",
            "Epoch 242/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6629 - mae: 0.6629\n",
            "Epoch 242: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6624 - mae: 0.6624 - val_loss: 0.9171 - val_mae: 0.9171\n",
            "Epoch 243/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6745 - mae: 0.6745\n",
            "Epoch 243: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6815 - mae: 0.6815 - val_loss: 0.8305 - val_mae: 0.8305\n",
            "Epoch 244/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6353 - mae: 0.6353\n",
            "Epoch 244: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6367 - mae: 0.6367 - val_loss: 0.6688 - val_mae: 0.6688\n",
            "Epoch 245/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6553 - mae: 0.6553\n",
            "Epoch 245: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6571 - mae: 0.6571 - val_loss: 0.7801 - val_mae: 0.7801\n",
            "Epoch 246/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6842 - mae: 0.6842\n",
            "Epoch 246: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6866 - mae: 0.6866 - val_loss: 0.9278 - val_mae: 0.9278\n",
            "Epoch 247/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6687 - mae: 0.6687\n",
            "Epoch 247: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6694 - mae: 0.6694 - val_loss: 0.9818 - val_mae: 0.9818\n",
            "Epoch 248/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6242 - mae: 0.6242\n",
            "Epoch 248: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6307 - mae: 0.6307 - val_loss: 0.9853 - val_mae: 0.9853\n",
            "Epoch 249/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6398 - mae: 0.6398\n",
            "Epoch 249: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6398 - mae: 0.6398 - val_loss: 0.8857 - val_mae: 0.8857\n",
            "Epoch 250/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6230 - mae: 0.6230\n",
            "Epoch 250: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6230 - mae: 0.6230 - val_loss: 1.0464 - val_mae: 1.0464\n",
            "Epoch 251/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6457 - mae: 0.6457\n",
            "Epoch 251: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6492 - mae: 0.6492 - val_loss: 0.6657 - val_mae: 0.6657\n",
            "Epoch 252/500\n",
            "53/59 [=========================>....] - ETA: 0s - loss: 0.6733 - mae: 0.6733\n",
            "Epoch 252: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6621 - mae: 0.6621 - val_loss: 0.6195 - val_mae: 0.6195\n",
            "Epoch 253/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6789 - mae: 0.6789\n",
            "Epoch 253: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6775 - mae: 0.6775 - val_loss: 0.6627 - val_mae: 0.6627\n",
            "Epoch 254/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6867 - mae: 0.6867\n",
            "Epoch 254: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 17ms/step - loss: 0.6870 - mae: 0.6870 - val_loss: 1.3063 - val_mae: 1.3063\n",
            "Epoch 255/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6718 - mae: 0.6718\n",
            "Epoch 255: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 2s 26ms/step - loss: 0.6747 - mae: 0.6747 - val_loss: 0.7892 - val_mae: 0.7892\n",
            "Epoch 256/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.5970 - mae: 0.5970\n",
            "Epoch 256: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 2s 26ms/step - loss: 0.5976 - mae: 0.5976 - val_loss: 1.0178 - val_mae: 1.0178\n",
            "Epoch 257/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6531 - mae: 0.6531\n",
            "Epoch 257: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 2s 27ms/step - loss: 0.6520 - mae: 0.6520 - val_loss: 1.6475 - val_mae: 1.6475\n",
            "Epoch 258/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6383 - mae: 0.6383\n",
            "Epoch 258: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 18ms/step - loss: 0.6367 - mae: 0.6367 - val_loss: 1.0745 - val_mae: 1.0745\n",
            "Epoch 259/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6445 - mae: 0.6445\n",
            "Epoch 259: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6535 - mae: 0.6535 - val_loss: 0.8245 - val_mae: 0.8245\n",
            "Epoch 260/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6246 - mae: 0.6246\n",
            "Epoch 260: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6241 - mae: 0.6241 - val_loss: 0.7294 - val_mae: 0.7294\n",
            "Epoch 261/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6540 - mae: 0.6540\n",
            "Epoch 261: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6540 - mae: 0.6540 - val_loss: 1.2708 - val_mae: 1.2708\n",
            "Epoch 262/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6581 - mae: 0.6581\n",
            "Epoch 262: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6581 - mae: 0.6581 - val_loss: 1.1380 - val_mae: 1.1380\n",
            "Epoch 263/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6425 - mae: 0.6425\n",
            "Epoch 263: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6419 - mae: 0.6419 - val_loss: 1.1325 - val_mae: 1.1325\n",
            "Epoch 264/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6813 - mae: 0.6813\n",
            "Epoch 264: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6643 - mae: 0.6643 - val_loss: 1.2177 - val_mae: 1.2177\n",
            "Epoch 265/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6536 - mae: 0.6536\n",
            "Epoch 265: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6536 - mae: 0.6536 - val_loss: 1.5621 - val_mae: 1.5621\n",
            "Epoch 266/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6392 - mae: 0.6392\n",
            "Epoch 266: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6392 - mae: 0.6392 - val_loss: 0.6361 - val_mae: 0.6361\n",
            "Epoch 267/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.5925 - mae: 0.5925\n",
            "Epoch 267: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6017 - mae: 0.6017 - val_loss: 1.2731 - val_mae: 1.2731\n",
            "Epoch 268/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6788 - mae: 0.6788\n",
            "Epoch 268: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6790 - mae: 0.6790 - val_loss: 2.0962 - val_mae: 2.0962\n",
            "Epoch 269/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6102 - mae: 0.6102\n",
            "Epoch 269: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6092 - mae: 0.6092 - val_loss: 0.5340 - val_mae: 0.5340\n",
            "Epoch 270/500\n",
            "53/59 [=========================>....] - ETA: 0s - loss: 0.6293 - mae: 0.6293\n",
            "Epoch 270: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6225 - mae: 0.6225 - val_loss: 0.6293 - val_mae: 0.6293\n",
            "Epoch 271/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6806 - mae: 0.6806\n",
            "Epoch 271: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6770 - mae: 0.6770 - val_loss: 0.5103 - val_mae: 0.5103\n",
            "Epoch 272/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6253 - mae: 0.6253\n",
            "Epoch 272: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 15ms/step - loss: 0.6294 - mae: 0.6294 - val_loss: 1.8480 - val_mae: 1.8480\n",
            "Epoch 273/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6646 - mae: 0.6646\n",
            "Epoch 273: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 15ms/step - loss: 0.6642 - mae: 0.6642 - val_loss: 1.3199 - val_mae: 1.3199\n",
            "Epoch 274/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6737 - mae: 0.6737\n",
            "Epoch 274: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 17ms/step - loss: 0.6732 - mae: 0.6732 - val_loss: 0.8031 - val_mae: 0.8031\n",
            "Epoch 275/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6697 - mae: 0.6697\n",
            "Epoch 275: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 14ms/step - loss: 0.6550 - mae: 0.6550 - val_loss: 1.1544 - val_mae: 1.1544\n",
            "Epoch 276/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6123 - mae: 0.6123\n",
            "Epoch 276: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6085 - mae: 0.6085 - val_loss: 1.3002 - val_mae: 1.3002\n",
            "Epoch 277/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.5756 - mae: 0.5756\n",
            "Epoch 277: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.5688 - mae: 0.5688 - val_loss: 1.5125 - val_mae: 1.5125\n",
            "Epoch 278/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6235 - mae: 0.6235\n",
            "Epoch 278: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6232 - mae: 0.6232 - val_loss: 0.7838 - val_mae: 0.7838\n",
            "Epoch 279/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.5717 - mae: 0.5717\n",
            "Epoch 279: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.5764 - mae: 0.5764 - val_loss: 0.7188 - val_mae: 0.7188\n",
            "Epoch 280/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6223 - mae: 0.6223\n",
            "Epoch 280: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6203 - mae: 0.6203 - val_loss: 0.8396 - val_mae: 0.8396\n",
            "Epoch 281/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6339 - mae: 0.6339\n",
            "Epoch 281: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6307 - mae: 0.6307 - val_loss: 0.8128 - val_mae: 0.8128\n",
            "Epoch 282/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.5721 - mae: 0.5721\n",
            "Epoch 282: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.5729 - mae: 0.5729 - val_loss: 1.1920 - val_mae: 1.1920\n",
            "Epoch 283/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6886 - mae: 0.6886\n",
            "Epoch 283: val_loss did not improve from 0.35850\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6888 - mae: 0.6888 - val_loss: 0.6676 - val_mae: 0.6676\n",
            "Epoch 284/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6911 - mae: 0.6911\n",
            "Epoch 284: val_loss improved from 0.35850 to 0.34793, saving model to best_model.h5\n",
            "59/59 [==============================] - 1s 12ms/step - loss: 0.6887 - mae: 0.6887 - val_loss: 0.3479 - val_mae: 0.3479\n",
            "Epoch 285/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6183 - mae: 0.6183\n",
            "Epoch 285: val_loss did not improve from 0.34793\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6230 - mae: 0.6230 - val_loss: 0.4355 - val_mae: 0.4355\n",
            "Epoch 286/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6828 - mae: 0.6828\n",
            "Epoch 286: val_loss did not improve from 0.34793\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6852 - mae: 0.6852 - val_loss: 0.9990 - val_mae: 0.9990\n",
            "Epoch 287/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6041 - mae: 0.6041\n",
            "Epoch 287: val_loss did not improve from 0.34793\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6114 - mae: 0.6114 - val_loss: 0.6016 - val_mae: 0.6016\n",
            "Epoch 288/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6495 - mae: 0.6495\n",
            "Epoch 288: val_loss did not improve from 0.34793\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6508 - mae: 0.6508 - val_loss: 0.9056 - val_mae: 0.9056\n",
            "Epoch 289/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6577 - mae: 0.6577\n",
            "Epoch 289: val_loss did not improve from 0.34793\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6552 - mae: 0.6552 - val_loss: 0.8075 - val_mae: 0.8075\n",
            "Epoch 290/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6213 - mae: 0.6213\n",
            "Epoch 290: val_loss did not improve from 0.34793\n",
            "59/59 [==============================] - 1s 13ms/step - loss: 0.6233 - mae: 0.6233 - val_loss: 0.6500 - val_mae: 0.6500\n",
            "Epoch 291/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6354 - mae: 0.6354\n",
            "Epoch 291: val_loss did not improve from 0.34793\n",
            "59/59 [==============================] - 1s 15ms/step - loss: 0.6354 - mae: 0.6354 - val_loss: 0.7365 - val_mae: 0.7365\n",
            "Epoch 292/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.5705 - mae: 0.5705\n",
            "Epoch 292: val_loss did not improve from 0.34793\n",
            "59/59 [==============================] - 1s 16ms/step - loss: 0.5706 - mae: 0.5706 - val_loss: 1.7114 - val_mae: 1.7114\n",
            "Epoch 293/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6185 - mae: 0.6185\n",
            "Epoch 293: val_loss did not improve from 0.34793\n",
            "59/59 [==============================] - 1s 16ms/step - loss: 0.6139 - mae: 0.6139 - val_loss: 0.5109 - val_mae: 0.5109\n",
            "Epoch 294/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6025 - mae: 0.6025\n",
            "Epoch 294: val_loss did not improve from 0.34793\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6017 - mae: 0.6017 - val_loss: 1.5980 - val_mae: 1.5980\n",
            "Epoch 295/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6637 - mae: 0.6637\n",
            "Epoch 295: val_loss did not improve from 0.34793\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6645 - mae: 0.6645 - val_loss: 0.5057 - val_mae: 0.5057\n",
            "Epoch 296/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6591 - mae: 0.6591\n",
            "Epoch 296: val_loss did not improve from 0.34793\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6591 - mae: 0.6591 - val_loss: 1.0605 - val_mae: 1.0605\n",
            "Epoch 297/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6311 - mae: 0.6311\n",
            "Epoch 297: val_loss did not improve from 0.34793\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6311 - mae: 0.6311 - val_loss: 0.8170 - val_mae: 0.8170\n",
            "Epoch 298/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6750 - mae: 0.6750\n",
            "Epoch 298: val_loss did not improve from 0.34793\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6817 - mae: 0.6817 - val_loss: 0.5822 - val_mae: 0.5822\n",
            "Epoch 299/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6570 - mae: 0.6570\n",
            "Epoch 299: val_loss did not improve from 0.34793\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6527 - mae: 0.6527 - val_loss: 0.5994 - val_mae: 0.5994\n",
            "Epoch 300/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6952 - mae: 0.6952\n",
            "Epoch 300: val_loss did not improve from 0.34793\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6952 - mae: 0.6952 - val_loss: 0.9506 - val_mae: 0.9506\n",
            "Epoch 301/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6792 - mae: 0.6792\n",
            "Epoch 301: val_loss did not improve from 0.34793\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6822 - mae: 0.6822 - val_loss: 0.7514 - val_mae: 0.7514\n",
            "Epoch 302/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6539 - mae: 0.6539\n",
            "Epoch 302: val_loss did not improve from 0.34793\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6617 - mae: 0.6617 - val_loss: 0.7112 - val_mae: 0.7112\n",
            "Epoch 303/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6725 - mae: 0.6725\n",
            "Epoch 303: val_loss did not improve from 0.34793\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6717 - mae: 0.6717 - val_loss: 0.5705 - val_mae: 0.5705\n",
            "Epoch 304/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6899 - mae: 0.6899\n",
            "Epoch 304: val_loss did not improve from 0.34793\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6841 - mae: 0.6841 - val_loss: 0.6617 - val_mae: 0.6617\n",
            "Epoch 305/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6037 - mae: 0.6037\n",
            "Epoch 305: val_loss did not improve from 0.34793\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.5968 - mae: 0.5968 - val_loss: 0.5088 - val_mae: 0.5088\n",
            "Epoch 306/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6218 - mae: 0.6218\n",
            "Epoch 306: val_loss did not improve from 0.34793\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6218 - mae: 0.6218 - val_loss: 0.5864 - val_mae: 0.5864\n",
            "Epoch 307/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6554 - mae: 0.6554\n",
            "Epoch 307: val_loss did not improve from 0.34793\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6498 - mae: 0.6498 - val_loss: 1.5251 - val_mae: 1.5251\n",
            "Epoch 308/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6688 - mae: 0.6688\n",
            "Epoch 308: val_loss did not improve from 0.34793\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6698 - mae: 0.6698 - val_loss: 0.6221 - val_mae: 0.6221\n",
            "Epoch 309/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6238 - mae: 0.6238\n",
            "Epoch 309: val_loss did not improve from 0.34793\n",
            "59/59 [==============================] - 1s 15ms/step - loss: 0.6281 - mae: 0.6281 - val_loss: 0.8177 - val_mae: 0.8177\n",
            "Epoch 310/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6300 - mae: 0.6300\n",
            "Epoch 310: val_loss improved from 0.34793 to 0.34025, saving model to best_model.h5\n",
            "59/59 [==============================] - 1s 17ms/step - loss: 0.6331 - mae: 0.6331 - val_loss: 0.3402 - val_mae: 0.3402\n",
            "Epoch 311/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6048 - mae: 0.6048\n",
            "Epoch 311: val_loss did not improve from 0.34025\n",
            "59/59 [==============================] - 1s 18ms/step - loss: 0.6053 - mae: 0.6053 - val_loss: 1.7851 - val_mae: 1.7851\n",
            "Epoch 312/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6508 - mae: 0.6508\n",
            "Epoch 312: val_loss did not improve from 0.34025\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6441 - mae: 0.6441 - val_loss: 1.1970 - val_mae: 1.1970\n",
            "Epoch 313/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6118 - mae: 0.6118\n",
            "Epoch 313: val_loss improved from 0.34025 to 0.28649, saving model to best_model.h5\n",
            "59/59 [==============================] - 1s 12ms/step - loss: 0.6117 - mae: 0.6117 - val_loss: 0.2865 - val_mae: 0.2865\n",
            "Epoch 314/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6067 - mae: 0.6067\n",
            "Epoch 314: val_loss did not improve from 0.28649\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6065 - mae: 0.6065 - val_loss: 0.5795 - val_mae: 0.5795\n",
            "Epoch 315/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6331 - mae: 0.6331\n",
            "Epoch 315: val_loss did not improve from 0.28649\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6230 - mae: 0.6230 - val_loss: 1.1330 - val_mae: 1.1330\n",
            "Epoch 316/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6385 - mae: 0.6385\n",
            "Epoch 316: val_loss did not improve from 0.28649\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6325 - mae: 0.6325 - val_loss: 0.7048 - val_mae: 0.7048\n",
            "Epoch 317/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6614 - mae: 0.6614\n",
            "Epoch 317: val_loss did not improve from 0.28649\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6630 - mae: 0.6630 - val_loss: 1.0409 - val_mae: 1.0409\n",
            "Epoch 318/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6124 - mae: 0.6124\n",
            "Epoch 318: val_loss did not improve from 0.28649\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6080 - mae: 0.6080 - val_loss: 1.4682 - val_mae: 1.4682\n",
            "Epoch 319/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6542 - mae: 0.6542\n",
            "Epoch 319: val_loss did not improve from 0.28649\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6548 - mae: 0.6548 - val_loss: 1.5083 - val_mae: 1.5083\n",
            "Epoch 320/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6574 - mae: 0.6574\n",
            "Epoch 320: val_loss did not improve from 0.28649\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6574 - mae: 0.6574 - val_loss: 1.7089 - val_mae: 1.7089\n",
            "Epoch 321/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6783 - mae: 0.6783\n",
            "Epoch 321: val_loss did not improve from 0.28649\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6777 - mae: 0.6777 - val_loss: 1.1432 - val_mae: 1.1432\n",
            "Epoch 322/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6503 - mae: 0.6503\n",
            "Epoch 322: val_loss did not improve from 0.28649\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6497 - mae: 0.6497 - val_loss: 0.6200 - val_mae: 0.6200\n",
            "Epoch 323/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6103 - mae: 0.6103\n",
            "Epoch 323: val_loss did not improve from 0.28649\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6139 - mae: 0.6139 - val_loss: 2.0980 - val_mae: 2.0980\n",
            "Epoch 324/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6203 - mae: 0.6203\n",
            "Epoch 324: val_loss did not improve from 0.28649\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6215 - mae: 0.6215 - val_loss: 0.9417 - val_mae: 0.9417\n",
            "Epoch 325/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6738 - mae: 0.6738\n",
            "Epoch 325: val_loss did not improve from 0.28649\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6763 - mae: 0.6763 - val_loss: 0.3218 - val_mae: 0.3218\n",
            "Epoch 326/500\n",
            "53/59 [=========================>....] - ETA: 0s - loss: 0.6404 - mae: 0.6404\n",
            "Epoch 326: val_loss did not improve from 0.28649\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6368 - mae: 0.6368 - val_loss: 0.7633 - val_mae: 0.7633\n",
            "Epoch 327/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6596 - mae: 0.6596\n",
            "Epoch 327: val_loss did not improve from 0.28649\n",
            "59/59 [==============================] - 1s 13ms/step - loss: 0.6499 - mae: 0.6499 - val_loss: 0.8633 - val_mae: 0.8633\n",
            "Epoch 328/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.5640 - mae: 0.5640\n",
            "Epoch 328: val_loss did not improve from 0.28649\n",
            "59/59 [==============================] - 1s 17ms/step - loss: 0.5632 - mae: 0.5632 - val_loss: 1.1492 - val_mae: 1.1492\n",
            "Epoch 329/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6504 - mae: 0.6504\n",
            "Epoch 329: val_loss did not improve from 0.28649\n",
            "59/59 [==============================] - 1s 17ms/step - loss: 0.6519 - mae: 0.6519 - val_loss: 0.7472 - val_mae: 0.7472\n",
            "Epoch 330/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6438 - mae: 0.6438\n",
            "Epoch 330: val_loss did not improve from 0.28649\n",
            "59/59 [==============================] - 1s 14ms/step - loss: 0.6438 - mae: 0.6438 - val_loss: 1.1591 - val_mae: 1.1591\n",
            "Epoch 331/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6067 - mae: 0.6067\n",
            "Epoch 331: val_loss did not improve from 0.28649\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6062 - mae: 0.6062 - val_loss: 0.9507 - val_mae: 0.9507\n",
            "Epoch 332/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6828 - mae: 0.6828\n",
            "Epoch 332: val_loss did not improve from 0.28649\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6856 - mae: 0.6856 - val_loss: 0.4393 - val_mae: 0.4393\n",
            "Epoch 333/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6699 - mae: 0.6699\n",
            "Epoch 333: val_loss did not improve from 0.28649\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6849 - mae: 0.6849 - val_loss: 0.6679 - val_mae: 0.6679\n",
            "Epoch 334/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6632 - mae: 0.6632\n",
            "Epoch 334: val_loss did not improve from 0.28649\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6650 - mae: 0.6650 - val_loss: 0.5339 - val_mae: 0.5339\n",
            "Epoch 335/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6402 - mae: 0.6402\n",
            "Epoch 335: val_loss did not improve from 0.28649\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6411 - mae: 0.6411 - val_loss: 1.0482 - val_mae: 1.0482\n",
            "Epoch 336/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6134 - mae: 0.6134\n",
            "Epoch 336: val_loss improved from 0.28649 to 0.28355, saving model to best_model.h5\n",
            "59/59 [==============================] - 1s 12ms/step - loss: 0.6153 - mae: 0.6153 - val_loss: 0.2835 - val_mae: 0.2835\n",
            "Epoch 337/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6040 - mae: 0.6040\n",
            "Epoch 337: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6047 - mae: 0.6047 - val_loss: 0.6007 - val_mae: 0.6007\n",
            "Epoch 338/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6636 - mae: 0.6636\n",
            "Epoch 338: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6612 - mae: 0.6612 - val_loss: 0.9478 - val_mae: 0.9478\n",
            "Epoch 339/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6415 - mae: 0.6415\n",
            "Epoch 339: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6415 - mae: 0.6415 - val_loss: 0.3426 - val_mae: 0.3426\n",
            "Epoch 340/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6561 - mae: 0.6561\n",
            "Epoch 340: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6472 - mae: 0.6472 - val_loss: 1.6460 - val_mae: 1.6460\n",
            "Epoch 341/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6666 - mae: 0.6666\n",
            "Epoch 341: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6689 - mae: 0.6689 - val_loss: 1.0076 - val_mae: 1.0076\n",
            "Epoch 342/500\n",
            "53/59 [=========================>....] - ETA: 0s - loss: 0.6100 - mae: 0.6100\n",
            "Epoch 342: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.5994 - mae: 0.5994 - val_loss: 1.2674 - val_mae: 1.2674\n",
            "Epoch 343/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6152 - mae: 0.6152\n",
            "Epoch 343: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6152 - mae: 0.6152 - val_loss: 1.0536 - val_mae: 1.0536\n",
            "Epoch 344/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6938 - mae: 0.6938\n",
            "Epoch 344: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6904 - mae: 0.6904 - val_loss: 1.8348 - val_mae: 1.8348\n",
            "Epoch 345/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6287 - mae: 0.6287\n",
            "Epoch 345: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6282 - mae: 0.6282 - val_loss: 2.0895 - val_mae: 2.0895\n",
            "Epoch 346/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6880 - mae: 0.6880\n",
            "Epoch 346: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 13ms/step - loss: 0.6906 - mae: 0.6906 - val_loss: 0.8772 - val_mae: 0.8772\n",
            "Epoch 347/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.5915 - mae: 0.5915\n",
            "Epoch 347: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 16ms/step - loss: 0.5938 - mae: 0.5938 - val_loss: 0.4425 - val_mae: 0.4425\n",
            "Epoch 348/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6013 - mae: 0.6013\n",
            "Epoch 348: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 16ms/step - loss: 0.5956 - mae: 0.5956 - val_loss: 1.1944 - val_mae: 1.1944\n",
            "Epoch 349/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6834 - mae: 0.6834\n",
            "Epoch 349: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 16ms/step - loss: 0.6885 - mae: 0.6885 - val_loss: 1.0300 - val_mae: 1.0300\n",
            "Epoch 350/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7123 - mae: 0.7123\n",
            "Epoch 350: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.7123 - mae: 0.7123 - val_loss: 0.7666 - val_mae: 0.7666\n",
            "Epoch 351/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6812 - mae: 0.6812\n",
            "Epoch 351: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6601 - mae: 0.6601 - val_loss: 0.5235 - val_mae: 0.5235\n",
            "Epoch 352/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6342 - mae: 0.6342\n",
            "Epoch 352: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6342 - mae: 0.6342 - val_loss: 0.4040 - val_mae: 0.4040\n",
            "Epoch 353/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6911 - mae: 0.6911\n",
            "Epoch 353: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6886 - mae: 0.6886 - val_loss: 1.5370 - val_mae: 1.5370\n",
            "Epoch 354/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6885 - mae: 0.6885\n",
            "Epoch 354: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6886 - mae: 0.6886 - val_loss: 1.4287 - val_mae: 1.4287\n",
            "Epoch 355/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6564 - mae: 0.6564\n",
            "Epoch 355: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6503 - mae: 0.6503 - val_loss: 0.9427 - val_mae: 0.9427\n",
            "Epoch 356/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6423 - mae: 0.6423\n",
            "Epoch 356: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6402 - mae: 0.6402 - val_loss: 1.0910 - val_mae: 1.0910\n",
            "Epoch 357/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.5952 - mae: 0.5952\n",
            "Epoch 357: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.5938 - mae: 0.5938 - val_loss: 0.5428 - val_mae: 0.5428\n",
            "Epoch 358/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.5753 - mae: 0.5753\n",
            "Epoch 358: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.5673 - mae: 0.5673 - val_loss: 1.2642 - val_mae: 1.2642\n",
            "Epoch 359/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6449 - mae: 0.6449\n",
            "Epoch 359: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6449 - mae: 0.6449 - val_loss: 1.0284 - val_mae: 1.0284\n",
            "Epoch 360/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6635 - mae: 0.6635\n",
            "Epoch 360: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6635 - mae: 0.6635 - val_loss: 0.8845 - val_mae: 0.8845\n",
            "Epoch 361/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6252 - mae: 0.6252\n",
            "Epoch 361: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6250 - mae: 0.6250 - val_loss: 0.4594 - val_mae: 0.4594\n",
            "Epoch 362/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6307 - mae: 0.6307\n",
            "Epoch 362: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6330 - mae: 0.6330 - val_loss: 0.4640 - val_mae: 0.4640\n",
            "Epoch 363/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6328 - mae: 0.6328\n",
            "Epoch 363: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6328 - mae: 0.6328 - val_loss: 1.1681 - val_mae: 1.1681\n",
            "Epoch 364/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6605 - mae: 0.6605\n",
            "Epoch 364: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6613 - mae: 0.6613 - val_loss: 1.0533 - val_mae: 1.0533\n",
            "Epoch 365/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6651 - mae: 0.6651\n",
            "Epoch 365: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 12ms/step - loss: 0.6651 - mae: 0.6651 - val_loss: 0.5396 - val_mae: 0.5396\n",
            "Epoch 366/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6288 - mae: 0.6288\n",
            "Epoch 366: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 16ms/step - loss: 0.6288 - mae: 0.6288 - val_loss: 1.1060 - val_mae: 1.1060\n",
            "Epoch 367/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6285 - mae: 0.6285\n",
            "Epoch 367: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 14ms/step - loss: 0.6169 - mae: 0.6169 - val_loss: 1.2040 - val_mae: 1.2040\n",
            "Epoch 368/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6433 - mae: 0.6433\n",
            "Epoch 368: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 17ms/step - loss: 0.6664 - mae: 0.6664 - val_loss: 1.1120 - val_mae: 1.1120\n",
            "Epoch 369/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6345 - mae: 0.6345\n",
            "Epoch 369: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6377 - mae: 0.6377 - val_loss: 0.8660 - val_mae: 0.8660\n",
            "Epoch 370/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6383 - mae: 0.6383\n",
            "Epoch 370: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6403 - mae: 0.6403 - val_loss: 0.2955 - val_mae: 0.2955\n",
            "Epoch 371/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6427 - mae: 0.6427\n",
            "Epoch 371: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6483 - mae: 0.6483 - val_loss: 0.7150 - val_mae: 0.7150\n",
            "Epoch 372/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6451 - mae: 0.6451\n",
            "Epoch 372: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6541 - mae: 0.6541 - val_loss: 0.7898 - val_mae: 0.7898\n",
            "Epoch 373/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6255 - mae: 0.6255\n",
            "Epoch 373: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6222 - mae: 0.6222 - val_loss: 1.2403 - val_mae: 1.2403\n",
            "Epoch 374/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6182 - mae: 0.6182\n",
            "Epoch 374: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6182 - mae: 0.6182 - val_loss: 1.9279 - val_mae: 1.9279\n",
            "Epoch 375/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6006 - mae: 0.6006\n",
            "Epoch 375: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6000 - mae: 0.6000 - val_loss: 0.5310 - val_mae: 0.5310\n",
            "Epoch 376/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6637 - mae: 0.6637\n",
            "Epoch 376: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6645 - mae: 0.6645 - val_loss: 0.8826 - val_mae: 0.8826\n",
            "Epoch 377/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.5748 - mae: 0.5748\n",
            "Epoch 377: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.5744 - mae: 0.5744 - val_loss: 0.5573 - val_mae: 0.5573\n",
            "Epoch 378/500\n",
            "53/59 [=========================>....] - ETA: 0s - loss: 0.6319 - mae: 0.6319\n",
            "Epoch 378: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6300 - mae: 0.6300 - val_loss: 1.5766 - val_mae: 1.5766\n",
            "Epoch 379/500\n",
            "53/59 [=========================>....] - ETA: 0s - loss: 0.6993 - mae: 0.6993\n",
            "Epoch 379: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6950 - mae: 0.6950 - val_loss: 0.9631 - val_mae: 0.9631\n",
            "Epoch 380/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6661 - mae: 0.6661\n",
            "Epoch 380: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6706 - mae: 0.6706 - val_loss: 0.5040 - val_mae: 0.5040\n",
            "Epoch 381/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6360 - mae: 0.6360\n",
            "Epoch 381: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6369 - mae: 0.6369 - val_loss: 2.6565 - val_mae: 2.6565\n",
            "Epoch 382/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6675 - mae: 0.6675\n",
            "Epoch 382: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6582 - mae: 0.6582 - val_loss: 1.7777 - val_mae: 1.7777\n",
            "Epoch 383/500\n",
            "53/59 [=========================>....] - ETA: 0s - loss: 0.6333 - mae: 0.6333\n",
            "Epoch 383: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6417 - mae: 0.6417 - val_loss: 0.6139 - val_mae: 0.6139\n",
            "Epoch 384/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6128 - mae: 0.6128\n",
            "Epoch 384: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6135 - mae: 0.6135 - val_loss: 0.3232 - val_mae: 0.3232\n",
            "Epoch 385/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.7025 - mae: 0.7025\n",
            "Epoch 385: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 17ms/step - loss: 0.7025 - mae: 0.7025 - val_loss: 1.1687 - val_mae: 1.1687\n",
            "Epoch 386/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6244 - mae: 0.6244\n",
            "Epoch 386: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 15ms/step - loss: 0.6318 - mae: 0.6318 - val_loss: 0.5540 - val_mae: 0.5540\n",
            "Epoch 387/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6743 - mae: 0.6743\n",
            "Epoch 387: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 17ms/step - loss: 0.6748 - mae: 0.6748 - val_loss: 0.6966 - val_mae: 0.6966\n",
            "Epoch 388/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6545 - mae: 0.6545\n",
            "Epoch 388: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 13ms/step - loss: 0.6545 - mae: 0.6545 - val_loss: 1.0748 - val_mae: 1.0748\n",
            "Epoch 389/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6737 - mae: 0.6737\n",
            "Epoch 389: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6752 - mae: 0.6752 - val_loss: 1.0110 - val_mae: 1.0110\n",
            "Epoch 390/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6886 - mae: 0.6886\n",
            "Epoch 390: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6823 - mae: 0.6823 - val_loss: 0.8427 - val_mae: 0.8427\n",
            "Epoch 391/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6666 - mae: 0.6666\n",
            "Epoch 391: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6633 - mae: 0.6633 - val_loss: 1.4771 - val_mae: 1.4771\n",
            "Epoch 392/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.5953 - mae: 0.5953\n",
            "Epoch 392: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.5952 - mae: 0.5952 - val_loss: 0.6083 - val_mae: 0.6083\n",
            "Epoch 393/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6522 - mae: 0.6522\n",
            "Epoch 393: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6454 - mae: 0.6454 - val_loss: 1.0079 - val_mae: 1.0079\n",
            "Epoch 394/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6306 - mae: 0.6306\n",
            "Epoch 394: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6269 - mae: 0.6269 - val_loss: 1.1173 - val_mae: 1.1173\n",
            "Epoch 395/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.5728 - mae: 0.5728\n",
            "Epoch 395: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.5728 - mae: 0.5728 - val_loss: 1.2170 - val_mae: 1.2170\n",
            "Epoch 396/500\n",
            "53/59 [=========================>....] - ETA: 0s - loss: 0.5867 - mae: 0.5867\n",
            "Epoch 396: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.5977 - mae: 0.5977 - val_loss: 1.2995 - val_mae: 1.2995\n",
            "Epoch 397/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6335 - mae: 0.6335\n",
            "Epoch 397: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6275 - mae: 0.6275 - val_loss: 0.5875 - val_mae: 0.5875\n",
            "Epoch 398/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.5654 - mae: 0.5654\n",
            "Epoch 398: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.5654 - mae: 0.5654 - val_loss: 0.9423 - val_mae: 0.9423\n",
            "Epoch 399/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.5875 - mae: 0.5875\n",
            "Epoch 399: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.5994 - mae: 0.5994 - val_loss: 0.7394 - val_mae: 0.7394\n",
            "Epoch 400/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6577 - mae: 0.6577\n",
            "Epoch 400: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6577 - mae: 0.6577 - val_loss: 1.2118 - val_mae: 1.2118\n",
            "Epoch 401/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6379 - mae: 0.6379\n",
            "Epoch 401: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6381 - mae: 0.6381 - val_loss: 0.4190 - val_mae: 0.4190\n",
            "Epoch 402/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6583 - mae: 0.6583\n",
            "Epoch 402: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6593 - mae: 0.6593 - val_loss: 1.7139 - val_mae: 1.7139\n",
            "Epoch 403/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6159 - mae: 0.6159\n",
            "Epoch 403: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 13ms/step - loss: 0.6101 - mae: 0.6101 - val_loss: 0.6696 - val_mae: 0.6696\n",
            "Epoch 404/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6303 - mae: 0.6303\n",
            "Epoch 404: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 18ms/step - loss: 0.6303 - mae: 0.6303 - val_loss: 1.4911 - val_mae: 1.4911\n",
            "Epoch 405/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6110 - mae: 0.6110\n",
            "Epoch 405: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 16ms/step - loss: 0.6096 - mae: 0.6096 - val_loss: 1.0519 - val_mae: 1.0519\n",
            "Epoch 406/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6266 - mae: 0.6266\n",
            "Epoch 406: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 16ms/step - loss: 0.6263 - mae: 0.6263 - val_loss: 1.0634 - val_mae: 1.0634\n",
            "Epoch 407/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6366 - mae: 0.6366\n",
            "Epoch 407: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6288 - mae: 0.6288 - val_loss: 1.1904 - val_mae: 1.1904\n",
            "Epoch 408/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6827 - mae: 0.6827\n",
            "Epoch 408: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6816 - mae: 0.6816 - val_loss: 0.9759 - val_mae: 0.9759\n",
            "Epoch 409/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6165 - mae: 0.6165\n",
            "Epoch 409: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 16ms/step - loss: 0.6184 - mae: 0.6184 - val_loss: 0.9358 - val_mae: 0.9358\n",
            "Epoch 410/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6499 - mae: 0.6499\n",
            "Epoch 410: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6499 - mae: 0.6499 - val_loss: 1.0266 - val_mae: 1.0266\n",
            "Epoch 411/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6730 - mae: 0.6730\n",
            "Epoch 411: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6730 - mae: 0.6730 - val_loss: 0.8971 - val_mae: 0.8971\n",
            "Epoch 412/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6417 - mae: 0.6417\n",
            "Epoch 412: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6496 - mae: 0.6496 - val_loss: 1.1572 - val_mae: 1.1572\n",
            "Epoch 413/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6308 - mae: 0.6308\n",
            "Epoch 413: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6318 - mae: 0.6318 - val_loss: 0.7377 - val_mae: 0.7377\n",
            "Epoch 414/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.5702 - mae: 0.5702\n",
            "Epoch 414: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.5846 - mae: 0.5846 - val_loss: 0.8897 - val_mae: 0.8897\n",
            "Epoch 415/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6336 - mae: 0.6336\n",
            "Epoch 415: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6466 - mae: 0.6466 - val_loss: 0.7407 - val_mae: 0.7407\n",
            "Epoch 416/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6197 - mae: 0.6197\n",
            "Epoch 416: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6243 - mae: 0.6243 - val_loss: 0.5661 - val_mae: 0.5661\n",
            "Epoch 417/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.5872 - mae: 0.5872\n",
            "Epoch 417: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.5998 - mae: 0.5998 - val_loss: 0.9474 - val_mae: 0.9474\n",
            "Epoch 418/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6944 - mae: 0.6944\n",
            "Epoch 418: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6895 - mae: 0.6895 - val_loss: 0.4880 - val_mae: 0.4880\n",
            "Epoch 419/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.5699 - mae: 0.5699\n",
            "Epoch 419: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.5705 - mae: 0.5705 - val_loss: 0.7237 - val_mae: 0.7237\n",
            "Epoch 420/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6180 - mae: 0.6180\n",
            "Epoch 420: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6177 - mae: 0.6177 - val_loss: 0.8394 - val_mae: 0.8394\n",
            "Epoch 421/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6863 - mae: 0.6863\n",
            "Epoch 421: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 13ms/step - loss: 0.6869 - mae: 0.6869 - val_loss: 0.7200 - val_mae: 0.7200\n",
            "Epoch 422/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.5505 - mae: 0.5505\n",
            "Epoch 422: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 16ms/step - loss: 0.5509 - mae: 0.5509 - val_loss: 0.4681 - val_mae: 0.4681\n",
            "Epoch 423/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6344 - mae: 0.6344\n",
            "Epoch 423: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 14ms/step - loss: 0.6352 - mae: 0.6352 - val_loss: 0.6507 - val_mae: 0.6507\n",
            "Epoch 424/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.5780 - mae: 0.5780\n",
            "Epoch 424: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 17ms/step - loss: 0.5751 - mae: 0.5751 - val_loss: 0.6505 - val_mae: 0.6505\n",
            "Epoch 425/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6222 - mae: 0.6222\n",
            "Epoch 425: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 12ms/step - loss: 0.6191 - mae: 0.6191 - val_loss: 0.9325 - val_mae: 0.9325\n",
            "Epoch 426/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6551 - mae: 0.6551\n",
            "Epoch 426: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6520 - mae: 0.6520 - val_loss: 0.4594 - val_mae: 0.4594\n",
            "Epoch 427/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6494 - mae: 0.6494\n",
            "Epoch 427: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6431 - mae: 0.6431 - val_loss: 0.5681 - val_mae: 0.5681\n",
            "Epoch 428/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6407 - mae: 0.6407\n",
            "Epoch 428: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6497 - mae: 0.6497 - val_loss: 0.9451 - val_mae: 0.9451\n",
            "Epoch 429/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.5820 - mae: 0.5820\n",
            "Epoch 429: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.5845 - mae: 0.5845 - val_loss: 0.7918 - val_mae: 0.7918\n",
            "Epoch 430/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6419 - mae: 0.6419\n",
            "Epoch 430: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6411 - mae: 0.6411 - val_loss: 0.5203 - val_mae: 0.5203\n",
            "Epoch 431/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6270 - mae: 0.6270\n",
            "Epoch 431: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6279 - mae: 0.6279 - val_loss: 0.6380 - val_mae: 0.6380\n",
            "Epoch 432/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6226 - mae: 0.6226\n",
            "Epoch 432: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6246 - mae: 0.6246 - val_loss: 0.9049 - val_mae: 0.9049\n",
            "Epoch 433/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.5900 - mae: 0.5900\n",
            "Epoch 433: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.5900 - mae: 0.5900 - val_loss: 0.6242 - val_mae: 0.6242\n",
            "Epoch 434/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6627 - mae: 0.6627\n",
            "Epoch 434: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6579 - mae: 0.6579 - val_loss: 0.5955 - val_mae: 0.5955\n",
            "Epoch 435/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6048 - mae: 0.6048\n",
            "Epoch 435: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6048 - mae: 0.6048 - val_loss: 0.5559 - val_mae: 0.5559\n",
            "Epoch 436/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.5902 - mae: 0.5902\n",
            "Epoch 436: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.5915 - mae: 0.5915 - val_loss: 1.0106 - val_mae: 1.0106\n",
            "Epoch 437/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6926 - mae: 0.6926\n",
            "Epoch 437: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6951 - mae: 0.6951 - val_loss: 0.6707 - val_mae: 0.6707\n",
            "Epoch 438/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6106 - mae: 0.6106\n",
            "Epoch 438: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6241 - mae: 0.6241 - val_loss: 0.6758 - val_mae: 0.6758\n",
            "Epoch 439/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6120 - mae: 0.6120\n",
            "Epoch 439: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6048 - mae: 0.6048 - val_loss: 0.5287 - val_mae: 0.5287\n",
            "Epoch 440/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6234 - mae: 0.6234\n",
            "Epoch 440: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 15ms/step - loss: 0.6238 - mae: 0.6238 - val_loss: 0.7456 - val_mae: 0.7456\n",
            "Epoch 441/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6451 - mae: 0.6451\n",
            "Epoch 441: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 18ms/step - loss: 0.6442 - mae: 0.6442 - val_loss: 0.8031 - val_mae: 0.8031\n",
            "Epoch 442/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.5848 - mae: 0.5848\n",
            "Epoch 442: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 15ms/step - loss: 0.5840 - mae: 0.5840 - val_loss: 0.4583 - val_mae: 0.4583\n",
            "Epoch 443/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.5806 - mae: 0.5806\n",
            "Epoch 443: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 16ms/step - loss: 0.5789 - mae: 0.5789 - val_loss: 0.9412 - val_mae: 0.9412\n",
            "Epoch 444/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6246 - mae: 0.6246\n",
            "Epoch 444: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6235 - mae: 0.6235 - val_loss: 0.9897 - val_mae: 0.9897\n",
            "Epoch 445/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6050 - mae: 0.6050\n",
            "Epoch 445: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6021 - mae: 0.6021 - val_loss: 0.4151 - val_mae: 0.4151\n",
            "Epoch 446/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6119 - mae: 0.6119\n",
            "Epoch 446: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6170 - mae: 0.6170 - val_loss: 1.1383 - val_mae: 1.1383\n",
            "Epoch 447/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6299 - mae: 0.6299\n",
            "Epoch 447: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6350 - mae: 0.6350 - val_loss: 0.5421 - val_mae: 0.5421\n",
            "Epoch 448/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.5524 - mae: 0.5524\n",
            "Epoch 448: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.5494 - mae: 0.5494 - val_loss: 1.2691 - val_mae: 1.2691\n",
            "Epoch 449/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6835 - mae: 0.6835\n",
            "Epoch 449: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6828 - mae: 0.6828 - val_loss: 0.5536 - val_mae: 0.5536\n",
            "Epoch 450/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.5963 - mae: 0.5963\n",
            "Epoch 450: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6072 - mae: 0.6072 - val_loss: 0.7480 - val_mae: 0.7480\n",
            "Epoch 451/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.5926 - mae: 0.5926\n",
            "Epoch 451: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.5936 - mae: 0.5936 - val_loss: 0.8079 - val_mae: 0.8079\n",
            "Epoch 452/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6387 - mae: 0.6387\n",
            "Epoch 452: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6350 - mae: 0.6350 - val_loss: 0.3412 - val_mae: 0.3412\n",
            "Epoch 453/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.5632 - mae: 0.5632\n",
            "Epoch 453: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.5685 - mae: 0.5685 - val_loss: 0.7941 - val_mae: 0.7941\n",
            "Epoch 454/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6269 - mae: 0.6269\n",
            "Epoch 454: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6232 - mae: 0.6232 - val_loss: 0.8171 - val_mae: 0.8171\n",
            "Epoch 455/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6349 - mae: 0.6349\n",
            "Epoch 455: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.6453 - mae: 0.6453 - val_loss: 2.6336 - val_mae: 2.6336\n",
            "Epoch 456/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6239 - mae: 0.6239\n",
            "Epoch 456: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 16ms/step - loss: 0.6195 - mae: 0.6195 - val_loss: 1.4484 - val_mae: 1.4484\n",
            "Epoch 457/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6459 - mae: 0.6459\n",
            "Epoch 457: val_loss did not improve from 0.28355\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6369 - mae: 0.6369 - val_loss: 1.3965 - val_mae: 1.3965\n",
            "Epoch 458/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6663 - mae: 0.6663\n",
            "Epoch 458: val_loss improved from 0.28355 to 0.20990, saving model to best_model.h5\n",
            "59/59 [==============================] - 1s 15ms/step - loss: 0.6664 - mae: 0.6664 - val_loss: 0.2099 - val_mae: 0.2099\n",
            "Epoch 459/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6290 - mae: 0.6290\n",
            "Epoch 459: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 15ms/step - loss: 0.6292 - mae: 0.6292 - val_loss: 0.5776 - val_mae: 0.5776\n",
            "Epoch 460/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6749 - mae: 0.6749\n",
            "Epoch 460: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 18ms/step - loss: 0.6751 - mae: 0.6751 - val_loss: 0.6746 - val_mae: 0.6746\n",
            "Epoch 461/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6208 - mae: 0.6208\n",
            "Epoch 461: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 16ms/step - loss: 0.6208 - mae: 0.6208 - val_loss: 0.9544 - val_mae: 0.9544\n",
            "Epoch 462/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6056 - mae: 0.6056\n",
            "Epoch 462: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6086 - mae: 0.6086 - val_loss: 1.4627 - val_mae: 1.4627\n",
            "Epoch 463/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6543 - mae: 0.6543\n",
            "Epoch 463: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6566 - mae: 0.6566 - val_loss: 0.6597 - val_mae: 0.6597\n",
            "Epoch 464/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6277 - mae: 0.6277\n",
            "Epoch 464: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6288 - mae: 0.6288 - val_loss: 0.4092 - val_mae: 0.4092\n",
            "Epoch 465/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6110 - mae: 0.6110\n",
            "Epoch 465: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6112 - mae: 0.6112 - val_loss: 0.4884 - val_mae: 0.4884\n",
            "Epoch 466/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6436 - mae: 0.6436\n",
            "Epoch 466: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6439 - mae: 0.6439 - val_loss: 0.7680 - val_mae: 0.7680\n",
            "Epoch 467/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6146 - mae: 0.6146\n",
            "Epoch 467: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6142 - mae: 0.6142 - val_loss: 1.2141 - val_mae: 1.2141\n",
            "Epoch 468/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6582 - mae: 0.6582\n",
            "Epoch 468: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6605 - mae: 0.6605 - val_loss: 0.4701 - val_mae: 0.4701\n",
            "Epoch 469/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6643 - mae: 0.6643\n",
            "Epoch 469: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6806 - mae: 0.6806 - val_loss: 0.6236 - val_mae: 0.6236\n",
            "Epoch 470/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.5888 - mae: 0.5888\n",
            "Epoch 470: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.5968 - mae: 0.5968 - val_loss: 0.9374 - val_mae: 0.9374\n",
            "Epoch 471/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6357 - mae: 0.6357\n",
            "Epoch 471: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6393 - mae: 0.6393 - val_loss: 0.6903 - val_mae: 0.6903\n",
            "Epoch 472/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.5908 - mae: 0.5908\n",
            "Epoch 472: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6040 - mae: 0.6040 - val_loss: 0.5994 - val_mae: 0.5994\n",
            "Epoch 473/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6277 - mae: 0.6277\n",
            "Epoch 473: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6260 - mae: 0.6260 - val_loss: 0.4444 - val_mae: 0.4444\n",
            "Epoch 474/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6435 - mae: 0.6435\n",
            "Epoch 474: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6391 - mae: 0.6391 - val_loss: 1.2854 - val_mae: 1.2854\n",
            "Epoch 475/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6420 - mae: 0.6420\n",
            "Epoch 475: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6465 - mae: 0.6465 - val_loss: 1.2052 - val_mae: 1.2052\n",
            "Epoch 476/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6436 - mae: 0.6436\n",
            "Epoch 476: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6358 - mae: 0.6358 - val_loss: 0.6704 - val_mae: 0.6704\n",
            "Epoch 477/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6054 - mae: 0.6054\n",
            "Epoch 477: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 15ms/step - loss: 0.6078 - mae: 0.6078 - val_loss: 1.0398 - val_mae: 1.0398\n",
            "Epoch 478/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6166 - mae: 0.6166\n",
            "Epoch 478: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 17ms/step - loss: 0.6166 - mae: 0.6166 - val_loss: 1.0383 - val_mae: 1.0383\n",
            "Epoch 479/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.5836 - mae: 0.5836\n",
            "Epoch 479: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 15ms/step - loss: 0.5871 - mae: 0.5871 - val_loss: 0.7220 - val_mae: 0.7220\n",
            "Epoch 480/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6597 - mae: 0.6597\n",
            "Epoch 480: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 16ms/step - loss: 0.6601 - mae: 0.6601 - val_loss: 0.6175 - val_mae: 0.6175\n",
            "Epoch 481/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.6503 - mae: 0.6503\n",
            "Epoch 481: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6495 - mae: 0.6495 - val_loss: 1.1705 - val_mae: 1.1705\n",
            "Epoch 482/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6178 - mae: 0.6178\n",
            "Epoch 482: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6179 - mae: 0.6179 - val_loss: 0.6633 - val_mae: 0.6633\n",
            "Epoch 483/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6562 - mae: 0.6562\n",
            "Epoch 483: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6557 - mae: 0.6557 - val_loss: 0.5849 - val_mae: 0.5849\n",
            "Epoch 484/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.5557 - mae: 0.5557\n",
            "Epoch 484: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.5581 - mae: 0.5581 - val_loss: 1.7571 - val_mae: 1.7571\n",
            "Epoch 485/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.5647 - mae: 0.5647\n",
            "Epoch 485: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.5686 - mae: 0.5686 - val_loss: 1.1106 - val_mae: 1.1106\n",
            "Epoch 486/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6480 - mae: 0.6480\n",
            "Epoch 486: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6480 - mae: 0.6480 - val_loss: 0.6307 - val_mae: 0.6307\n",
            "Epoch 487/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6221 - mae: 0.6221\n",
            "Epoch 487: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6164 - mae: 0.6164 - val_loss: 0.9872 - val_mae: 0.9872\n",
            "Epoch 488/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.6548 - mae: 0.6548\n",
            "Epoch 488: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6521 - mae: 0.6521 - val_loss: 0.5982 - val_mae: 0.5982\n",
            "Epoch 489/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6255 - mae: 0.6255\n",
            "Epoch 489: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6174 - mae: 0.6174 - val_loss: 0.6596 - val_mae: 0.6596\n",
            "Epoch 490/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.6447 - mae: 0.6447\n",
            "Epoch 490: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6476 - mae: 0.6476 - val_loss: 0.8200 - val_mae: 0.8200\n",
            "Epoch 491/500\n",
            "54/59 [==========================>...] - ETA: 0s - loss: 0.5870 - mae: 0.5870\n",
            "Epoch 491: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.5951 - mae: 0.5951 - val_loss: 1.7683 - val_mae: 1.7683\n",
            "Epoch 492/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6411 - mae: 0.6411\n",
            "Epoch 492: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6436 - mae: 0.6436 - val_loss: 0.5430 - val_mae: 0.5430\n",
            "Epoch 493/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6429 - mae: 0.6429\n",
            "Epoch 493: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 12ms/step - loss: 0.6456 - mae: 0.6456 - val_loss: 0.8134 - val_mae: 0.8134\n",
            "Epoch 494/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.5853 - mae: 0.5853\n",
            "Epoch 494: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 10ms/step - loss: 0.5853 - mae: 0.5853 - val_loss: 0.9512 - val_mae: 0.9512\n",
            "Epoch 495/500\n",
            "55/59 [==========================>...] - ETA: 0s - loss: 0.5869 - mae: 0.5869\n",
            "Epoch 495: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.5768 - mae: 0.5768 - val_loss: 1.0842 - val_mae: 1.0842\n",
            "Epoch 496/500\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6535 - mae: 0.6535\n",
            "Epoch 496: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 18ms/step - loss: 0.6535 - mae: 0.6535 - val_loss: 0.4712 - val_mae: 0.4712\n",
            "Epoch 497/500\n",
            "56/59 [===========================>..] - ETA: 0s - loss: 0.5867 - mae: 0.5867\n",
            "Epoch 497: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 17ms/step - loss: 0.5932 - mae: 0.5932 - val_loss: 0.7927 - val_mae: 0.7927\n",
            "Epoch 498/500\n",
            "57/59 [===========================>..] - ETA: 0s - loss: 0.6182 - mae: 0.6182\n",
            "Epoch 498: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 16ms/step - loss: 0.6122 - mae: 0.6122 - val_loss: 1.2358 - val_mae: 1.2358\n",
            "Epoch 499/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6047 - mae: 0.6047\n",
            "Epoch 499: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 14ms/step - loss: 0.6055 - mae: 0.6055 - val_loss: 0.5368 - val_mae: 0.5368\n",
            "Epoch 500/500\n",
            "58/59 [============================>.] - ETA: 0s - loss: 0.6563 - mae: 0.6563\n",
            "Epoch 500: val_loss did not improve from 0.20990\n",
            "59/59 [==============================] - 1s 11ms/step - loss: 0.6563 - mae: 0.6563 - val_loss: 0.5756 - val_mae: 0.5756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reconstructions = model.predict(X_test)\n",
        "mse = np.mean(np.power(X_test - reconstructions, 2), axis=1)"
      ],
      "metadata": {
        "id": "Q01tavSRWczh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f26f5a2-8281-4591-d7c3-7e1946321161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30/30 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = np.mean(mse) + 4 * np.std(mse)"
      ],
      "metadata": {
        "id": "W_TgxGqPdl_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anomalies = np.where(mse > threshold)[0]\n",
        "print(\"Anomalies:\", anomalies)"
      ],
      "metadata": {
        "id": "0zbIRl5bdpYI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f0892fc-b286-4846-b50e-2bb82d2fbebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anomalies: [319]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(X_test, label='Original Data')\n",
        "plt.plot(reconstructions, label='Reconstructed Data')\n",
        "plt.scatter(anomalies, X_test[anomalies], color='red', marker='o', label='Anomalies')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Wkg9l5WQet5w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "a2b602a6-803d-4647-a002-fdd9115864b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAGdCAYAAAA8F1jjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABygElEQVR4nO3dd3gU5doG8Hu2pjdSISGhdwJSA4KgCCgqCBYUBRRBFJQiFsTCORZQUcEGykdRDgoWsGChCQiIICV0Qgs9hZZGypZ5vz822d3JlvTsovfvunJld+o7ZWeeedtIQggBIiIiIi+k8nQCiIiIiFxhoEJERERei4EKEREReS0GKkREROS1GKgQERGR12KgQkRERF6LgQoRERF5LQYqRERE5LU0nk5AVcmyjAsXLiAwMBCSJHk6OURERFQOQgjk5uaibt26UKlc55tc94HKhQsXEBcX5+lkEBERUSWcPXsWsbGxLsdf94FKYGAgAMuGBgUFeTg1REREVB45OTmIi4uz3sddue4DlZLinqCgIAYqRERE15myqm2wMi0RERF5LQYqRERE5LUYqBAREZHXuu7rqBARXY+EEDCZTDCbzZ5OClGNUKvV0Gg0Ve46hIEKEVEtMxgMSEtLQ35+vqeTQlSj/Pz8EBMTA51OV+llMFAhIqpFsiwjNTUVarUadevWhU6nY2eV9I8jhIDBYMDFixeRmpqKJk2auO3UzR0GKkREtchgMECWZcTFxcHPz8/TySGqMb6+vtBqtTh9+jQMBgN8fHwqtRxWpiUi8oDKPl0SXU+q4zznL4WIiIi8FgMVIiIi8loMVIiIqFacOnUKkiQhOTm53PMsXrwYISEhHk8HeQ4DFSIiKrezZ8/i0UcftbZYio+Px4QJE3D58uUy542Li0NaWhpat25d7vXdf//9OHr0aFWSXCm9evWCJEmQJAl6vR716tXDnXfeiRUrVlR4WdOnT0e7du2qP5H/EgxUiDzt7A5gx3xACE+nhMitkydPomPHjjh27Bi++uorHD9+HPPmzcP69euRlJSEK1euuJzXYDBArVYjOjoaGk35G5z6+voiMjKyOpJfYaNHj0ZaWhpOnDiB7777Di1btsTQoUMxZswYj6Tn34qBCpGnLbgV+GUKcHS1p1NCHiKEQL7BVOt/ooLB8bhx46DT6bBmzRrcdNNNqF+/Pm677TasW7cO58+fx7Rp06zTJiQk4LXXXsPw4cMRFBSEMWPGOC1y+fHHH9GkSRP4+Pigd+/e+PzzzyFJErKysgA4Fv2U5E4sWbIECQkJCA4OxtChQ5Gbm2ud5rfffsONN96IkJAQ1KlTB3fccQdOnDhR4ePi5+eH6OhoxMbGomvXrnjrrbfw6aefYv78+Vi3bp11uueffx5NmzaFn58fGjZsiJdffhlGo9Ga/v/85z/Yu3evNYdm8eLFAID33nsPbdq0gb+/P+Li4vDkk08iLy+vwun8p2M/KkTe4vIxAP09nQrygAKjGS1fqf1A9dB/+8FPV77bwJUrV7B69Wq88cYb8PX1VYyLjo7GsGHDsHz5cnzyySfWDuxmzZqFV155Ba+++qrTZaampuKee+7BhAkT8Nhjj2HPnj2YMmVKmWk5ceIEvv/+e6xatQpXr17Ffffdh5kzZ+KNN94AAFy7dg2TJ09G27ZtkZeXh1deeQV33303kpOTq9xcdsSIEXjmmWewYsUK9OnTBwAQGBiIxYsXo27duti/fz9Gjx6NwMBAPPfcc7j//vtx4MAB/Pbbb9bgJjg4GICl6e4HH3yABg0a4OTJk3jyySfx3HPP4ZNPPqlSGv9pGKgQEVGZjh07BiEEWrRo4XR8ixYtcPXqVVy8eNFaVHPzzTfjmWeesU5z6tQpxTyffvopmjVrhnfeeQcA0KxZMxw4cMAacLgiyzIWL16MwMBAAMDDDz+M9evXW+cbMmSIYvqFCxciIiIChw4dqlD9GGdUKhWaNm2q2JaXXnrJ+jkhIQFTpkzBsmXL8Nxzz8HX1xcBAQHQaDSIjo5WLGvixImK+V5//XWMHTuWgUopDFSIiDzMV6vGof/288h6K6oixUUdO3Z0Oz4lJQWdOnVSDOvcuXOZy01ISLAGKQAQExODzMxM6/djx47hlVdewfbt23Hp0iXIsgwAOHPmTJUDFcCyD+xfe7B8+XJ88MEHOHHiBPLy8mAymRAUFFTmctatW4cZM2bgyJEjyMnJgclkQmFhIfLz89lrsR3WUSEi8jBJkuCn09T6X0XeMdS4cWNIkoTDhw87HX/48GGEhoYiIiLCOszf37/K+8YZrVar+C5JkjUYAYA777wTV65cwfz587F9+3Zs374dgKVCb1WZzWYcO3YMDRo0AABs27YNw4YNw+23345Vq1Zhz549mDZtWpnrOnXqFO644w60bdsW3333HXbt2oWPP/642tL5T8IcFSIiKlOdOnVw66234pNPPsGkSZMU9VTS09OxdOlSDB8+vELBT7NmzfDLL78ohv39999VSufly5eRkpKC+fPno0ePHgCALVu2VGmZ9j7//HNcvXrVWrz0559/Ij4+XlGR+PTp04p5dDodzGazYtiuXbsgyzLeffdda72Zr7/+utrS+U/CHBUiIiqXjz76CEVFRejXrx/++OMPnD17Fr/99htuvfVW1KtXr8y6JaU9/vjjOHLkCJ5//nkcPXoUX3/9tbVFTGXfKB0aGoo6dergs88+w/Hjx/H7779j8uTJlVpWfn4+0tPTce7cOfz11194/vnnMXbsWDzxxBPo3bs3AKBJkyY4c+YMli1bhhMnTuCDDz7AypUrFctJSEhAamoqkpOTcenSJRQVFaFx48YwGo348MMPcfLkSSxZsgTz5s2rVDr/6RioEBFRuTRp0gQ7d+5Ew4YNcd9996FRo0YYM2YMevfujW3btiEsLKxCy2vQoAG+/fZbrFixAm3btsXcuXOtORN6vb5SaVSpVFi2bBl27dqF1q1bY9KkSdbKuhU1f/58xMTEoFGjRhg8eDAOHTpkbdlU4q677sKkSZMwfvx4tGvXDn/++SdefvllxXKGDBmC/v37o3fv3oiIiMBXX32FxMREvPfee3jrrbfQunVrLF26FDNmzKhUOv/pJFHRhvReJicnB8HBwcjOzi5X5SUirzPd0lQRfV8Huj3l2bRQjSssLERqaioaNGhQ6dfe/5O98cYbmDdvHs6ePevppFA1cHe+l/f+zToqRETkMZ988gk6deqEOnXqYOvWrXjnnXcwfvx4TyeLvAgDFSIi8phjx47h9ddfx5UrV1C/fn0888wzmDp1qqeTRV6EgQoREXnM+++/j/fff9/TySAvxsq0RERE5LUYqBAREZHXYqBCREREXouBChEREXktBipERETktWo0UJkxYwY6deqEwMBAREZGYtCgQUhJSVFMU1hYiHHjxqFOnToICAjAkCFDkJGRUZPJIiIiuu6MHDkSgwYN8nQyal2NBiqbNm3CuHHj8Ndff2Ht2rUwGo3o27cvrl27Zp1m0qRJ+Omnn/DNN99g06ZNuHDhAgYPHlyTySIiokoYOXIkJEmCJEnQarVo0KABnnvuORQWFno6aeW2ceNGSJKErKysWllfbQYXJdsmSRJUKhWCg4PRvn17PPfcc0hLS6vw8iRJwvfff1/9Ca2gGu1H5bffflN8X7x4MSIjI7Fr1y707NkT2dnZWLBgAb788kvcfPPNAIBFixahRYsW+Ouvv9C1a9eaTB4REVVQ//79sWjRIhiNRuzatQsjRoyAJEl46623PJ20amUwGKDT6TydjEpJSUlBUFAQcnJysHv3brz99ttYsGABNm7ciDZt2ng6eRVWq3VUsrOzAcD64qpdu3bBaDSiT58+1mmaN2+O+vXrY9u2bU6XUVRUhJycHMUfERHVDr1ej+joaMTFxWHQoEHo06cP1q5dax0vyzJmzJiBBg0awNfXF4mJifj2228Vyzh48CDuuOMOBAUFITAwED169MCJEyes8//3v/9FbGws9Ho92rVrp3joPXXqFCRJwooVK9C7d2/4+fkhMTFRcc84ffo07rzzToSGhsLf3x+tWrXCL7/8glOnTlnfehwaGgpJkjBy5EgAQK9evTB+/HhMnDgR4eHh6Nevn3VdycnJ1mVnZWVBkiRs3LixzO2ZPn06Pv/8c/zwww/WnI6S+c6ePYv77rsPISEhCAsLw8CBA3Hq1CnrMs1mMyZPnoyQkBDUqVMHzz33HMr7ar7IyEhER0ejadOmGDp0KLZu3YqIiAg88cQT1mn+/vtv3HrrrQgPD0dwcDBuuukm7N692zo+ISEBAHD33XdDkiTr9xMnTmDgwIGIiopCQEAAOnXqhHXr1pUrXZVVa4GKLMuYOHEiunfvjtatWwMA0tPTodPpEBISopg2KioK6enpTpczY8YMBAcHW//i4uJqOulERDVLCMBwrfb/qvhO2gMHDuDPP/9U5DzMmDEDX3zxBebNm4eDBw9i0qRJeOihh7Bp0yYAwPnz59GzZ0/o9Xr8/vvv2LVrFx599FGYTCYAwJw5c/Duu+9i1qxZ2LdvH/r164e77roLx44dU6x72rRpmDJlCpKTk9G0aVM88MAD1mWMGzcORUVF+OOPP7B//3689dZbCAgIQFxcHL777jsAllyHtLQ0zJkzx7rMzz//HDqdDlu3bsW8efPKtQ/cbc+UKVNw3333oX///khLS0NaWhq6desGo9GIfv36ITAwEJs3b8bWrVsREBCA/v37w2AwAADeffddLF68GAsXLsSWLVtw5coVrFy5slLHydfXF2PHjsXWrVuRmZkJAMjNzcWIESOwZcsW/PXXX2jSpAluv/125ObmArAEMoCllCMtLc36PS8vD7fffjvWr1+PPXv2oH///rjzzjtx5syZSqWtPGqtC/1x48bhwIED2LJlS5WWM3XqVEyePNn6PScnh8EKEV3fjPnAm3Vrf70vXgB0/hWaZdWqVQgICIDJZEJRURFUKhU++ugjAJYc7zfffBPr1q1DUlISAKBhw4bYsmULPv30U9x00034+OOPERwcjGXLlkGr1QIAmjZtal3+rFmz8Pzzz2Po0KEAgLfeegsbNmzA7Nmz8fHHH1unmzJlCgYMGAAA+M9//oNWrVrh+PHjaN68Oc6cOYMhQ4ZYizkaNmxona8kRz8yMtLhIblJkyZ4++23rd/tczhcKWt7fH19UVRUhOjoaOuw//3vf5BlGf/3f/8HSZIAWAKCkJAQbNy4EX379sXs2bMxdepUa53NefPmYfXq1WWmx5XmzZtbtykyMtJa3aLEZ599hpCQEGzatAl33HEHIiIiAAAhISGKtCcmJiIxMdH6/bXXXsPKlSvx448/1tjLJGslUBk/fjxWrVqFP/74A7Gxsdbh0dHRMBgMyMrKUpwwGRkZih1jT6/XQ6/X13SSiYjIid69e2Pu3Lm4du0a3n//fWg0GgwZMgQAcPz4ceTn5+PWW29VzGMwGNC+fXsAQHJyMnr06GG9qdvLycnBhQsX0L17d8Xw7t27Y+/evYphbdu2tX6OiYkBAGRmZqJ58+Z4+umn8cQTT2DNmjXo06cPhgwZopjelQ4dOpRjDyi52x5X9u7di+PHjyMwMFAxvLCwECdOnEB2djbS0tLQpUsX6ziNRoOOHTuWu/intJL5SgKjjIwMvPTSS9i4cSMyMzNhNpuRn59fZs5IXl4epk+fjp9//hlpaWkwmUwoKCi4fnNUhBB46qmnsHLlSmzcuBENGjRQjO/QoQO0Wi3Wr19vPdFTUlJw5swZazRORPSPp/Wz5G54Yr0V5O/vj8aNGwMAFi5ciMTERCxYsACjRo1CXl4eAODnn39GvXr1FPOVPGD6+vpWMdEW9oFByc1XlmUAwGOPPYZ+/frh559/xpo1azBjxgy8++67eOqpp8rcNnsqlaV2hH1wYDQaFdNUZnvy8vLQoUMHLF261GFcSU5GdTt8+DAAW92TESNG4PLly5gzZw7i4+Oh1+uRlJRkLXpyZcqUKVi7di1mzZqFxo0bw9fXF/fcc0+Z81VFjQYq48aNw5dffokffvgBgYGB1nonwcHB8PX1RXBwMEaNGoXJkycjLCwMQUFBeOqpp5CUlMQWP0T07yFJFS6C8QYqlQovvvgiJk+ejAcffBAtW7aEXq/HmTNncNNNNzmdp23btvj8889hNBodciGCgoJQt25dbN26VTH/1q1b0blz5wqlLS4uDmPHjsXYsWMxdepUzJ8/H0899ZS1Po3ZbC5zGSVBQ1pamiJHqLzbAwA6nc5hXTfccAOWL1+OyMhIBAUFOV13TEwMtm/fjp49ewIATCYTdu3ahRtuuKHMdJdWUFCAzz77DD179rRu09atW/HJJ5/g9ttvB2Cp3Hvp0iXFfFqt1iHtW7duxciRI3H33XcDsARd5Skiq4oarUw7d+5cZGdno1evXoiJibH+LV++3DrN+++/jzvuuANDhgxBz549ER0djRUrVtRksoiIqJrce++9UKvV+PjjjxEYGIgpU6Zg0qRJ+Pzzz3HixAns3r0bH374IT7//HMAlqoAOTk5GDp0KHbu3Iljx45hyZIl1s5An332Wbz11ltYvnw5UlJS8MILLyA5ORkTJkwod5omTpyI1atXIzU1Fbt378aGDRvQokULAEB8fDwkScKqVatw8eJFay6QM76+vujatStmzpyJw4cPY9OmTXjppZcU05S1PQkJCdi3bx9SUlJw6dIlGI1GDBs2DOHh4Rg4cCA2b96M1NRUbNy4EU8//TTOnTsHAJgwYQJmzpyJ77//HkeOHMGTTz5Z7r5fMjMzkZ6ejmPHjmHZsmXo3r07Ll26hLlz51qnadKkCZYsWYLDhw9j+/btGDZsmEPuUEJCAtavX4/09HRcvXrVOt+KFSuQnJyMvXv34sEHH7TmZNUYcZ3Lzs4WAER2drank0JUOa8GWf62fuDplFAtKCgoEIcOHRIFBQWeTkqFjRgxQgwcONBh+IwZM0RERITIy8sTsiyL2bNni2bNmgmtVisiIiJEv379xKZNm6zT7927V/Tt21f4+fmJwMBA0aNHD3HixAkhhBBms1lMnz5d1KtXT2i1WpGYmCh+/fVX67ypqakCgNizZ4912NWrVwUAsWHDBiGEEOPHjxeNGjUSer1eREREiIcfflhcunTJOv1///tfER0dLSRJEiNGjBBCCHHTTTeJCRMmOGzboUOHRFJSkvD19RXt2rUTa9asUayrrO3JzMwUt956qwgICFDMl5aWJoYPHy7Cw8OFXq8XDRs2FKNHj7bey4xGo5gwYYIICgoSISEhYvLkyWL48OFO93+JDRs2CAACgJAkSQQGBorExETx7LPPirS0NMW0u3fvFh07dhQ+Pj6iSZMm4ptvvhHx8fHi/ffft07z448/isaNGwuNRiPi4+Ot+793797C19dXxMXFiY8++sjlvhPC/fle3vu3JEQV26d5WE5ODoKDg5Gdne0yC43Iq00Ptvzv+zrQzX0ZOl3/CgsLkZqaigYNGsDHx8fTySGqUe7O9/Lev/lSQiIiIvJaDFSIiIjIazFQISIiIq/FQIWIiIi8FgMVIiIi8loMVIiIiMhrMVAhIiIir8VAhYiIiLwWAxUiIiLyWgxUiIjoXyEhIQGzZ8+2fpckCd9//73H0kPlw0CFiIgqZNu2bVCr1RgwYICnk1IlaWlpuO222zydDCoDAxUiouuV2Qxs3Ah89ZXlv9lcK6tdsGABnnrqKfzxxx+4cOFCrayzJkRHR0Ov13s6GVQGBipERNejFSuAhASgd2/gwQct/xMSLMNrUF5eHpYvX44nnngCAwYMwOLFi63jNm7cCEmSsH79enTs2BF+fn7o1q0bUlJSFMuYO3cuGjVqBJ1Oh2bNmmHJkiWK8ZIk4dNPP8Udd9wBPz8/tGjRAtu2bcPx48fRq1cv+Pv7o1u3bjhx4oR1nhMnTmDgwIGIiopCQEAAOnXqhHXr1rndltJFP2fPnsV9992HkJAQhIWFYeDAgTh16pRi+zp37gx/f3+EhISge/fuOH36dMV3IlUIAxUiouvNihXAPfcA584ph58/bxleg8HK119/jebNm6NZs2Z46KGHsHDhQgghFNNMmzYN7777Lnbu3AmNRoNHH33UOm7lypWYMGECnnnmGRw4cACPP/44HnnkEWzYsEGxjNdeew3Dhw9HcnIymjdvjgcffBCPP/44pk6dip07d0IIgfHjx1unz8vLw+23347169djz5496N+/P+68806cOXOmXNtlNBrRr18/BAYGYvPmzdi6dSsCAgLQv39/GAwGmEwmDBo0CDfddBP27duHbdu2YcyYMZAkqQp7k8pFXOeys7MFAJGdne3ppBBVzqtBlr+tH3g6JVQLCgoKxKFDh0RBQUHlFmAyCREbKwTg/E+ShIiLs0xXA7p16yZmz54thBDCaDSK8PBwsWHDBiGEEBs2bBAAxLp166zT//zzzwKAdXu7desmRo8erVjmvffeK26//XbrdwDipZdesn7ftm2bACAWLFhgHfbVV18JHx8ft2lt1aqV+PDDD63f4+Pjxfvvv69Yz8qVK4UQQixZskQ0a9ZMyLJsHV9UVCR8fX3F6tWrxeXLlwUAsXHjRrfrJCV353t579/MUSEiup5s3uyYk2JPCODsWct01SwlJQU7duzAAw88AADQaDS4//77sWDBAsV0bdu2tX6OiYkBAGRmZgIADh8+jO7duyum7969Ow4fPuxyGVFRUQCANm3aKIYVFhYiJycHgCVHZcqUKWjRogVCQkIQEBCAw4cPlztHZe/evTh+/DgCAwMREBCAgIAAhIWFobCwECdOnEBYWBhGjhyJfv364c4778ScOXOQlpZWrmVT1Wg8nQAiIqqA8t4ca+AmumDBAphMJtStW9c6TAgBvV6Pjz76yDpMq9VaP5cUjciyXKF1OVuGu+VOmTIFa9euxaxZs9C4cWP4+vrinnvugcFgKNf68vLy0KFDByxdutRhXEREBABg0aJFePrpp/Hbb79h+fLleOmll7B27Vp07dq1QttGFcNAhYjoelKcQ1Ft05WTyWTCF198gXfffRd9+/ZVjBs0aBC++uorNG/evMzltGjRAlu3bsWIESOsw7Zu3YqWLVtWKX1bt27FyJEjcffddwOwBB72FWHLcsMNN2D58uWIjIxEUFCQy+nat2+P9u3bY+rUqUhKSsKXX37JQKWGseiHiOh60qMHEBsLuKrEKUlAXJxlumq0atUqXL16FaNGjULr1q0Vf0OGDHEo/nHl2WefxeLFizF37lwcO3YM7733HlasWIEpU6ZUKX1NmjTBihUrkJycjL179+LBBx+sUC7OsGHDEB4ejoEDB2Lz5s1ITU3Fxo0b8fTTT+PcuXNITU3F1KlTsW3bNpw+fRpr1qzBsWPH0KJFiyqlm8rGQIWI6HqiVgNz5lg+lw5WSr7Pnm2ZrhotWLAAffr0QXBwsMO4IUOGYOfOndi3b1+Zyxk0aBDmzJmDWbNmoVWrVvj000+xaNEi9OrVq0rpe++99xAaGopu3brhzjvvRL9+/XDDDTeUe34/Pz/88ccfqF+/PgYPHowWLVpg1KhRKCwsRFBQEPz8/HDkyBEMGTIETZs2xZgxYzBu3Dg8/vjjVUo3lU0SolS7sutMTk4OgoODkZ2d7Ta7jshrTS++8Pd9Hej2lGfTQjWusLAQqampaNCgAXx8fCq/oBUrgAkTlBVr4+IsQcrgwVVOJ1F1cHe+l/f+zToqRETXo8GDgYEDLa170tIsdVJ69Kj2nBQiT2OgQkR0vVKrgSoWmRB5O9ZRISIiIq/FQIWIiIi8FgMVIiIi8loMVIiIPOA6b3BJVC7VcZ4zUCEiqkUl3cDn5+d7OCVENa/kPLd//UFFsdUPEVEtUqvVCAkJsb6kz8/Pz/reGqJ/CiEE8vPzkZmZiZCQEKir0GyegQoRUS2Ljo4GYHujMNE/VUhIiPV8rywGKkREtUySJMTExCAyMhJGo9HTySGqEVqttko5KSUYqBAReYhara6WCznRPxkr0xIREZHXYqBCREREXouBChEREXktBipERETktRioEBERkddioEJERERei4EKEREReS0GKkREROS1GKgQERGR12KgQkRERF6LgQoRERF5LQYqRERE5LUYqBAREZHXYqBCREREXouBChEREXktBipERETktRioEBERkddioEJERERei4EKEREReS0GKkREROS1GKgQERGR12KgQkRERF6LgQoRERF5LQYqRJ5kNts+nzih/E5ERAxUiDxmxQogIcH2fe5cy/cVKzyVIiIir8NAhcgTVqwA7rkHOHdOOfz8ectwBitERAAYqBDVPrMZmDABEMJxXMmwiRNZDEREBAYqRLVv82bHnBQARklt+SAEcPasZToion85rwhUPv74YyQkJMDHxwddunTBjh07PJ0kopqTluZ08L6YJuWajojo38Tjgcry5csxefJkvPrqq9i9ezcSExPRr18/ZGZmejppRDUjJsbpYKNaU67piIj+TTweqLz33nsYPXo0HnnkEbRs2RLz5s2Dn58fFi5c6OmkEdWMHj2A2FhAkpyPlyQgLs4yHRHRv5xHAxWDwYBdu3ahT58+1mEqlQp9+vTBtm3bnM5TVFSEnJwcxR/RdUWtBubMsXwuHayUfJ892zIdEdG/nEcDlUuXLsFsNiMqKkoxPCoqCunp6U7nmTFjBoKDg61/cXFxtZFUouo1eDDw7bdAvXrK4bGxluGDB3smXUREXsbjRT8VNXXqVGRnZ1v/zp496+kkEVXO4MHAqVO27926AampDFKIiOxoyp6k5oSHh0OtViMjI0MxPCMjA9HR0U7n0ev10Ov1tZE8oppnX7xTpw6Le4iISvFojopOp0OHDh2wfv166zBZlrF+/XokJSV5MGVEtcRZp29ERGTl0RwVAJg8eTJGjBiBjh07onPnzpg9ezauXbuGRx55xNNJIyIiIg/zeKBy//334+LFi3jllVeQnp6Odu3a4bfffnOoYEv0j8QcFSIitzweqADA+PHjMX78eE8ng4iIiLzMddfqh4iIiP49GKgQeRSLfoiI3GGgQkRERF6LgQqRJ7EyLRGRWwxUiIiIyGsxUCEiIiKvxUCFyKNY9ENE5A4DFSIiIvJaDFSIPImVaYmI3GKgQkRERF6LgQoRERF5LQYqRB7Foh8iIncYqBAREZHXYqBCREREXouBCpEnsdUPEZFbDFSIiIjIazFQIfIo5qgQEbnDQIWIiIi8FgMVIiIi8loMVIg8iZVpiYjcYqBCREREXouBCpFHMUeFiMgdBipERETktRioEHkNydMJICLyOgxUiDxJUZmWxUBERKUxUCEiIiKvxUCFiIiIvBYDFSKPYnEPEZE7DFSIiIjIazFQIfIk9kxLROQWAxUiIiLyWgxUiIiIyGsxUCHyKBb9EBG5w0CFiIiIvBYDFSJPYmVaIiK3GKgQERGR12KgQuQ1+FJCIqLSGKgQeRRfSkhE5A4DFSIiIvJaDFSIiIjIazFQIfIktvohInKLgQoRERF5LQYqRERE5LUYqBAREZHXYqBC5DXYjwoRUWkMVIg8SbAfFSIidxioEBERkddioELkUcxFISJyh4EKEREReS0GKkReg5VpiYhKY6BC5EmsTEtE5BYDFSIiIvJaDFSIPIq5KERE7jBQISIiIq/FQIWIiIi8FgMVIk+yq0wrsRiIiMgBAxUib8E4hYjIAQMVIiIi8loMVIg8iv2oEBG5w0CFyIOEYHBCROROjQUqp06dwqhRo9CgQQP4+vqiUaNGePXVV2EwGBTT7du3Dz169ICPjw/i4uLw9ttv11SSiLwQK9MSEbmjqakFHzlyBLIs49NPP0Xjxo1x4MABjB49GteuXcOsWbMAADk5Oejbty/69OmDefPmYf/+/Xj00UcREhKCMWPG1FTSiLyGEHzDDxGROzUWqPTv3x/9+/e3fm/YsCFSUlIwd+5ca6CydOlSGAwGLFy4EDqdDq1atUJycjLee+89Bir0r8A8FCIi92q1jkp2djbCwsKs37dt24aePXtCp9NZh/Xr1w8pKSm4evWq02UUFRUhJydH8Ud03bKvo8L6KkREDmotUDl+/Dg+/PBDPP7449Zh6enpiIqKUkxX8j09Pd3pcmbMmIHg4GDrX1xcXM0lmqiGsTItEZF7FQ5UXnjhBUiS5PbvyJEjinnOnz+P/v37495778Xo0aOrlOCpU6ciOzvb+nf27NkqLY/IsxioEBG5U+E6Ks888wxGjhzpdpqGDRtaP1+4cAG9e/dGt27d8Nlnnymmi46ORkZGhmJYyffo6Giny9br9dDr9RVNNpFXEm6+ERFRJQKViIgIRERElGva8+fPo3fv3ujQoQMWLVoElUqZgZOUlIRp06bBaDRCq9UCANauXYtmzZohNDS0okkjIiKif5gaq6Ny/vx59OrVC/Xr18esWbNw8eJFpKenK+qePPjgg9DpdBg1ahQOHjyI5cuXY86cOZg8eXJNJYvIqwjZvh8VIiIqrcaaJ69duxbHjx/H8ePHERsbqxhXUoEwODgYa9aswbhx49ChQweEh4fjlVdeYdNkIiIiAlCDgcrIkSPLrMsCAG3btsXmzZtrKhlEXk2wXgoRkVt81w+R12DQQkRUGgMVIg9iNypERO4xUCHyKLvKtIxaiIgcMFAhIiIir8VAhYiIiLwWAxUiT7LrRwUSi36IiEpjoEJERERei4EKkQcJyJ5OAhGRV2OgQuQl2OqHiMgRAxUiD2JsQkTkHgMVIg8SjFSIiNxioEJERERei4EKkQcJwcq0RETuMFAh8hISX0pIROSAgQqRBzE0ISJyj4EKkUcxVCEicoeBCpEn2cUpbAFEROSIgQoRERF5LQYqRJ5k91JCVqYlInLEQIWIiIi8FgMVIg8SzEUhInKLgQqR12DQQkRUGgMVIg9iQx8iIvcYqBB5lF1lWgYtREQOGKgQeRBjEyIi9xioEHkSy36IiNxioELkQfa90Uos+yEicsBAhYiIiLwWAxUij7LlorAUiIjIEQMVIg9SBieMVIiISmOgQuQ1GKgQEZXGQIXIgwRkTyeBiMirMVAh8hbMUCEicsBAhciTZEYnRETuMFAh8hoMWoiISmOgQuRBDE2IiNxjoELkUexHhYjIHQYqRJ5kF5xIzF8hInLAQIXIo4STT0REVIKBCpEH2QcnzFEhInLEQIXIS7COChGRIwYqRB4k2I8KEZFbDFSIvASLfoiIHDFQIfIgweCEiMgtBipEniTY6oeIyB0GKkTegrVpiYgcMFAh8iDB4ISIyC0GKkReg0ELEVFpDFSIiIjIazFQIfIk+6IfZqgQETlgoELkQcLNNyIiYqBC5FmsTEtE5BYDFSKPEi4+ExERwECFyHswTiEicsBAhciDWPJDROQeAxUir8GohYioNAYqRB7EnmmJiNxjoEJERERei4EKkQcJtvohInKLgQqRB0ks+iEicouBCpEHKcIUBi1ERA5qJVApKipCu3btIEkSkpOTFeP27duHHj16wMfHB3FxcXj77bdrI0lEXoGxCRGRe7USqDz33HOoW7euw/CcnBz07dsX8fHx2LVrF9555x1Mnz4dn332WW0ki8gLMFIhInJHU9Mr+PXXX7FmzRp89913+PXXXxXjli5dCoPBgIULF0Kn06FVq1ZITk7Ge++9hzFjxtR00oi8ACvTEhG5U6M5KhkZGRg9ejSWLFkCPz8/h/Hbtm1Dz549odPprMP69euHlJQUXL161ekyi4qKkJOTo/gjun4xOCEicqfGAhUhBEaOHImxY8eiY8eOTqdJT09HVFSUYljJ9/T0dKfzzJgxA8HBwda/uLi46k04US2yr6PC+ipERI4qHKi88MILkCTJ7d+RI0fw4YcfIjc3F1OnTq3WBE+dOhXZ2dnWv7Nnz1br8ok8RWLuChGRgwrXUXnmmWcwcuRIt9M0bNgQv//+O7Zt2wa9Xq8Y17FjRwwbNgyff/45oqOjkZGRoRhf8j06OtrpsvV6vcMyia5bdtkoDFOIiBxVOFCJiIhAREREmdN98MEHeP31163fL1y4gH79+mH58uXo0qULACApKQnTpk2D0WiEVqsFAKxduxbNmjVDaGhoRZNGdF1jjgoRkaMaa/VTv359xfeAgAAAQKNGjRAbGwsAePDBB/Gf//wHo0aNwvPPP48DBw5gzpw5eP/992sqWURehXVUiIjcq/Hmye4EBwdjzZo1GDduHDp06IDw8HC88sorbJpMREREAGoxUElISHD6Svu2bdti8+bNtZUMIq9i/1JCFv0QETniu36IPIjBCRGRewxUiDxImcnIoIWIqDQGKkQexeCEiMgdBipEHsSWPkRE7jFQIfIWjFqIiBwwUCHyKNnTCSAi8moMVIi8BPNTiIgcMVAh8iD70h6JRT9ERA4YqBB5El9KSETkFgMVIi8heToBREReiIEKkQcpXkrouWQQEXktBipERETktRioEHmU/UsJ2VSZiKg0BipEHsUCHyIidxioEHmQoo4KYxYiIgcMVIg8yr7oh4iISmOgQkRERF6LgQqR12DZDxFRaQxUiDxICLuWPoxTiIgcMFAhIiIir8VAhciDhJtvRETEQIXIsxibEBG5xUCFyJOEyy9ERAQGKkQexuCEiMgdBipEHsVAhYjIHQYqRB6k6DaffegTETlgoELkUQxOiIjcYaBCREREXouBCpFHCRefiYgIYKBC5FGslkJE5B4DFSJvwaiFiMgBAxUiT2JwQkTkFgMVIiIi8loMVIg8SNjlqEisTEtE5ICBCpGXYJhCROSIgQqRB/GdhERE7jFQIfIgSbAfFSIidxioEHmQYHBCROQWAxUiL8HKtEREjhioEHmSXdEPwxQiIkcMVIiIiMhrMVAh8ijmoxARucNAhciD2IM+EZF7DFSIvITEqIWIyAEDFSIPYvNkIiL3GKgQeQkGLUREjhioEHmS4qWERERUGgMVIiIi8loMVIiq4vIJ4PBPlW6+w8IeqlVHVwOZhz2z7rxMYP+3gMngmfXTdYuBClFVfHgDsPwh4NjaSs2uaOnDVj9Uk87vBr68D/ikq2fWP/9m4LtRwJb3PLN+um4xUCGqDud2VGo2VqClWpNxwLPrzz5r+X/kZ8+mg647DFSIqkV1VIVl0EI1yUuqa0tekg66bjBQIaoOlbz4srSH/nUk3naoYnjGEFULPiWSl/OanAxvSQddLxioUM1K/QP4rDdwIdnTKalZlbwJKLvN94LslYxDwGe9gGPrPJ0S8hayDHz1ALDmpepZ3oXdwJb3q2dZ9K/AQIVq1ud3Wi5MS+/xdEpqWDU8JXpBnIJlDwAX9gBLh3g6JeQtzmwDUn4B/vyw+pa5bnr1LYv+8RioUO24dsnTKfBKXtfqJ/ucp1NANcYumJbN5Z/NWFD9SSGqAAYqVDu8pny8Zpy+kl/JOb2s6Ec2eToFVEPOZ9kFHBU5zqICQQ1RDWCgQrXjH17T/9zVyj11stUP1ZZz9oGK2Vj+Ge2DGlmuvgQRldM/++5BNafCF6wazFHxxMVTlpVRRmUr00I4/VxtSqezupZZE4Tw7I2wqusua37ZDJiKqraOKrEv+nGSo1J6/5d8ti8mkisQ4NSW8h43BlnXLQYqVHF5F4F3mwK/PFvuWeSaClQuHQfeTgA2vV0zy3dm0zvA2w2U70zxxn5UZDPw2U3A4gHVt6KCq8B7LYAfn6qe5dn79hHgg3aAobLFaFWw4nFgdmugMKdy868c635+Qz7wQXvg7YaWysqe5ixQsd//Kb8BM+OAQz9AKAKVaiwarI5z8tJxy2+xrN//zkXAzPrAme1VXyfVuhoNVH7++Wd06dIFvr6+CA0NxaBBgxTjz5w5gwEDBsDPzw+RkZF49tlnYTKxjNzr7fgUuHYR2PFZuWcxyjV0R173KlCYDWx4o2aW78yG14HCLGD1i9ZBUiUDsRqtoXIlFUjfB5zeCpir6UVwyV8CeenA7i+qZ3n2Dq4Esk4Dx9ZU/7LLsm8ZkHPe8oLJytj7lWX+gyucj79ywrJthjzg3M7Kp7MKVLDPLXFynS3Z/yfWA1/db0nr18NxMjPbNk1FiozKUh3LWvOS5bdY1u9/1UTAkGsJxui6o6mpBX/33XcYPXo03nzzTdx8880wmUw4cMD2rgmz2YwBAwYgOjoaf/75J9LS0jB8+HBotVq8+eabNZUsqg6VeKqqsZyDirReqPZ12+2Hain6qWbC7sZkKgI0+nLPahYS1GUts8Z4sOKOWlu1+V2dj8ZCu2k88zCmsqsUK8wG1+ebSnlbKCi0K66qzt+b2QBodFVbRkXPR1Nh2dOQ16mRQMVkMmHChAl45513MGrUKOvwli1bWj+vWbMGhw4dwrp16xAVFYV27drhtddew/PPP4/p06dDp6viCUw1pxIXWrmmMu9q5cbpguKiXQ1FP9V9f7ZvrVHBuhGF0MHf6TJrIYio7RrG9sdRVcVLoqvz0VTJiqzVSLI7H4xGExRXWMU+UAZrwi69QjZWX0BdLbl8FTtXhKmI/eJeh2rk7rF7926cP38eKpUK7du3R0xMDG677TZFjsq2bdvQpk0bREVFWYf169cPOTk5OHjwoMtlFxUVIScnR/FHtetKbsXrENTUrefKNc89IWXn29YtSZXdwpq7KRcVXLN+lo0V20+FcP6gcOVaDVUGtbtRZhdUUzFVeRls++lKFU+nqy7Ox7w823Uq1b4opRbZ56iYjKX2sV1fKVeLlLkm9ueOufR8VVENgcqVvIodMNmjlZmpsmokUDl58iQAYPr06XjppZewatUqhIaGolevXrhy5QoAID09XRGkALB+T09Pd7nsGTNmIDg42PoXFxdXE5tgc3wdkLrZ9XizyVJefyXV9TRHVwOntymHXT4B7Plf+bNSc9OBXZ9bKrpdPQWs+4/le8ZBYPcSSxqyzpRvWUIAe5dbukuviOzzwK7PcTajMp23lfM55uzfZb8G/kqqZZvNJly4arvJWLf/9DbLPi/t0A/AuV3Ol3khGTiwAijKA3YtBvIyy0zqpRxbwKayf5IuvX77439ghZvXCRQvoyjXUvmvdCd5h34AdsxXFiO4cfGq7YZoKCqj+bSpyLLOYq4CleMZpR4MMg9bzqWq5oLY1ds4eTG3assqS+nz/6ytguXRzLyKz2/XmuRkhvMg5EqWbb9l5TkJ9NP2Vvj8c6kwx7KM/d8CJzZYB6uELWfEbCqVq2MXqBy7qDy/hF1xidls31TZbLkGrfuP++ufC2ZDAbBnqeVaWBH7vgF2LrT8/rPsfv8HnNQPunYZ2DrH+lVdG62WnF1/ctMtx8SQbzku6fstw6vr9+PKtcuW33VJJe+sM5Z7hal0oFpoOZZe2uFjhfI5X3jhBbz11ltupzl8+DDk4h/utGnTMGSIpSvuRYsWITY2Ft988w0ef/zxSiYXmDp1KiZPnmz9npOTU3PBSv4V4H/FXYm/cgVQOSm13/EZsHoqoAsEXnRykHMuAF/eZ/k83e4i9uENlv+yCegwsuy0LOhrqeiWeRjYPg9On8R1AcCL58teVsovwMoxjmkqy2e9gGuZSCz/HFaivIHKgj6W/+N3AeGNnU/zQXsAAjDmK3/gn3QDpp4FFvW3fJ98GAiqa/mccRD4erjls7Nt/uwmy/+o1kDGAWD7p8CT2xyns6OCLchU2322rn/SIUv/MSXH/7HfbZX53O33n58B9i0H9i4DRhVf8LLP2dLvEwK0vddt2gAoihsMRfnwcTfthjeBrbOtX4tcBCrCfn8LAXzS1fJZ5w+0uKPsNLmysK9tsTVV8brE0dXK89/u9Q56VTkeHErPb3fzk108eBgL7YITZ0Wnn/a0/I9sBWQeLNf559KqicCB72zfp54H9AGQZPuiH+WNSjbkW59adSpb4GUUakWxodFYBGtNpz3/A3562vJ5y3sVu5YAkHcthvrP2ZYv5Z038zCw4jHL58AY5WXw20eAoHpA/S62YZveslT+r032v//gepbPC/tZHjC3f2Y5voBlm0t+P/oAoPmA6k/LsgeBs39ZKtQP+T/gkyRLJelrF4Eez9im2/SW5RgG1gWeOex6eR5SoRyVZ555BocPH3b717BhQ8TExABQ1knR6/Vo2LAhzpyxPPVGR0cjIyNDsfyS79HR0S7ToNfrERQUpPirMfmXbZ9dlCvLh1dZPhhcPAXmptlN7HgRM5/8o3xpyTptmf7IL3BZXGAox9MgALmyTfSuVf4pr8K3nmx3uUOWpZlStyqXbMhVdvdtlyMhrpy0m91NajKKiyczy85tss9Kt7aosH9SycsA8u3SUPIUZUc4S8u+5Zb/Z/+yTZd30fq5IOdi6TmcMtvlohgK3efCyPu+VqbLRQd9ivTa/SbkszvKlabyEDXcE6qcttflOL1Udv0ruXSOmN2NXLgKVOybXLuro1JyEyvH+eeSfZACWK8Lwi5AMpd6oi4qtOVM6CS7QAUaZY6K0W4Zp/6sfBoBiNStFZ5HzrXdMwxXz0OCsk6QKHVs8zOOVy5xlWX/+7C/9l89Zfmf6bxag3xud82kp+Qasv8by//ic8F0fINiMvOh4tZuuRdqJh1VVKFAJSIiAs2bN3f7p9Pp0KFDB+j1eqSkpFjnNRqNOHXqFOLj4wEASUlJ2L9/PzIzbTe/tWvXIigoSBHgeNJlu/JPk4vs9sxL7m8aaVl2Fygn5aOpGVkVStOV/KpnXR6/cLnsicqrnFmWrm58rpaVeumamwktjl42OKxftuvHIi3HdjE+cdlu3xtLZb2bK9cKQ7Ir7ikJWoRdfYcL2UVIt09Dun3g66zDLdfrSr981fo55Xz5jp/B7uZjKqNvElOecpkaV3VuFIGKbZ9W6ZwqtS9UNZw9fyjDTTFY6SxxJw5nKPelsKtr4SpQsQ8aa7vVT+614kDFbj+bShX9FObbtkljlztohAYw2o6zyW5bj2VWoIjOSWdr6bkVr6NyJuOK9fOJs+dLvX0cOJ6mLC49d6mW6wPZBaGKYmln7NKekp5VQwly7tRV5b0oK7d8D7meUiN1VIKCgjB27Fi8+uqrWLNmDVJSUvDEE08AAO6915Jl3bdvX7Rs2RIPP/ww9u7di9WrV+Oll17CuHHjoNeXvxllTbpWZDvpHCqfFRNldBCVa1cx0GRwvEBeK3QMXtwpNFU9WzznWjV2qFXOOjblSrXdBTyvqOzlZhskRbAAAPm5tguZwS67PbfQ7kkwP0uZtsLKXczsc1Sk4rQXFdjOh/xCA4qKbGnIzrO7cBWVTFe+41mQbxd0GMt3ztgHJ8Yi9/PohHK8Fq72v21/C7ubelXOqdL7XzJX7DdRUVlFrve5XI5A5Wqh3TlnNiqOh+SigqjZ7liI0oFKOdZZFYZrlnPNPlApXSnWPkdFtrvZGiVljopstI2z/02VyUlwlm+oeM5ZoV0FcVN+Fkr/frJzlcFBeY5ndRJ2525uYRnrtntgysmv2XTKQln0XjpGrO39VFE11uHbO++8g6FDh+Lhhx9Gp06dcPr0afz+++8IDQ0FAKjVaqxatQpqtRpJSUl46KGHMHz4cPz3v/+tqSRVnN1FR/3tSIivHoB4LRJiz/+sw33lUlFz1lnI344Czu8GMg8j4fdx1lGGkkDl+HrrsMS8Lc4rfrpQ7roeDjMKyL8+D7FzseJiavr8btfrzzwM+ZtHIZY/DPGN846S5BWjFcUtYvtnkH+bZlnfplmu020shLzyCWsHW2L3Esg/jLeObvP7CIiPOit7f5VlyKts5apC7QOUyvrVf/2g7YvZdoHVybY0Xsu5Yj8LivKynG6bg9PbIH/7mPWrfXPPxLNLYP5pEgrtWnc0/vEuxH9nq7ehKbLdkE3Lhzup/Oc6FyPmz5dt380G4OhqyCvHWire5l+B/N1o4OQmxWxmu0AtZs1ox8rWh1dBXvmk07fjaiTHJ2DAFpABKNcN2oHZBPnHCcDuJZZ1J38J+asHFZNIJgPkLXMg/17xTvxKzj958/sQ61+H+P1NyJvft01wchNuPDnb9r3Ued16z3TXuYSyGfJPk9Di4q+29RXmwFhkO886n/4MuHLScg1I22eZbe1/0PzYfNv2lcpBss+FU6zum0eUFd6Lz3+x63PIPzylOH/Env9BXjXZac6FoSAPOPs32qXaOmi0dqx5YAXkH8bDb7dtXN0DdtNBC2GXE6w9tdFyruUrf0MAIHYuhvzLs9btF1+PhLyxuMfYM471bZobbK1AXV6D0vdb9mVx0a0i4CvIcXhQEaYiyH9+BLF2OiAEVMJFMHV0jeXaV9LBX16mZT2nKlAcdfgnyzXM7vdj/nOu9XPDtY9B3vSOy56I5T1L7RJe6py7dMzx+JdWlGe5BqT8VmZSVaVySIWkrG+pES6qNvz+JuTNs8tcfk2rsQ7ftFotZs2ahVmzZrmcJj4+Hr/88ktNJaHq7H+gZ+1O4B/GAe0fAgD4iEJFg5asZaMRkr4NOPAtinSh0BtsWfamkgva/wYr1/PlfeWuTCYqGVsWHv0dPtvnWb4E2youalJ/x5W0owh7wbHstGDRIPgWuG6BBQCqgyuQE9oSQX0s3elLvz4LCUBh/e7w2fCaXbqVgUrO+lkI2vslsPdL4NUsSD+OdwjBpEspyF3yIAKfsfzQCw78BN+d/2ebQKNzyPrVZttaH5jss9vtLnD5OZdhX7MpL/uy+4qmJRb1V+x9ValcB/WuhSjwa4oQF7NrCmzZ0prTm4HTm4Eus8tcreHcHvjm2LWqMBuBL++DCsBV/4aQLh9HSMrXwP6vFeeR/UVdY8jFlW+eRtjo723LWT4MKgA5AQkoXdNL7aKeiFq2C3KL8mHtcaOcgcq1Pd/Af/diYPdiy77cu9ShYzmV8RpU6yw3OHP7h6EOrV+uZQO2808xDIBIGgdJowO+uEs50klPsoXHNsKnaW/HtB9ZB/9dC1HHftq8KzAaSl3kP2gPFYD841ugGfk9dFvfU1ZNLnVTKCrIdXr+qQ6uQP7Jv+D3vCVYtz//JQDY8wXQ2nItkX4YBwlAfv2b4FdqOYaCPODbOxXDrE/Q3z4CFaA4/kGXk62fzZIGWoPtnArZNgMAcNWoRunAWlo1wZKuHZ9ZfyfSoZWQu4yFqvR+L83FNdCw4HbojDnIO7sPAZP+hmyXKy0VZTvWUTEWQLVmmmXeprdDIzvPncv5dhyCDJkwntoK7auZuLLyOYSdWAkc+Lb8FXuXPwQVgKzQlgjp9RRgNkKzyRZcaw1Zll6sN7zudHbVr7ZXkIhS+zJ3yYMIzD6qOP6lXf1jHkL3fmXpFbmCFZkdAhU4BirmrPNQ/2FpPCO6joWkLddVskbwXT9umA1lNwNV3IBlGeZ020llH6QAgLGsJqLlUckMlcPHbZVJVXKpm0qB85O8rCDFuuyUI5YPdk9zx5JLN+lWJvzcAVvgJ+e6rqTrb3eD3n/spGKcpNE5XKjsmeyKXcxFtjLYglzlcbEvLqoIlZPOvS6nn3I5vbrgkpOhZb+UMOWsstK5ZJcVvz/1AtJPOlbSBQC5VE5J9gXnzUDPHnB8ilTD+ZOo2u7Cb8q3nTdS6XPKhX0pZTdFta9ndD6zAnVf3Lx0zr5PmbIcSnXeRPPwqTSHYXnZV2BycZ3wK0zHmQuO85QuBinMd10/wK/AVrlx/9FjzieyC9YPlvqNAICxwLF42mwyOs1Jc1i0pEZIvmPFdlfnnDO518q/70vTGS1pD8g+CkB5TmuMOQ45ESq73KnUI8nQCyfHxmxCkMFyzdEWF3leO1XB1xrY5YodPH4KAFB0qeJNtK1KbUdg8fbaH//S9p+0a+FZwRculg5UdE5yntIu237fhdc8219ZjeWo/BP4nyw7t0cRqGTst1Q+czXtxaPAyR8rlojDq4BzthYVOlGBssS8i8C2jwC1FuoiW72f9rkbFZNlS0EIO7sDuHQMaD+sYukD0OXit8CunpBbDbFGvqqryh9tKHIsfYBEtQLiu8HXYKuEXPTzc/B1sewMhCLm4PeAT7DDuI7pX7utmBizfy7QsQ8QEIG4U99ah4fsnQ9c/gOo0wTo8jg0pzY5X0DeRUsuReIDwInfHUZHCceK1FJehsOwEs2ztzgMC8qyZe0m5m8Dvh+nnCAnDapS3Yx3uPCl9bPsF67M2SnMAfYsARr0ROuTCxXzXVXXQYPdSyz9hthdGFtlb3RIV4jIAda8DLS9H0j9A2j3AOAbqijiMdgFKqqS4YU5ln4aWt0NqHWW1gaJQwG/MGDf14jKcd3ipoRUZLso+qX+BjRrb+l7IiDKUnxVp7GlCWr6AUurhg6PAiqVooltaYaCXPjknS1z3QDQ+PAnAI4A3Z4CAotbIO7/FjEZGx2m1W+bDWPjfi6XFXTse4dh7S+tAv78EOj8OCBJ0JY00XUn6yzapnzkdJRckGXd7sanvnIYH3L0O4dhukuHYPzlD1uOmAuRciZ0zio3C4FO2eV7J5P64LdlTwQAf80DmvUHQhOAY2udNj4QdjkqLfK2o1BS1mfseGml9bPxyhnoZcdgTJz4HQahsbXw2v4p4sx254axwPK3dxnQ5l7g9BZL/zaGPKD9w4A+AIbLJ625ZNGFJ4AfxsGYdRl6h7WVj9ZcAGyZDVw6CjQslZuXc8HyHiaNHohsCdRPAnYtRh3Z9uBjzjoLtX8dy2+/+R1AiJMuO+yK60oHKlr7B5N93wBt74X/SVt/VuaDPwDdR1dy66qOgYorRXmIPLS4zMkUT8Gf9oRJinZZ1SB43RSgqAJP70W5wHJl4OCD8lc0NH31IDTnLUFOWzfT6SUzsOBWy5ewBkB8t/KnscRPT8Mk6aw/Xp88J/25/DLF8v/VLPibsqyDfVO+d7lYjQTgmxGWL61fVYzTyu5zvIIu74Vp2UPQDPkMoblHrcND07cC6ZZcBFnjg9DDS53Ob1zxBLQn10L+61Oosk+7XZc1vfnlazpcolHKfOWA5P8pvhb8MQfqyD4u5w/QCEU5vXHtdGh3LQAAh5tQFC4DP45Huf35geUPgPH0X9AO/UJx8zDk2c5lH7OlBYhx9cvQ7lkM496vIWl9oTn3F0zH1kFz63RgxWg0LMdq1XZN/cP/mgm07Q98N0o50fRsYF53AIDQBUJKvB9FBddcBrzGgmu4tv5V568FKCUo6xCw7RCM55OhffRn4OJR4LtRqOds2tRfgFTXDzSRhz93PmLNSzDLApAk+B9e7j5Bhmso/HUafIxXHcfJMq5dzURg8dfQ3BSHScLPOQYU8cmui+Tt6ZwUCQBAC6Pr3sNLC9jwUvkm/O15yBvehOr5VEXfNvaESRl4+AjX10NN1kmnOSrSl/eiCH7Ql9ycf31OMb7oUirUa1+C5uR6mP9eAPUVWxNnU/YFaPq9hktnj6K4hyY0urQBuAQElGMTXWl9+VdgXXExZLLyemRafCc0dmkQ9y2BtGoiWtlNc+XMAYSeWQfN7oUw//kx1JMPoDTTL89bb/ii1KsiFHVYVjwGRLVE6FZbkVXA2ilA0ihA5ZlCGBb9uFBQVgVLswmQZfhKyhwOk+T6GUVbkSAFQH6O4/QVyVEpCVLKooVtmaePlv3E60raAVufMKWLvRTyMsvdWsgf9r2/VryVgObcX8i86Lpo6Uzyeoc+JUpoT661rLecQQoA+BRWLFApy4nUk5DMrgMyyVigKILK2/+ry2nrmcrZc7Gz9RT3FmzfIqfQ7jcSbSx+Ik22PNFrM/ZCc87Sh4Pm5HqcP+V4A3VFbVQ2e00/6KRnaLscoZMHLBU1DQWui1AMhdeQm3bU5XhntGcsOWCZJ/dVaL7yOr93PS4kl50rUZh9EZmnnddTEIZcXMuqQi+2XkZVlA1zkZuiogq8CiIg5yR8XTzYBUmuW6llXrwIzUlLgwf7IAUALu63DM+66ub6VgnOcn5KaEql4dQBx4rJeecOoWC/JbdenVP8WyxVnGQ4bLs2qOxL4guyHJZ3af9ah2GVbR1ZHZij4kJ+XpbLpzMAMJ3bBY3OcYoEuXzZyw4OrLBUjDu309JrYLPbcO1ankPFOH+UUa6ccwHIOmvpfbac7IOfy1euIP6vuY59jZTDlewcxBd/DpSzXE5n+O0lRV8N7vjBdmFSGSvX1t98YqPLcQnnV1Vqma4EGKunj5pzIhyx0iUEXzuFwL2fuJ7QlK8o+rlg9EdotaRAyShU0Bz5GVEFtotm2KEvrJ9D5CwgfT/Msgytk3pU6XlmpzkSzjS/9rfiu/+e+Y4TrX3F+jHi1Crg0jhodimLun41d0JL6TTiVZkI2PE+Ao2V6B589xJcO+X4dFodruYVIrHgrzKnMyQvR/1C50GW/PNzCDnnvFXJ9Ur+/XXnb+5eNx3tz37hbIxTsQWV62E1cI/rnmxj8g4CpiIYiyp+fawuDQ45Xg/id81UtnDKuwj4KKvI+5ltDwAds9dYqhUU5cJ0+i+HQCBgxxyUVpCVAT+/mri6lE0STrvGvH7k5OQgODgY2dnZ1dpLbfqhPxH99W3VtrxyeWiFrUXQU7tx/uJl1Ft2a8WWERBl6RG1kgyaQOhMlXvXyoHgm9A620V9j2qwN34kEk8vrrHlVwcZElTV8KLBHWiFzig7e313zFDUyfgT8bIlt2S3ph1uMCVXef2VITe6BebjG6GVHIPQ3T3m44bNtVvGvTHgDrTK24oIVO/TL/3L9ZqK5CwftEt+texpPaVeR8j3fwnVe02rbZEX7/sRES1vqrblAeW/f7PoxwVDqU7ByrLZ3LrK6zTtXGz9bE7bD2Nh2bXlL4pSlUyrEKQAcBukrDJ3dTtvUFHV1l0WdUHlWufYW2duXw0pca06ghQAuKwrX/6DqlSOSojkmOuUJ2zNCo/IVXsvVrZwXcvj2tkDToMUQNmsuTYsMvXD+ohhCEb1v+DwDeODNX4eUe07L+qUPRGAov0/QC6jp2d3LooafO1LifM7Ycg4Uq2LLMyq2eu7OwxUXDDmV6w87mHji1hm6lWldWqO2FoEFa17A4GHv3YztcXbpvurtM7yOiHHYIO5ndtp6hc6/jCKRFntCsqvdWYFW0yVslNuiguIKntCL3DNL7Zc07W7+CPiZFvF5YZGx3ebTDGOtX5+zDgFH5gGVTpdR0U97JCbOR0XaHB9IVPVcG+z9o7J9fAf0wg0bNwCulLNrEv30FkZ35u7Y7ppRJWXU91GGJ73dBLKrUjUXq0DczmO+RTj49jp4rwGgAJha3mnv3wIbVI+qHR60kS4y3FPGypQ2b0sm96uvmUBMJTzHWM1gYGKC+aCij2J7Zh2C05qXbzttxL8so6iTsqXZU7XrW2LalunO36SAf169azwfHqp/O9tMQqnJdPVR+ODuMiKl7GekGMqvUqTqNxPzBDcoNou5l062XLCnh7cGyFNkiq9rEJtKCS18zcruyOq8ATqzHG5rstxdVVXsGFKLzzSvYHDuNMissrr/njMbZjx8M1VXk51yhF+uK1LmxpbfnUHFhdddotYvdaaO+C0KPvh5IZmjRAcHOJy/BGh7HRQa658n1iX1c7PwXyhR2Jr2zHMEK7TUx4+5yyVwS+LwDKmLB+zm/6uahoDFRfMZbzDp7TIQB/ktbgf043DMct4bw2lypExoRemG4c7DM8S/m5vsN+ae+J5Y/nrDPipDMgKa4snDU9jnukOh/HJctkNT8+5eZIAAJPzKnRuHZAT0L9oZvkm1vpCrbMVg8wz3YG+RW/heeNorHeRlf+QYSruN7yC941DMMd0N/4wV+xmcL/hZfxi7lyheQDAPygMjxqfxfvGIRWe195rxmGIbtwOYwyTMLhoOpIaRSAzqhfyReV6fCjQhWFlsPJ8K88Ftays8sNyfcw2DXY7TYl3jPfhNsNMHJad91jrjwI0CHdeRGW2O8e+N7tuhu8s5+Ut41AMKvovEsL9kRAd4TD+JzdFo1+bbsJc051Ox8lCwiE53um48rrD8Abi48ruwfdl48hyL7PQLjc0z65pgavg+4IIK/eyL8OxXyR3/jTbXlT7lnEobi0qO7dgmvFRPGscA43all5X17z6sbEIhvPK+t+bu+FzMcDpuP8z3YbnjaPxrHFMmekpkat3HjhdQSAS4m3H8G83OTzOZIvSTS8sPi5nDuoqc1fMNA51Ou5T0wCcDbqhQumpTgxUXCjrZYPO1IsIw2JzfywzV+1pqyLZ074+PvjC3Ndh+DPGsXjN9LDL+fbLDbDc3LvcT/x6UYRgXy1+kbtipulBh/GPGp5zMpfSZybHH/vPdjdxcyUClYWm/jgqyldMYlb7QFbbbtAzTQ/iqIjDcrNjd+kA8JRhPLbIbXAJwZhjHoL3Tfdiu+w6BytZbuQw7IIIx1eVOB/qBOiwVW6DOeaKByqf2u3n1XInhPjpsEbuhN2iKWKCfRAV7IPvzD3KXI6zAMSgC0WqrzJYu1SqntRV4dijRLsDb7pd1//MfTDXVEY368Xmmu+CERpskt31DuScXmWrQ3NaRCnqmuyUbRUPL8PxKXSp+RYki8aICNQj2M+xSPO8CFcUEdj7Xu7u8sYzxfg4vjFXLLfypByN78w3Wr+fEVHwCy0752CJufyV8+3PdftA5ZqLF04clBPKvewcdfmDGgBYKdu29VPzHTgmYnFSjnY7z1JzH+iDwiFJtuupq9+6PrAO6pid9RwNzDLdj4AA54HvNfhgubk31pvLfxMv8nOe7isiGMF1bA+XKaom5V4mAHxiGugw7A9zG8d6jC7slRviS/MtTsfNMA3DCd+q18OsLAYqLoii8hf9lAQWd7evh1uaRyIhrnw3TleWmXuVe9qeTSPQu7njiX9ITkAhXGfRa0Lq4c7Euigqo2/Ko7KlUuevUk/0aRGFeztYts3+og4AVxBU5lP6bln5wzsgJ2Bz8c0mT/jAXInTMQ11MPuBDuWaVqi0yAhuZ/0uScCMwW1wR9sYHA9wXMYe0RhP39wYK5/shshAPeLr+OGovqXDdIAlazzHyRPN1Ls741I5K8bmCNvNQB/RCBNuaYKO8RUvqvrJbCvaads4AV0bhmFktwS8fEdLaNQq3NcpDn/KrdwswWKzk0Agzy8Wt7RU3hDzVbabepHQQq7Eex4M0KBBdPkqMybWD8OfL9yMvaUCw5Jcie1yc+uwTWblNqzW2G5U9fyBjXI7AJZs943mRACWYiVnQXPT+vXwn7taQZIkBOgci0IyIrpjuYvfrn9gMFpEOn/ivYxgXHQSFOYK1x0kmKHG5uJtKxJaLBjREUGBZWfx924WqcidcGW33BgnhK14LU+yBZ9/uAgQ09XuAwd717QVC1T22eXYtqsfhpmD28Dg5tq1Q26G21pHY9HIztDYNdvt3yoa25xsv29wBA4EOM9hK9KFol27jk7HNY+NxB1tY9Cvc9m/pxI6P+fHKVsdgoBg235JD69Yx5t7ZMeqBydEXXRrXr77UXpAKzSLt12rDhdXvP/J3BXhATrU8a9sv7tVx35UXDgZ0h0bTJcxXvNDmdP2NMzGFgB1Q3yxYGQnnL58DYNnTYdOMkGCgC+KEIxrKIAehdAhBHkwQoOPdZYKWdeEHrcZZqKr6hAuiWBsk1vioGiAN7S2fiF+NXfCbWpL/xJvG+/DaRGNgyIeG321+Gx4R6D4pdNZwh8TjeMwcUgv1M8/DGxQpvU/xoeRCz806zgQL/dujuxXNUCpTpFWmbtioak/dJIJh+X6uFm1B1t1SbhbJeGdexPxzr2JuHHqRfRT/Q09jPhR7obfJvbAPXNeRWfVEZwWUVike8e6vAcNLyJP+OLO/rdj1OqrMEOFhlI6Nsjt0KZNOzx9wAc75aZY5TNNkY4HDNPQQTqKKdpvrMPeMd6HDITCILRQQcadd92HAW1igOKes8+JcLxoHIU84YvuqgPYLrfA13rLyxH9RT4ywjpijGESToi6SJ1hyXl4oHN9PLdcxrP7VAhDLo6LuiiAHo/f1RsPJyUAAHZMs/QO++H6WGzftAxdVLaKw3nCB7cWvYNPAhegpCPP+4pehlqSMbNxHKYNC8fwReeQKJ3ALerdaKdyfB/L04bx2C43R7iUjTpSDsYExmPSDeGYdGtTYLrjOTfB8CT8pCLM0Fp6oV1j7oD9cgMcEA1Qt0USBh+eDr1kxGt3doIkSZh+l+1CqteoEdbxHjy5U8AHBryns7ys8mPTXfhbboYrIggBUgFStU0hmWUMUVvKuv80t8Tx6NswpWs8bvxlNrboJwIAijSBKKmzWgQtdHYtf9413gMjNHhBu0yR/hNyDN433YOPdB8CAAZ2bIipfbsC7ygmw41Fc9BP9TcKocNlEYTTIgrfjuoCf70Grfs8hPHrZOTBB3qYsEFuhwGqv/CH3Ba7iud/yjget5j34G/RDB2lo9jncxMeM1nS0rSOFr9FD8VTx/yxU26Gtk0bYeLxCGyW22CDzxRrGmYZ78UWuQ3mDeuA6GBLboJKJaFn0ftoI6UiRcSikZSGyPieeDezHtKNYQ7b++StiZAuHQGcdJsypHcXZGjq4dl1RbgsgnBJBKOd6jjy4YNZWud9eqjUGqhb3oun9qmwRzTBlhZRuHrNgAFFb6CJdB6zdZa+NuaZ7sAZEYXLIgjnRDh+GN4RPaY9gdvkHfBFEaKlK3hYs85h+aMMUzA2dCdKum1KU9VFK9ly3v6q7oWfDV0xTzdbMY+/nz/uuPo6eqksnUba/25LK9CHWy87TxgmIE7KhB5GCEhO5xs+6HaM/OEyMkSY9fgf/NVH0Qt4rvDFs8bH4QMDNsltsechy8NHgbA1jJj70A3oNvUJ9Jd3oLvqAPqoLX3QBIaG4zufu7HGoMNO0RSdpSPIgT+yhD+WTrwZ568WYNKmJ/C+bq4iXXpfP3z04A04kp6D2/6egV/1U63jbiyajcXat9FYZXlfz2TDWBwW8ZjsZ7venpBj0EhleR9UriYMTfz0GFj0XwRIBWjeoD3mZAzGBI3yxZk9i97HANV2aGBCqojBYVEfjaUL6NFnIJ5Yn4O5OlsfKIbAODSKCAVO2vbRC8bRuAYfNJIuwL+4n6oUEYcON9+GaYn1MHCmZf375Ea4RbUba+UOOPhaBbvJqGYMVFw46ZeI902+5QpUzglleXWwrxa7RVOXXemX+BiWQGWJuS/OiCicMVueVCMD9Via28caqMhCwhLzrdZAZYm5L3LtuoJT23Uz+KFpEDbK7TAmzA9qs/KJzCDUWGS29A3zmo/lghssOTaB/tGcpEj/SrkHdCZlbsc5EYEF5tut3+NC/XBIJOCQOcFheX/KlizDe/UarJctF4+NxeOayxJ+lS1PDnKpHJVtcitsQyvFhetj8yDFNL30WsX2bzQn4g/Z8mS826zM9QmUs6FVq7BG7uSQxvO5Jmwt9TT8SLDjE61GrcIGcztFoPKDuTvSUAf+ku0CtEO0AASg01i26Q85EX8gETnwtwYqq8xdcYfacuf6sXgfZIgwQABllXj/IN8IQFgDlSMiDh+aLXU8psQGY9Yhy/Hz0TrPpVKrVfhF7ooY2Dqo2yc3wka5uChEAPV8ffF/2QOsgcp7pntwo38AfLRqnLOrlGrS+lsDFSPU8LXrdr0kTaVv3Fvk1opcHbVajSAfx8tR6fMMAPx0ltyOi3lGrJKVFYNXysoirRwEWIedE5GIlWy5jCpzIaDX4KfifR9+TWB/cRGD/asqPjLfDcDyu7Z3RkThTHFFzeMiFk/5aSGr9Vhm6O2wvTr/IBgzbU/2X5huxXCNpfdPTWgcNEVafGN3/u0zN0JHyXXzUrOkgVCprGkHgCBfLQ6KBtY0AcDv5vaWc7GYRq1CGupgYfF1YLLGecvCqwiCFFTPGqhkaOtZA4sMhGOXk8rMgf5+OHClIQ6YLbkfYzSrECQ5r3Sq1tmKUjbLbWDftaWzQMVXq7aemyXHv1BSBiqb5Lb4TXasD+Zr12mkJElIQx0sMt+GBMn20tVgf1+k5QmsLz7+9ud3oI8GRrOMlfKNeB/KQMWn+FyPC/XDYRGPa0JvvQ6cE5H4P/PtmKmyvPH6N7kz8uEDva+tld4ic3+8rloEADBr/RHsq8Ve0RgQQL86fvjB3EYRqJyQY3BGRGGuWVlMekLUQwetCjvschMBAEF1IWlt17EvzTfjZ9lSl2oj2ikm7QhJsX4A+N6uyM2TWPTjQr7R9cvuSosMVGaJBfpUrEmuulQvrfd0UGbVqSQBrd00+cWvvtKpHQ9fyUsRm0YFQlPqtdz22dmxYc6zoQEoslRvamoJwh67UdmColczW3AWE+wDf70GdYPdvwY8qaEya79zQhjuaGu54DWK8K9UHZW4MGUwcclNJT1/czZ6NLFU6NWqlcUTAxMdi2cS4xyX1aNJuEOvumaoUD/Mz/p0Yi/YV4smUbZsc5Xd255LF3U1ibRN1zjSsZ6HI+dFLLe3sZVzh/o5L/7r29KSTZ9p1/qidK/HfVtFKYoG00QdayXVqCDbOa+yawUkQw2Nizcv26sj5eCKXT0Qf1O2otKjOyV1Dno3V7aeuL2NZZv6t7IVQQxurzyu9uexZCzEXe0s51/DcH/c29Hyu+veuI5Ds2bAMegLD1Du21tbRmF0z4ZOiyR8/AIhzLYAzv4ZJig4DPVCHIPikDqui1LMUFtyEgHUL/4tlwTsRrvfUb6T1+T1tSu6C4fzuni3toyCb6ht/Xk+ts+JrZ0Xc+j1yt+/qyAFAII0tv3r2P+2o47xtiKRkuOfLyn3mX3xmf05UNqtxdtvf4wDfbS4v5PzPoZC/XRoERMEZ7+3ALMlt8Zfb7nuilLTXLbrM8VQfG1W623bm2dfvKf1V5xjNzWNcHh4U7l5W3y3RuEO9Yd0deKhtutBPV+4vkYnNaqjWP8txb+vkd0SXM5TW5ij4sKgdvXQKCIAF38KRoSUjYNyPFqpnL/zZfVEZUU4tUrCxim9YDRbTiqzELh6zQg/nRq+OjUu5xmg16qA4pKdIr8YJE+8FXvPZSPMT4fmMYHoEB+KtGVhiJGu4LBcH7c0DQVOWaZ/c0g7NIkKtF6g7OnD4vDzAzciPECPC6Uu/Hvkxvhk2A0I8tGiWyNL0JADfwThmrXLdgDo174xXu3dCyazjLgwP+w5k4WOCcq6ErPvb4c/T1yGRiWhcwPLReSbJ7ph/7ksNAgPwIlPLFmap+VIrJ7YE346NeLC/PDT+Buh16pwPDMPnRuEoY6/Dl8/noRmUYHY+05DRAlbp26/TuiB05fzcfWbAIRKedghN8PcYTcgxE8HrVqCSRa4ob4yXUVhzfHL0B7w1amRkp5rCRI+tow7p45D+3rB+H5cd9QNUf5g7+kQi9hQX+QVmRAX5gdJsrTkKq11vWDsqhsP2HUpUBAQi5+euhGHPkxAPYPlrdG/P3MTTLKAn04DP50Gv07ogTNX8nFq02GguJVfpi4OJTHP148noXlMIHIKjMgpMFmLGABLPxBqSZk99/XjSRBCAMU9isfENcG87jegXogfGkYEYEPx+VdyAS3txibh+O6JJOQbzEDxO9Du6twEj3a6EcG+WuQbzEgI90N91SWguEf7Xp0SrYHlrxN6WotpVBrbOs5LkTgnotBeSkG28MO8hzqgyGQGvleuXwqqh6/v6QYUv7NP6C0X9L/kFuiqsnR9niX8sW96X/ydegVatQrBvlpE2D0U9GwSjm/HJiHIV4trRSa0jQ3BQ10vo11ciHWaNwe3wT0dY9EqJhiH0nIs52rxa0wuaSJxZ9sYxAT7oGlUIAL0GjSJDETb2GAcnNEYrXAc50Q45j3UAS1iAhWVMgFgzaSbkHopD3Fhfjh/tQBtY0PQMiYIzSJ8rUWRJfT+QSjU2tKVprEFUPXD/BAb6oslozojxFeHED8tDqXlINh8BVDm+lvJkho3N4/EN2OTFAHulud749zlXKD43ZaP3NoRoXUbIiLAB6H+lgDqnXsTcffxS1CpJPhv2wCcc3wz+AdD2+O3LQagOFNHU6chUFyC0qddE9zRWWO9fpW45lsXm5/rjYMXciBJwNHl9dBU5eTlpABkf1uQ+d0TSTh3tQB6jRqSBOxd3hCJdsWjh+R4tKzjh1VP3Ygwf1tweEbXGCiwvc/son9T/PBQd+QbzIpz4KoIRKiUi51yU3QEMOveRPx5/BKMGxsCly1l42qVhIe6xqNpVCBaxgTh4IVsBPtp4aNVw0druXZ9MzYJWKzcjiIfW0vGP1+4GeJ92zmy99W+mLk4FyjuXujLMTciNswPp3bb3u/UokE9oHgX5QY0gCRJ+OPZ3sg3mhBfxx+P3XoDYHuFGtSQkfzKrfj71FXIQiA8QIe4MD9cyCpE63rBWP5kbxQtsL0ZOjCqIVRaW655YEgYVtzfDQF6DY5n5kGCpa5edLAvWtezPJiVrL9BuD92n85Ch0rUk6tuDFRcaBEThBYxQbjlu5cwSv0LPjLdjQgpCzO18xEuZSNCsj2JhPo7PrUmuGgeCQAlfY6NMDyP3qo9yOswHCF+OmvuBQDc0iIKvQ3TMFq9CnPNd+HD5gnWQOXm5lGKCzYAjDM8jTaqk4jteg9a1bWccCq/EOv45aZe+BD3YYvd0zYATDKMxT3qP/CFuS+icBVNVOfQNrG3onlnUiPHSo4hfjrFkzsA1AvxtT4Z9jA+hyfUP2K++Q5siLY9ObeJtaStaZRtWEmgM1PzJHoaHwUAXBEB1mPQf+lLGK5ejTmmIdheap0lxhueQmtVKgxNBqBlXctNr2Qb+hfNxHD1avymH4EvAMVFrIRKJaFbY/fNp0vkNrsHC9MOQAMzZEi40PRhBPtqMdf3MaTmSPjSfDNWRShzREq2ZfHVQZj/2y7slJsioNVt+OLwFayWO2Jp8T4I8tGi9Mt6BhhmYLh6DT4w3Y2RmjU4ICfgo+LpRxmewY2qA8iKvxuTWtv2javmufY6xIchp9CIxw0T0UmVgnoN+1svViVu6twBs/68F5cRhOHdGlmf2u1vGGm+TTE+8yncrt6O5bpBOF0UhDH4DgvNt2Fda8uT7YDlb2CYej02yW3RQ7Ufx+qPwtTYYGv6gyL7IhGWujrvaD9FoJSP54xjsM5Hi1taOG/NIkkSOiYoK2V2a6Q8hj5atXVYyXl8f9HLuEO9DYdChuEmSUInu2WUTPO08Sk8Jn2P+eYB+L2186fzMH8dwvwt85YEtRq1Cj2ax+AV4wj4Fr/sswA6TPL1w4XwHphnuhMH5ASEdbgbn+85jw1yO8wP8YFKJaFHE9vvPy7MD0cz/DHTOBT1pQwESfnYJCfiHe1nAIBDmhZoVSrtABAb6ocwfx1eNo6EH4pwW5MWDud7sK8WtxX/jt45cT8unj6ENlIq3jHdh56q/ZbiAZ0awXUb4U3jAyiCDgkNe+Ljo3fhuFwPz9TxR2yoH0YYnsdrmoXwkYz42dwF16L647YwP8QVP0D1NT+DkfJPaKy6gM4q5Usp90XcgTMpu7FRbof5sSHoYJdj0s0wEeM1K7FRbofeqmTMNd+FPwCHc3Nt2FDIpy1RfxG0uNhyMBKd/LYfMEyzXT/stv+VlKHIyjhj+f3BEqyUHH9n14JOCWG4p+gVDFZvwWa5DZJUh5AdNhgl1fDrhvhiLdrgVuzARRGMCF8tOnW5Ee98dx+yEIBX64dCp1HhcGRHLDb1hQ4mRCf2w+OpF9FJlYL06P4AgPp1bA+gMY3b4J319+FZraWI7pCuDfr76ay5QiVKzr8m0YGYbhqBW1S7sUtuhqSYOEgFto4Y1c36Wx/u7K/B9uzX7+za7wkMVMpwQtTDi6bR8NepccEQjtsMbwGwZMGNUa/CX3LL0g+L5bZJTsQmORFPap1nx6WKGLxoGg1frRq5ddrgJeMjOCMi8ZGTegc/y13xs9wV79g9QasCIjHJ8AQKocOvchen2cvr5Q5YL3eAJBW/bFMGvqhEq43SzooovGiq2LtdZP8IJKZ/huHqNVgp34gtxcOPiPp40TRacXMsbZWchFVyEv4b7lhkUjJ/G3XF+m5wRaPV4b8mW18ij/taftgGfR28aHrM/bwaLaabHgIADPPzxyumR8pcnyX9luXOND0AAPioeFzJ8XtWX7ka+UE+WqyWO2O13BnvOumbT5Ikax2NaaVy8AYV/RdJqkO4HDoAq1LTsEpOQsMQf+SrTHgxV3nsD4oG1m1YLXfGYFjqupSk/78myzknAqMxIveFSm1LeW0XLbDd1AL3al0Hc9m+9TA1r3LvJtKpVfjC3E8x7EWdGhqNynr8xgcE4NXiY691UeQVoNdgXqm6CMlyY/RT/Y2/wobCVW9NWrUKS4q7LLjXRY5aiWvwwyTjOOv31XZ1PKKDfPGZ2dL3y8dBvhhnsvSxMau47tYmORE9DbaKm6/4KM/B8+p6eNEwGvVwEQPVf+J/5j4YrN6MoyIWnTV6TDeNdLr9eT7ReLHQsu/XyJ0Q4GIbokJD8fKJR63fn41w3jV9ye8/pFSTcpWufL8/eztFc+w0WeqB/Cp3wROlWla+irHYZ6xvvX7FhvphcnG9uteLi5v1etu2f+wTYP39TQt2PB91ahU+Ng/CKrkr7lL9iY3+g9HfTfp8NGosk2/BV8XNjIfW8UN6dgwmG8biCgKRFOzYCeL1gHVUyrD4kU5oHBmAJY91wZyh7azDZagwz3wXkkXle6OddnsLtIwJwugezjtLW/pYFzSODMDnj3aGSRb4n/lW/CEnwkfjWJfjyV6NcEP9ENyZaKvkptOosFLugV/lLgjx0+LThx2b4D7brxnCA3T4712t8Ej3BHRKCEXXhlWPor9+PAmNIwOw9LEu5Z5nztD2yEYAPjQPVlRmmz+8IxpF+GPxI46VYEs8Ubz993ZwLGf+9OEOaBwZgDfvrp6eO4d1qY829YIR6qdFy5ggjO1paSY7/a5WaBwZgA8fcP0emME31ENiXAievqUJJvZpiubRgZh+p/vmovOHd0STyAB890Q3dIgPxRO9bM1yn+3XDK3rBeHhpMp3GPb0zY2RGBuM/k5yDuLD/NCzaQTu7RDrcMNIFo0x13wXVGoNnujVCOEBejx1c2Pr/l5kd7w+erA9mkQG4NHuDdAsKhAT+liaqk/p2xSt6wVhYDtLUcjCEbZ53B3vqnjj7tZoHh1oaVHlwscP3oBGEf5YMqrinfXpNCoMalcXtzSPRJ8WkRjUri58tGr0axVtPX6PdE9Ai5ggPN+/ucvlxAT7oE+LSPjr1IgM1OPhrvE4JmLxkfluGNWumy5r1SoMbl8PfVpEoVGE+5y1MT0bwr+4curoHg3QNCoAnwyz9AnSPDoQfVpEYWC7uujTMhKdEkIxsluCNVftpQG2SroNI/wxtLPyt1fyuz2PCHxiHogc+GOxuT/+lFtjeFICWsYE4YXbHLf/80c7o3FkAB67sQEaRfhj/nDnTYOf6Wf5/QT5aJAYG4wRLupSzB/eEY0jA/DFo8pj+WTvRmgRE4RXy/j92XtrSBs0iQzAyG4JaB4diEdKrTOn1PWrQ3wobir+/ZQUHSbGBaNV3SA0jQpApwah1t/fA10cO+xrGROEm5pG4LSIxofmwYpGFM6oVBLu6xCHIB8Nbm4eibhQP+g0KqyQe2Kj3B7hAZ5rYlwVfHtyJU1anoyVeyyFi6dmOu+1sDqtP5yBUZ/vBACkzrjdobzcmZMX83Dzu5a3Ge99pa/TTqq8zS3vbsSJi5Yy1drYr1R5CS/8DAB4sEv9agsCyb2SfX5D/RCseLK7h1NTPiVptvdP/W23eXU1coss9UOqcxtL9mF8HT9setZ5p3WuHLyQjQEfWPKnvx2b5FBc6kl8e3INc9Xss6aYZFs8WZ4gpfR0Jc1kvV2+wfnbd8l7qct5PlL1UXGfe6caPixyJfIVJLtE2dc/uZ5cH3cvL1S6EltNiwut+Alm3+/D9RKolDR7TrhOf1D/Ru3rh3g6Cf8aJb/jG5uUr+K3Nyg5P0p3JfBPVNLsv2E5KrNXREnXD72bVfylmoF2/RNFsOjHMzxV9CPLAst3nsUN9UPRLLrsrqurw497L6BeiG+FmoutP5wBnUalaFHgzfKKTFix+xz6tYpGVJD7flnIs45m5GLX6au4v2McVCo+4deGM5fzsenYRdzXMRZ6J3XVvFFmTiF+PZCOIR1ise5QBhqE+zttnfNPUFPXr7TsAqw7lIEhHWLh5+T1DWX5eV8aIgL11haW3qK8928GKkRERFTrWEeFiIiIrnsMVIiIiMhrMVAhIiIir8VAhYiIiLwWAxUiIiLyWgxUiIiIyGsxUCEiIiKvxUCFiIiIvBYDFSIiIvJaDFSIiIjIazFQISIiIq/FQIWIiIi8FgMVIiIi8loVf1+0lyl5+XNOTo6HU0JERETlVXLfLrmPu3LdByq5ubkAgLi4OA+nhIiIiCoqNzcXwcHBLsdLoqxQxsvJsowLFy4gMDAQkiRV67JzcnIQFxeHs2fPIigoqFqXTWXj/vc8HgPP4zHwPB6DmiGEQG5uLurWrQuVynVNlOs+R0WlUiE2NrZG1xEUFMST04O4/z2Px8DzeAw8j8eg+rnLSSnByrRERETktRioEBERkddioOKGXq/Hq6++Cr1e7+mk/Ctx/3sej4Hn8Rh4Ho+BZ133lWmJiIjon4s5KkREROS1GKgQERGR12KgQkRERF6LgQoRERF5LQYqLnz88cdISEiAj48PunTpgh07dng6Sf8IM2bMQKdOnRAYGIjIyEgMGjQIKSkpimkKCwsxbtw41KlTBwEBARgyZAgyMjIU05w5cwYDBgyAn58fIiMj8eyzz8JkMtXmpvxjzJw5E5IkYeLEidZhPAY17/z583jooYdQp04d+Pr6ok2bNti5c6d1vBACr7zyCmJiYuDr64s+ffrg2LFjimVcuXIFw4YNQ1BQEEJCQjBq1Cjk5eXV9qZcl8xmM15++WU0aNAAvr6+aNSoEV577TXFe2d4DLyEIAfLli0TOp1OLFy4UBw8eFCMHj1ahISEiIyMDE8n7brXr18/sWjRInHgwAGRnJwsbr/9dlG/fn2Rl5dnnWbs2LEiLi5OrF+/XuzcuVN07dpVdOvWzTreZDKJ1q1biz59+og9e/aIX375RYSHh4upU6d6YpOuazt27BAJCQmibdu2YsKECdbhPAY168qVKyI+Pl6MHDlSbN++XZw8eVKsXr1aHD9+3DrNzJkzRXBwsPj+++/F3r17xV133SUaNGggCgoKrNP0799fJCYmir/++kts3rxZNG7cWDzwwAOe2KTrzhtvvCHq1KkjVq1aJVJTU8U333wjAgICxJw5c6zT8Bh4BwYqTnTu3FmMGzfO+t1sNou6deuKGTNmeDBV/0yZmZkCgNi0aZMQQoisrCyh1WrFN998Y53m8OHDAoDYtm2bEEKIX375RahUKpGenm6dZu7cuSIoKEgUFRXV7gZcx3Jzc0WTJk3E2rVrxU033WQNVHgMat7zzz8vbrzxRpfjZVkW0dHR4p133rEOy8rKEnq9Xnz11VdCCCEOHTokAIi///7bOs2vv/4qJEkS58+fr7nE/0MMGDBAPProo4phgwcPFsOGDRNC8Bh4Exb9lGIwGLBr1y706dPHOkylUqFPnz7Ytm2bB1P2z5SdnQ0ACAsLAwDs2rULRqNRsf+bN2+O+vXrW/f/tm3b0KZNG0RFRVmn6devH3JycnDw4MFaTP31bdy4cRgwYIBiXwM8BrXhxx9/RMeOHXHvvfciMjIS7du3x/z5863jU1NTkZ6erjgGwcHB6NKli+IYhISEoGPHjtZp+vTpA5VKhe3bt9fexlynunXrhvXr1+Po0aMAgL1792LLli247bbbAPAYeJPr/qWE1e3SpUswm82KCzAAREVF4ciRIx5K1T+TLMuYOHEiunfvjtatWwMA0tPTodPpEBISopg2KioK6enp1mmcHZ+ScVS2ZcuWYffu3fj7778dxvEY1LyTJ09i7ty5mDx5Ml588UX8/fffePrpp6HT6TBixAjrPnS2j+2PQWRkpGK8RqNBWFgYj0E5vPDCC8jJyUHz5s2hVqthNpvxxhtvYNiwYQDAY+BFGKiQx4wbNw4HDhzAli1bPJ2Uf5WzZ89iwoQJWLt2LXx8fDydnH8lWZbRsWNHvPnmmwCA9u3b48CBA5g3bx5GjBjh4dT9O3z99ddYunQpvvzyS7Rq1QrJycmYOHEi6taty2PgZVj0U0p4eDjUarVDC4eMjAxER0d7KFX/POPHj8eqVauwYcMGxMbGWodHR0fDYDAgKytLMb39/o+OjnZ6fErGkXu7du1CZmYmbrjhBmg0Gmg0GmzatAkffPABNBoNoqKieAxqWExMDFq2bKkY1qJFC5w5cwaAbR+6uw5FR0cjMzNTMd5kMuHKlSs8BuXw7LPP4oUXXsDQoUPRpk0bPPzww5g0aRJmzJgBgMfAmzBQKUWn06FDhw5Yv369dZgsy1i/fj2SkpI8mLJ/BiEExo8fj5UrV+L3339HgwYNFOM7dOgArVar2P8pKSk4c+aMdf8nJSVh//79igvE2rVrERQU5HDxJ0e33HIL9u/fj+TkZOtfx44dMWzYMOtnHoOa1b17d4dm+UePHkV8fDwAoEGDBoiOjlYcg5ycHGzfvl1xDLKysrBr1y7rNL///jtkWUaXLl1qYSuub/n5+VCplLdAtVoNWZYB8Bh4FU/X5vVGy5YtE3q9XixevFgcOnRIjBkzRoSEhChaOFDlPPHEEyI4OFhs3LhRpKWlWf/y8/Ot04wdO1bUr19f/P7772Lnzp0iKSlJJCUlWceXNI3t27evSE5OFr/99puIiIhg09gqsG/1IwSPQU3bsWOH0Gg04o033hDHjh0TS5cuFX5+fuJ///ufdZqZM2eKkJAQ8cMPP4h9+/aJgQMHOm0a2759e7F9+3axZcsW0aRJEzaNLacRI0aIevXqWZsnr1ixQoSHh4vnnnvOOg2PgXdgoOLChx9+KOrXry90Op3o3Lmz+OuvvzydpH8EAE7/Fi1aZJ2moKBAPPnkkyI0NFT4+fmJu+++W6SlpSmWc+rUKXHbbbcJX19fER4eLp555hlhNBpreWv+OUoHKjwGNe+nn34SrVu3Fnq9XjRv3lx89tlnivGyLIuXX35ZREVFCb1eL2655RaRkpKimOby5cvigQceEAEBASIoKEg88sgjIjc3tzY347qVk5MjJkyYIOrXry98fHxEw4YNxbRp0xTN63kMvIMkhF03fERERERehHVUiIiIyGsxUCEiIiKvxUCFiIiIvBYDFSIiIvJaDFSIiIjIazFQISIiIq/FQIWIiIi8FgMVIiIi8loMVIiIiMhrMVAhIiIir8VAhYiIiLwWAxUiIiLyWv8PFPBEmWnG69cAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision = len(anomalies) / (len(anomalies) + len(np.where(mse <= threshold)[0]))\n",
        "true_positives = len(np.intersect1d(anomalies, np.where(mse > threshold)[0]))\n",
        "false_negatives = len(anomalies) - true_positives\n",
        "recall = true_positives / (true_positives + false_negatives)\n",
        "f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1_score)"
      ],
      "metadata": {
        "id": "StlFp3tfWdRp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81789dd1-a054-4d05-f214-527ba49c51b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.0010741138560687433\n",
            "Recall: 1.0\n",
            "F1-score: 0.002145922746781116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gFk3lHydcwJF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "outputId": "9d0be417-a791-411b-8ec0-046097fd4b57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = keras.models.load_model('best_model.h5')\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "test_loss, test_accuracy = best_model.evaluate(X_test, X_test)\n",
        "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "id": "oWy3zbLDRbrs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "757a8a21-ca1c-424a-9e30-40d6454aeb28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30/30 [==============================] - 0s 4ms/step - loss: 0.2099 - mae: 0.2099\n",
            "Test Loss: 0.2099020630121231, Test Accuracy: 0.2099020630121231\n"
          ]
        }
      ]
    }
  ]
}